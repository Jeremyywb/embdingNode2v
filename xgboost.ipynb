{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "os.chdir(\"D:\\COMPETITIONS\\TianChen\\Feature\")\n",
    "\n",
    "\n",
    "\n",
    "def tprweight_funtion(y_true, y_prob ):\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_prob)\n",
    "    d['y'] = list(y_true)\n",
    "    print(d.head())\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3\n",
    "\n",
    "\n",
    "def tpr_weight_funtion(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    tpr_score = tprweight_funtion(labels, preds)\n",
    "    return [('gini', tpr_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'xgboost' from 'D:\\\\Programfiles2\\\\anaconda\\\\lib\\\\site-packages\\\\xgboost\\\\__init__.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,    # Revised to encode validation series\n",
    "                  val_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_val_series = pd.merge(\n",
    "        val_series.to_frame(val_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=val_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_val_series.index = val_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.81'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idTrain =  train_dfAderson[\"UID\"]\n",
    "idTest =  test_dfAderson[\"UID\"]\n",
    "\n",
    "\n",
    "\n",
    "train_dfAderson = train_dfAderson.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "test_dfAderson = test_dfAderson.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "\n",
    "train_dfAderson[\"UID\"] = idTrain\n",
    "test_dfAderson[\"UID\"] = idTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df, train_dfTrain_Mac_Wifi_Geo_Mode,how=\"left\")\n",
    "test_df = pd.merge(test_df, test_dfTrain_Mac_Wifi_Geo_Mode,how=\"left\")\n",
    "test_df = test_df.drop(\"tmpUID\",axis=1)\n",
    "train_df = pd.merge(train_df, trainAddr,how=\"left\")\n",
    "test_df = pd.merge(test_df, testAddr,how=\"left\")\n",
    "train_df = pd.merge(train_df, train_dfAderson,how=\"left\")\n",
    "test_df = pd.merge(test_df, test_dfAderson,how=\"left\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_df_latest.csv\",encoding=\"utf8\",index=False)\n",
    "test_df.to_csv(\"test_df_latest.csv\",encoding=\"utf8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (31179, 31198)\n",
    "# train_df.Tag.unique()\n",
    "# len(test_df.columns)\n",
    "# len(test_df)\n",
    "# train_df[\"User_maxOfAll_ip1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tag']\n",
      "['tmpUID']\n"
     ]
    }
   ],
   "source": [
    "# # len(train_df.columns),len(test_df.columns)\n",
    "b = list(train_df.columns)\n",
    "a = list(test_df.columns)\n",
    "print(list(set(b).difference(set(a))) )# b中有而a中没有的\n",
    "print(list(set(a).difference(set(b))) )# a中有而\n",
    "\n",
    "# print(len(list(set(b).intersection(set(a))))) # b中有而a中没有的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "b = [1,2,3,5]\n",
    "a = [1]\n",
    "print(list(set(b).difference(set(a))) )# b中有而a中没有的\n",
    "print(list(set(a).difference(set(b))) )# a中有而"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trainAddr = pd.read_csv(\"trainAddrProV.csv\",encoding=\"utf8\")\n",
    "# testAddr = pd.read_csv(\"testAddrProV.csv\",encoding=\"utf8\")\n",
    "# train_dfTrain_Mac_Wifi_Geo_Mode = train_dfTrain_Mac_Wifi_Geo_Mode.drop(\"Tag\",axis=1)\n",
    "# trainAddr = trainAddr.drop(\"Tag\",axis=1)\n",
    "# train_df.UID.unique()\n",
    "# train_dfTrain_Mac_Wifi_Geo_Mode = train_dfTrain_Mac_Wifi_Geo_Mode.drop(\"Tag\",axis=1)\n",
    "# train_dfTrain_Mac_Wifi_Geo_Mode.columns\n",
    "# train_df\n",
    "# train_dfTrain_Mac_Wifi_Geo_Mode\n",
    "# trainAddr\n",
    "# test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"trainDummyMerchantCntonUID_V2.csv\",encoding=\"utf8\")\n",
    "\n",
    "# test_df = pd.read_csv('testDummyMerchantCntonUID_V2.csv', na_values=\"-1\",encoding=\"utf8\")   \n",
    "\n",
    "train_df = pd.read_csv(\"train_df_latest.csv\",encoding=\"utf8\")\n",
    "\n",
    "test_df = pd.read_csv('test_df_latest.csv', na_values=\"-1\",encoding=\"utf8\")   \n",
    "# train_dayFea =  pd.read_csv(\"train_routeFeature.csv\",encoding=\"utf8\")\n",
    "# test_dayFea =  pd.read_csv(\"test_routeFeature.csv\",encoding=\"utf8\")\n",
    "\n",
    "# train_df = pd.merge(train_df, train_dayFea,how=\"left\")\n",
    "# test_df = pd.merge(test_df, test_dayFea,how=\"left\")\n",
    "# train_df = frequencyOrder(train_df,\"mostOperCntinRoutes\")\n",
    "# test_df =  frequencyOrder(test_df,\"mostOperCntinRoutes\")\n",
    "# train_df = train_df.drop(\"mostOperCntinRoutes\",axis=1)\n",
    "# test_df = test_df.drop(\"mostOperCntinRoutes\",axis=1)\n",
    "# for col in [\"FirOpperGroupCNT\",\"MINrouteGroupCNT\",\"MAXrouteGroupCNT\",\"mostOperCntinRoutesGroupCNT\"]:\n",
    "#     train_df[col] = np.log(train_df[col] + 1)\n",
    "#     test_df[col] = np.log(test_df[col]  + 1)\n",
    "\n",
    "# train_geo_codeMac1Ip1 =  pd.read_csv(\"train_geo_codeMac1Ip1.csv\",encoding=\"utf8\")\n",
    "# test_geo_codeMac1Ip1 =  pd.read_csv(\"test_geo_codeMac1Ip1.csv\",encoding=\"utf8\")\n",
    "# train_geo_codeMac1Ip1 = train_geo_codeMac1Ip1.drop(\"Tag\",axis=1)\n",
    "# test_geo_codeMac1Ip1 = test_geo_codeMac1Ip1.drop(\"tmpUID\",axis=1)\n",
    "# train_df = pd.merge(train_df, train_geo_codeMac1Ip1,how=\"left\")\n",
    "# test_df = pd.merge(test_df, test_geo_codeMac1Ip1,how=\"left\")\n",
    "# test_df = test_df.drop(\"tmpUID\",axis=1)\n",
    "# train_basDaysBalSummaryOverDay =  pd.read_csv(\"train_basDaysBalSummaryOverDay.csv\",encoding=\"utf8\")\n",
    "# test_basDaysBalSummaryOverDay =  pd.read_csv(\"test_basDaysBalSummaryOverDay.csv\",encoding=\"utf8\")\n",
    "# train_df = pd.merge(train_df, train_basDaysBalSummaryOverDay,how=\"left\")\n",
    "# test_df = pd.merge(test_df, test_basDaysBalSummaryOverDay,how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# UIDInGeoUserCntBoxIdx_3.0\n",
    "# UIDInGeoUserCntBoxIdx_4.0\n",
    "# UIDInGeoUserCntBoxIdx_5.0\n",
    "# UIDInGeoUserCntBoxIdx_6.0\n",
    "# UIDInGeoUserCntBoxIdx_7.0\n",
    "# UIDInGeoUserCntBoxIdx_nan\n",
    "# UIDInMacUserCntBoxIdx_4.0\n",
    "# UIDInMacUserCntBoxIdx_6.0\n",
    "# UIDInMacUserCntBoxIdx_7.0\n",
    "# UIDInMacUserCntBoxIdx_nan\n",
    "# train_dfAderson = pd.read_csv(\"train_TimeAnderson.csv\",encoding=\"utf8\")\n",
    "\n",
    "# test_dfAderson = pd.read_csv('test_TimeAnderson.csv',encoding=\"utf8\")   \n",
    "\n",
    "# train_dfTrain_Mac_Wifi_Geo_Mode = pd.read_csv(\"Train_Mac_Wifi_Geo_Mode.csv\",encoding=\"utf8\")\n",
    "\n",
    "# test_dfTrain_Mac_Wifi_Geo_Mode = pd.read_csv('Test_Mac_Wifi_Geo_Mode.csv', na_values=\"-1\",encoding=\"utf8\")   \n",
    "    \n",
    "# # predictors\n",
    "train_features =  [x for x in train_df.columns if x not in [\"UID\",\"Tag\"]]\n",
    "\n",
    "\n",
    "# train_features =list(trainAddr.columns[1:])+ [\"trans_amt_max\",\"memp_mean\",\"trans_amt_min\",\"uni_mem_mean\",\"memp_max\",\"uni_device_mean\",\"uni_mem_max\",\"uni_device_min\",\"uni_device_max\",\"members_min\",\"members_max\",\"memp_min\",\"merchant_x\",\"uni_mem_min\",\"trans_amt_mean\",\"members_mean\",\"max_time\",\"time2\",\"UIDInWifiUserCntBoxIdx_7.0\",\"modeTypes_2.0\",\"most_time\",\"UID_VAL4amt_src1merchanthas_nan\",\"UID_VAL2amt_src1merchanthas_nan\",\"modeTypes_9.0\",\"UID_VAL4trans_type1merchanthas_5.0\",\"UID_VAL7amt_src1merchanthas_nan\",\"time4\",\"min_time\",\"UIDInWifiUserCntBoxIdx_4.0\",\"UID_VAL4amt_src1merchanthas_4.0\",\"UID_VAL28amt_src1merchanthas_6.0\",\"modeTypes_11.0\",\"modeTypes_12.0\",\"UIDInGeoUserCntBoxIdx_5.0\",\"UIDInGeoUserCntBoxIdx_nan\",\"modeTypes_3.0\",\"UIDInMacUserCntBoxIdx_nan\",\"time3\",\"modeTypes_8.0\",\"UID_VAL8amt_src1merchanthas_nan\",\"modeTypes_1.0\",\"UID_VAL10amt_src1merchanthas_nan\",\"device2\",\"mean_time\",\"UID_VAL3trans_type1merchanthas_nan\",\"amt_src1\",\"UIDInWifiUserCntBoxIdx_5.0\",\"modeTypes_6.0\",\"UIDInGeoUserCntBoxIdx_4.0\",\"UID_VAL5trans_type1merchanthas_3.0\",\"UID_VAL3amt_src1merchanthas_5.0\",\"UID_VAL3amt_src1merchanthas_nan\",\"UID_VAL7amt_src1merchanthas_5.0\",\"UIDInMacUserCntBoxIdx_4.0\",\"UID_VAL2amt_src1merchanthas_7.0\",\"UID_VAL3amt_src1merchanthas_7.0\",\"UID_VAL7amt_src1merchanthas_4.0\",\"UID_VAL2trans_type1merchanthas_7.0\",\"UID_VAL2trans_type1merchanthas_4.0\",\"UID_VAL7trans_type1merchanthas_4.0\",\"UID_VAL3amt_src1merchanthas_6.0\",\"UID_VAL4amt_src1merchanthas_7.0\",\"UID_VAL6amt_src1merchanthas_nan\",\"UID_VAL15amt_src1merchanthas_4.0\",\"UIDInGeoUserCntBoxIdx_7.0\",\"UIDInMacUserCntBoxIdx_7.0\",\"UID_VAL1trans_type1merchanthas_nan\",\"UID_VAL2amt_src1merchanthas_4.0\",\"UID_VAL10amt_src1merchanthas_5.0\",\"UID_VAL1trans_type1merchanthas_7.0\",\"UID_VAL10amt_src1merchanthas_7.0\",\"UID_VAL4trans_type1merchanthas_nan\",\"UID_VAL2trans_type2merchanthas_7.0\",\"UID_VAL4amt_src1merchanthas_5.0\",\"UID_VAL2trans_type1merchanthas_6.0\",\"UID_VAL1amt_src1merchanthas_nan\",\"UID_VAL2amt_src1merchanthas_6.0\",\"UID_VAL3amt_src1merchanthas_4.0\",\"uni_trans_min\",\"modeTypes_4.0\",\"UIDInGeoUserCntBoxIdx_6.0\",\"UID_VAL2trans_type2merchanthas_5.0\",\"UID_VAL2trans_type2merchanthas_6.0\",\"UID_VAL2amt_src1merchanthas_5.0\",\"UID_VAL6amt_src1merchanthas_4.0\",\"UID_VAL7amt_src1merchanthas_6.0\",\"UID_VAL7amt_src1merchanthas_7.0\",\"UID_VAL10amt_src1merchanthas_4.0\",\"UID_VAL3trans_type1merchanthas_7.0\",\"UID_VAL2trans_type2merchanthas_nan\",\"UID_VAL5trans_type2merchanthas_nan\",\"UID_VAL2trans_type1merchanthas_5.0\",\"UID_VAL1trans_type2merchanthas_6.0\",\"UID_VAL5trans_type2merchanthas_7.0\",\"UID_VAL8amt_src1merchanthas_7.0\",\"UID_VAL9amt_src1merchanthas_nan\",\"time1\",\"UIDInWifiUserCntBoxIdx_6.0\",\"modeTypes_nan\",\"UID_VAL10trans_type1merchanthas_4.0\",\"UID_VAL1amt_src1merchanthas_7.0\",\"UID_VAL9amt_src1merchanthas_7.0\",\"uni_trans_mean\",\"UIDInWifiUserCntBoxIdx_nan\",\"UIDInMacUserCntBoxIdx_6.0\",\"UID_VAL3trans_type1merchanthas_5.0\",\"UID_VAL5trans_type1merchanthas_nan\",\"UID_VAL2trans_type2merchanthas_4.0\",\"uni_trans_max\",\"modeTypes_5.0\",\"UID_VAL2trans_type1merchanthas_nan\",\"UID_VAL4trans_type1merchanthas_4.0\",\"UID_VAL8trans_type1merchanthas_nan\",\"UID_VAL10trans_type1merchanthas_nan\",\"UID_VAL12trans_type1merchanthas_3.0\",\"UID_VAL12trans_type1merchanthas_nan\",\"UID_VAL13trans_type1merchanthas_nan\",\"UID_VAL1trans_type2merchanthas_7.0\",\"UID_VAL1trans_type2merchanthas_nan\",\"UID_VAL3trans_type2merchanthas_4.0\",\"UID_VAL3trans_type2merchanthas_7.0\",\"UID_VAL3trans_type2merchanthas_nan\",\"UID_VAL6amt_src1merchanthas_5.0\",\"UID_VAL6amt_src1merchanthas_7.0\",\"UID_VAL8amt_src1merchanthas_5.0\",\"UID_VAL9amt_src1merchanthas_4.0\",\"UID_VAL10amt_src1merchanthas_6.0\",\"UID_VAL15amt_src1merchanthas_3.0\",\"UID_VAL20amt_src1merchanthas_4.0\",\"device1_null\",\"time6\"]\n",
    "# trainB_features = list(train_dayFea.columns[1:]) + [\"trans_amt_max\",\"memp_mean\",\"trans_amt_min\",\"memp_max\",\"uni_mem_mean\",\"uni_device_mean\",\"memp_min\",\"uni_mem_max\",\"members_max\",\"uni_device_min\",\"uni_device_max\",\"uni_mem_min\",\"merchant_x\",\"trans_amt_mean\",\"members_min\",\"members_mean\",\"time2\",\"max_time\",\"UIDInWifiUserCntBoxIdx_7.0\",\"most_time\",\"ProV_nan\",\"modeTypes_2.0\",\"UID_VAL4amt_src1merchanthas_nan\",\"UID_VAL2amt_src1merchanthas_nan\",\"UID_VAL4amt_src1merchanthas_4.0\",\"UID_VAL7amt_src1merchanthas_nan\",\"modeTypes_9.0\",\"UID_VAL4trans_type1merchanthas_5.0\",\"min_time\",\"UIDInWifiUserCntBoxIdx_4.0\",\"modeTypes_3.0\",\"device2\",\"modeTypes_1.0\",\"mean_time\",\"UIDInGeoUserCntBoxIdx_5.0\",\"UID_VAL5trans_type1merchanthas_3.0\",\"UID_VAL28amt_src1merchanthas_6.0\",\"UIDInGeoUserCntBoxIdx_nan\",\"ProV_unknow\",\"time3\",\"UID_VAL8amt_src1merchanthas_nan\",\"modeTypes_8.0\",\"time4\",\"UIDInMacUserCntBoxIdx_nan\",\"amt_src1\",\"UID_VAL3amt_src1merchanthas_5.0\",\"UID_VAL2trans_type1merchanthas_7.0\",\"UID_VAL4amt_src1merchanthas_7.0\",\"ProV_NeiMengGu\",\"modeTypes_12.0\",\"UID_VAL10amt_src1merchanthas_nan\",\"UID_VAL6amt_src1merchanthas_nan\",\"UID_VAL1trans_type1merchanthas_nan\",\"UIDInWifiUserCntBoxIdx_5.0\",\"modeTypes_6.0\",\"UID_VAL3amt_src1merchanthas_nan\",\"time1\",\"UID_VAL3trans_type1merchanthas_nan\",\"UIDInGeoUserCntBoxIdx_4.0\",\"UID_VAL15amt_src1merchanthas_4.0\",\"UIDInGeoUserCntBoxIdx_7.0\",\"ProV_ShanXi\",\"UID_VAL7amt_src1merchanthas_5.0\",\"UID_VAL3amt_src1merchanthas_7.0\",\"ProV_GuangDong\",\"UIDInMacUserCntBoxIdx_4.0\",\"UID_VAL2amt_src1merchanthas_7.0\",\"UID_VAL4amt_src1merchanthas_5.0\",\"UID_VAL2amt_src1merchanthas_6.0\",\"ProV_HeBei\",\"ProV_JiangXi\",\"UID_VAL7trans_type1merchanthas_4.0\",\"UID_VAL3amt_src1merchanthas_6.0\",\"UIDInMacUserCntBoxIdx_7.0\",\"UID_VAL10amt_src1merchanthas_4.0\",\"ProV_JiangSu\",\"UID_VAL7amt_src1merchanthas_4.0\",\"ProV_SiChuan\",\"UID_VAL2amt_src1merchanthas_4.0\",\"ProV_AnHui\",\"UID_VAL4trans_type1merchanthas_nan\",\"UID_VAL2trans_type2merchanthas_7.0\",\"UID_VAL2trans_type2merchanthas_6.0\",\"uni_trans_mean\",\"UID_VAL10amt_src1merchanthas_5.0\",\"UID_VAL1trans_type1merchanthas_7.0\",\"UID_VAL2trans_type1merchanthas_6.0\",\"modeTypes_4.0\",\"UID_VAL6amt_src1merchanthas_4.0\",\"ProV_GuiZhou\",\"UID_VAL2trans_type1merchanthas_4.0\",\"UID_VAL2trans_type1merchanthas_5.0\",\"UID_VAL5trans_type2merchanthas_7.0\",\"UID_VAL10amt_src1merchanthas_7.0\",\"UID_VAL7amt_src1merchanthas_6.0\",\"UID_VAL5trans_type2merchanthas_nan\",\"UID_VAL1trans_type2merchanthas_6.0\",\"UID_VAL8amt_src1merchanthas_7.0\",\"UID_VAL1amt_src1merchanthas_nan\",\"UID_VAL3amt_src1merchanthas_4.0\",\"uni_trans_min\",\"UID_VAL2amt_src1merchanthas_5.0\",\"UID_VAL7amt_src1merchanthas_7.0\",\"UID_VAL2trans_type2merchanthas_nan\",\"modeTypes_nan\",\"uni_trans_max\",\"UID_VAL2trans_type1merchanthas_nan\",\"ProV_HuNan\",\"ProV_ShanDong\",\"UIDInGeoUserCntBoxIdx_6.0\",\"UID_VAL2trans_type2merchanthas_5.0\",\"UID_VAL3trans_type1merchanthas_7.0\",\"UID_VAL9amt_src1merchanthas_nan\",\"UID_VAL5trans_type1merchanthas_nan\",\"modeTypes_5.0\",\"UID_VAL3trans_type2merchanthas_7.0\",\"UID_VAL3trans_type2merchanthas_nan\",\"UID_VAL10amt_src1merchanthas_6.0\",\"ProV_CHINAArea\",\"ProV_GanSu\",\"UIDInWifiUserCntBoxIdx_6.0\",\"UID_VAL10trans_type1merchanthas_4.0\",\"UIDInWifiUserCntBoxIdx_nan\",\"UID_VAL4trans_type1merchanthas_4.0\",\"UID_VAL1trans_type2merchanthas_7.0\",\"ProV_LiaoNing\",\"modeTypes_11.0\",\"UID_VAL1amt_src1merchanthas_7.0\",\"UID_VAL9amt_src1merchanthas_7.0\",\"UIDInMacUserCntBoxIdx_6.0\",\"UID_VAL1trans_type2merchanthas_nan\",\"UID_VAL6amt_src1merchanthas_5.0\",\"UID_VAL6amt_src1merchanthas_7.0\",\"device1_null\"]\n",
    "# unImportant = [\"user_dayCnt12\",\"user_dayCnt13\",\"user_dayCnt21\",\"user_dayCnt27\",\"UID_VAL2trans_type2merchanthas_nan\",\"ProV_ShanDong\",\"ProV_CHINAArea\",\"UID_VAL9amt_src1merchanthas_7.0\",\"UIDInMacUserCntBoxIdx_6.0\",\"UID_VAL1trans_type2merchanthas_nan\",\"UID_VAL6amt_src1merchanthas_7.0\"]\n",
    "\n",
    "# unImportant = [\"UIDInGeoUserCntBoxIdx_3.0\",\"UIDInGeoUserCntBoxIdx_4.0\",\"UIDInGeoUserCntBoxIdx_5.0\",\"UIDInGeoUserCntBoxIdx_6.0\",\"UIDInGeoUserCntBoxIdx_7.0\",\"UIDInGeoUserCntBoxIdx_nan\",\"UIDInMacUserCntBoxIdx_4.0\",\"UIDInMacUserCntBoxIdx_6.0\",\"UIDInMacUserCntBoxIdx_7.0\",\"UIDInMacUserCntBoxIdx_nan\"]\n",
    "# um10 = [\"device1_null\",\"device2_null\",\"UIDInWifiUserCntBoxIdx_6.0\",\"UIDInWifiUserCntBoxIdx_nan\",\"modeTypes_7.0\",\"modeTypes_10.0\",\"ProV_AnHui\",\"ProV_BeiJing\",\"ProV_CHINAArea\",\"ProV_ChongQing\",\"ProV_FuJian\",\"ProV_GuangXi\",\"ProV_GuiZhou\",\"ProV_HaiNan\",\"ProV_HeBei\",\"ProV_HeNan\",\"ProV_HeiLongJiang\",\"ProV_HuBei\",\"ProV_HuNan\",\"ProV_JiLin\",\"ProV_LiaoNing\",\"ProV_NingXia\",\"ProV_QingHai\",\"ProV_ShanDong\",\"ProV_ShanXi\",\"ProV_ShangHai\",\"ProV_SiChuan\",\"ProV_TaiWan\",\"ProV_TianJin\",\"ProV_XiZang\",\"ProV_XinJiang\",\"ProV_ZheJiang\",\"ProV_nan\",\"uni_channel_min\",\"uni_channel_max\",\"uni_channel_mean\",\"uni_trans_max\",\"time0\",\"time1\",\"time5\",\"time6\",\"user_dayCnt2\",\"user_dayCnt3\",\"user_dayCnt4\",\"user_dayCnt5\",\"user_dayCnt6\",\"user_dayCnt7\",\"user_dayCnt8\",\"user_dayCnt11\",\"user_dayCnt12\",\"user_dayCnt13\",\"user_dayCnt14\",\"user_dayCnt16\",\"user_dayCnt17\",\"user_dayCnt20\",\"user_dayCnt21\",\"user_dayCnt24\",\"user_dayCnt25\",\"user_dayCnt27\",\"user_dayCnt28\",\"balUserbal_minOverDaysmean\",\"balUserbal_minOverDaysmax\",\"balUserbal_minOverDaysmin\",\"balUserbal_maxOverDaysmean\",\"balUserbal_maxOverDaysmax\",\"balUserbal_meanOverDaysmean\"]\n",
    "# umall = unImportant + um10\n",
    "# train_features = []\n",
    "# for m in tmp_features:\n",
    "#     if m not in umall:\n",
    "#         train_features.append(m)\n",
    "\n",
    "# # combs = [\n",
    "# #     ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "# #     ('ps_reg_01', 'ps_car_04_cat'),\n",
    "# # ]\n",
    "\n",
    "# # lambda x: \n",
    "# for col in train_features:\n",
    "#     test_df[col] = test_df[col].map(lambda x:0 if x ==0 else 1)\n",
    "# for col in train_features:\n",
    "#     train_df[col] = train_df[col].map(lambda x:0 if x ==0 else 1)\n",
    "# # f = \n",
    "# # f(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_df = frequencyOrder(train_df,\"mostOperCntinRoutes\")\n",
    "# test_df =  frequencyOrder(test_df,\"mostOperCntinRoutes\")\n",
    "# train_df = train_df.drop(\"mostOperCntinRoutes\",axis=1)\n",
    "# test_df = test_df.drop(\"mostOperCntinRoutes\",axis=1)\n",
    "# for col in [\"FirOpperGroupCNT\",\"MINrouteGroupCNT\",\"MAXrouteGroupCNT\",\"mostOperCntinRoutesGroupCNT\"]:\n",
    "#     train_df[col] = np.log(train_df[col] + 1)\n",
    "#     test_df[col] = np.log(test_df[col]  + 1)\n",
    "\n",
    "\n",
    "# mostOperCntinRoutes drop\n",
    "# FirOpperGroupCNT /MINrouteGroupCNT/MAXrouteGroupCNT/mostOperCntinRoutesGroupCNT/ log(x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_df.mostOperCntinRoutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save featuresFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change log id = 5\n",
    "# train_dfNew =   train_df[train_features+[\"UID\",\"Tag\" ]]\n",
    "# test_dfNew = test_df[train_features+[\"UID\" ]]\n",
    "# os.rename(\"train_df_latest.csv\",\"train_df_latestV5.csv\")\n",
    "# os.rename(\"test_df_latest.csv\",\"test_df_latestV5.csv\")\n",
    "# train_dfNew.to_csv(\"train_df_latest.csv\", encoding = \"utf8\",index = False)\n",
    "# test_dfNew.to_csv(\"test_df_latest.csv\", encoding = \"utf8\",index = False)\n",
    "# del train_dfNew,test_dfNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_dfNew\n",
    "# len(train_df.columns),len(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_test = test_df['UID'].values\n",
    "id_train = train_df['UID'].values\n",
    "y = train_df['Tag']\n",
    "# train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# start = time.time()\n",
    "# for n_c, (f1, f2) in enumerate(combs):\n",
    "#     name1 = f1 + \"_plus_\" + f2\n",
    "#     print('current feature %60s %4d in %5.1f'\n",
    "#           % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "#     print('\\r' * 75, end='')\n",
    "#     train_df[name1] = train_df[f1].apply(lambda x: str(x)) + \"_\" + train_df[f2].apply(lambda x: str(x))\n",
    "#     test_df[name1] = test_df[f1].apply(lambda x: str(x)) + \"_\" + test_df[f2].apply(lambda x: str(x))\n",
    "#     # Label Encode\n",
    "#     lbl = LabelEncoder()\n",
    "#     lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "#     train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "#     test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "#     train_features.append(name1)\n",
    "    \n",
    "X = train_df[train_features]\n",
    "test_df = test_df[train_features]\n",
    "\n",
    "# f_cats = [f for f in X.columns if \"_cat\" in f]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_valid_pred = 0*y\n",
    "y_test_pred = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def frequencyOrder(df,variable):\n",
    "    Countdf = df.groupby(variable).agg({ \"UID\":np.count_nonzero }).add_suffix('Cnt').reset_index()\n",
    "    Countdf[variable+\"Rank\"] = Countdf[\"UIDCnt\"].rank(ascending=0,method='dense')\n",
    "    Countdf.columns = [variable,variable+ \"GroupCNT\",variable+\"Rank\" ]\n",
    "    return pd.merge(df,Countdf, how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_dayFea.head()\n",
    "# test_dayFea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 试试别人的调参代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\ipykernel\\__main__.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "y = train_df[\"Tag\"]\n",
    "X = train_df[train_features]\n",
    "\n",
    "# Create train and test set \n",
    "x_train,x_test,y_train,y_test = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
    "# train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
    "# print(\"Taille des predictor sur le train set : \", train_X.shape)\n",
    "# print(\"Taille de la target sur le train set : \", train_y.shape)\n",
    "# print(\"Taille des predictor sur le test set : \", test_X.shape)\n",
    "# print(\"Taille de la target sur le test set : \", test_y.shape)\n",
    "\n",
    "my_imputer = Imputer()\n",
    "x_train = my_imputer.fit_transform(x_train)\n",
    "x_test = my_imputer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance sur le train :  0.9719221214906406\n",
      "Performance sur le test :  0.9673989074298157\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "my_model = XGBRegressor()\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "my_model.fit(train_X, train_y, verbose=False)\n",
    "\n",
    "\"\"\"\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
    "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)\n",
    "\"\"\"\n",
    "\n",
    "# Performance sur le train\n",
    "train_y_pred = my_model.predict(train_X)\n",
    "auc = roc_auc_score(train_y, train_y_pred)\n",
    "print(\"Performance sur le train : \", auc)\n",
    "\n",
    "# Performance sur le test\n",
    "test_y_pred = my_model.predict(test_X)\n",
    "auc = roc_auc_score(test_y, test_y_pred)\n",
    "print(\"Performance sur le test : \", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       prob  y\n",
      "0  0.035304  0\n",
      "1  0.087473  0\n",
      "2  0.026162  0\n",
      "3  0.120814  1\n",
      "4  0.009345  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7178538390379279"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tprweight_funtion(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prob  y\n",
      "0     1  1\n",
      "1     1  1\n",
      "2     0  0\n",
      "3     0  0\n",
      "4     0  0\n",
      "Performance sur le train :  0.5277777777777778\n",
      "   prob  y\n",
      "0     0  0\n",
      "1     0  0\n",
      "2     0  0\n",
      "3     0  1\n",
      "4     0  0\n",
      "Performance sur le test :  0.404995374653099\n"
     ]
    }
   ],
   "source": [
    "# Step 1 : Fix learning rate and number of estimators for tuning tree-based parameters\n",
    "xgb1 = XGBClassifier(\n",
    "    learning_rate =0.1, n_estimators=1000,\n",
    "    max_depth=5, min_child_weight=50, gamma=0,subsample=0.8,colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic', nthread=4,scale_pos_weight=1,seed=27)\n",
    "# modelfit(xgb1, train, train_X)\n",
    "xgb1.fit(train_X, train_y)\n",
    "\n",
    "# Performance sur le train\n",
    "auc = tprweight_funtion(train_y, xgb1.predict(train_X))\n",
    "print(\"Performance sur le train : \", auc)\n",
    "\n",
    "# Performance sur le test\n",
    "auc = tprweight_funtion(test_y, xgb1.predict(test_X))\n",
    "print(\"Performance sur le test : \", auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prob  y\n",
      "0     1  1\n",
      "1     1  1\n",
      "2     0  0\n",
      "3     0  0\n",
      "4     0  0\n",
      "Performance sur le train :  0.5277777777777778\n",
      "   prob  y\n",
      "0     0  0\n",
      "1     0  0\n",
      "2     0  0\n",
      "3     0  1\n",
      "4     0  0\n",
      "Performance sur le test :  0.404995374653099\n"
     ]
    }
   ],
   "source": [
    "# Performance sur le train\n",
    "auc = tprweight_funtion(train_y, xgb1.predict(train_X))\n",
    "print(\"Performance sur le train : \", auc)\n",
    "\n",
    "# Performance sur le test\n",
    "auc = tprweight_funtion(test_y, xgb1.predict(test_X))\n",
    "print(\"Performance sur le test : \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label == 1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    return (dtrain, dtest, param)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'grid_scores_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-b00980b00deb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mgsearch1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'grid_scores_'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_test1 = {\n",
    "    'max_depth':range(3,10,2),\n",
    "    'min_child_weight':range(1,20,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, \n",
    "                                                  n_estimators=1000, \n",
    "                                                  max_depth=5,\n",
    "                                                  min_child_weight=1,\n",
    "                                                  gamma=0, \n",
    "                                                  subsample=0.8, \n",
    "                                                  colsample_bytree=0.8,\n",
    "                                                  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    "                        param_grid = param_test1, \n",
    "                        scoring='roc_auc',\n",
    "                        n_jobs=4,\n",
    "                        iid=False, \n",
    "                        cv=5)\n",
    "\n",
    "gsearch1.fit(train_X,train_y)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 3, 'min_child_weight': 1}, 0.9761450068291383)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gsearch1.feature_importances_\n",
    "# min_child_weight = float(np.sum(label == 0)) / np.sum(label == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance sur le train :  0.9306664489012124\n",
      "Performance sur le test :  0.8980270835623961\n"
     ]
    }
   ],
   "source": [
    "# Fix new param with optimal parameter\n",
    "# fpreproc=fpreproc\n",
    "xgb2 = XGBClassifier(\n",
    "    learning_rate =0.1, n_estimators=1000,\n",
    "    gamma=0,subsample=0.8,colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic', nthread=4,fpreproc=fpreproc,seed=27,\n",
    "    max_depth = 3,\n",
    "    min_child_weight = 1\n",
    ")\n",
    "# modelfit(xgb1, train, train_X)\n",
    "xgb2.fit(train_X, train_y)\n",
    "\n",
    "# Performance sur le train\n",
    "auc = roc_auc_score(train_y, xgb2.predict(train_X))\n",
    "print(\"Performance sur le train : \", auc)\n",
    "\n",
    "# Performance sur le test\n",
    "auc = roc_auc_score(test_y, xgb2.predict(test_X))\n",
    "print(\"Performance sur le test : \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prob  y\n",
      "0     1  1\n",
      "1     1  1\n",
      "2     0  0\n",
      "3     0  0\n",
      "4     0  0\n",
      "Performance sur le train :  0.5993445692883895\n",
      "   prob  y\n",
      "0     0  0\n",
      "1     0  0\n",
      "2     0  0\n",
      "3     0  1\n",
      "4     0  0\n",
      "Performance sur le test :  0.41036077705827934\n"
     ]
    }
   ],
   "source": [
    "auc = tprweight_funtion(train_y, xgb2.predict(train_X))\n",
    "print(\"Performance sur le train : \", auc)\n",
    "\n",
    "# Performance sur le test\n",
    "auc = tprweight_funtion(test_y, xgb2.predict(test_X))\n",
    "print(\"Performance sur le test : \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.txt'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tempModels = eval(\"[(0,4),(0,2),(0,9),(0,3)]\")\n",
    "# tempModels.sort(key=lambda x: int(x[1]))\n",
    "# tempModels\n",
    "import random\n",
    "str(random.randint(1,100)) + \".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'mean_fit_time': array([157.65483241, 164.17204833, 166.31916804, 160.82249088,\n",
       "         147.32151566]),\n",
       "  'mean_score_time': array([0.16231499, 0.13429403, 0.15430932, 0.14410257, 0.11988511]),\n",
       "  'mean_test_score': array([0.97614501, 0.9761114 , 0.97626236, 0.97629653, 0.97608132]),\n",
       "  'mean_train_score': array([0.99626998, 0.99630189, 0.99628598, 0.99630158, 0.99630657]),\n",
       "  'param_gamma': masked_array(data=[0.0, 0.1, 0.2, 0.3, 0.4],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'gamma': 0.0},\n",
       "   {'gamma': 0.1},\n",
       "   {'gamma': 0.2},\n",
       "   {'gamma': 0.3},\n",
       "   {'gamma': 0.4}],\n",
       "  'rank_test_score': array([3, 4, 2, 1, 5]),\n",
       "  'split0_test_score': array([0.97759575, 0.97762706, 0.97732015, 0.97734257, 0.97661704]),\n",
       "  'split0_train_score': array([0.99616544, 0.99610693, 0.99611497, 0.99613453, 0.99613921]),\n",
       "  'split1_test_score': array([0.97468938, 0.97473538, 0.97461207, 0.97505852, 0.97516868]),\n",
       "  'split1_train_score': array([0.99647445, 0.99641535, 0.99636981, 0.99646482, 0.99638528]),\n",
       "  'split2_test_score': array([0.97372594, 0.97383262, 0.97380827, 0.97373792, 0.97379629]),\n",
       "  'split2_train_score': array([0.99622745, 0.99629324, 0.99630518, 0.9962036 , 0.99621902]),\n",
       "  'split3_test_score': array([0.98120098, 0.98127558, 0.98156471, 0.98156857, 0.98125161]),\n",
       "  'split3_train_score': array([0.9958986 , 0.99599035, 0.99587576, 0.99601852, 0.99614584]),\n",
       "  'split4_test_score': array([0.97351299, 0.97308636, 0.9740066 , 0.97377509, 0.973573  ]),\n",
       "  'split4_train_score': array([0.99658398, 0.99670359, 0.99676416, 0.99668646, 0.99664348]),\n",
       "  'std_fit_time': array([ 3.10267079,  2.54696912,  1.36557975,  2.34973487, 46.58813379]),\n",
       "  'std_score_time': array([0.01868329, 0.01275785, 0.02435123, 0.01472097, 0.02411216]),\n",
       "  'std_test_score': array([0.00291773, 0.00300676, 0.0029349 , 0.00294365, 0.00280588]),\n",
       "  'std_train_score': array([0.00024127, 0.00024875, 0.00029447, 0.00024183, 0.00019038])},\n",
       " {'gamma': 0.3},\n",
       " 0.9762965337121908)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets tune gamma value using the parameters already tuned above.\n",
    "param_test3 = { \n",
    "    'gamma':[i/10.0 for i in range(0,5)] \n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, \n",
    "                                                  n_estimators=1000, \n",
    "                                                  max_depth = 3,\n",
    "                                                  min_child_weight = 1,\n",
    "                                                  gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                  objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test3, \n",
    "                        scoring='roc_auc',\n",
    "                        n_jobs=4,\n",
    "                        iid=False, \n",
    "                        cv=5)\n",
    "\n",
    "gsearch3.fit(train_X,train_y)\n",
    "gsearch3.cv_results_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prob  y\n",
      "0     1  1\n",
      "1     1  1\n",
      "2     0  0\n",
      "3     0  0\n",
      "4     0  0\n",
      "Performance sur le train :  0.5984706616729087\n",
      "   prob  y\n",
      "0     0  0\n",
      "1     0  0\n",
      "2     0  0\n",
      "3     0  1\n",
      "4     0  0\n",
      "Performance sur le test :  0.37400555041628125\n"
     ]
    }
   ],
   "source": [
    "xgb2 = XGBClassifier(\n",
    "    learning_rate =0.1, n_estimators=1000,\n",
    "    gamma=0.3,subsample=0.8,colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic', nthread=4,fpreproc=fpreproc,seed=27,\n",
    "    max_depth = 3,\n",
    "    min_child_weight = 1\n",
    ")\n",
    "# modelfit(xgb1, train, train_X)\n",
    "xgb2.fit(train_X, train_y)\n",
    "\n",
    "# Performance sur le train\n",
    "auc = tprweight_funtion(train_y, xgb2.predict(train_X))\n",
    "print(\"Performance sur le train : \", auc)\n",
    "\n",
    "# Performance sur le test\n",
    "auc = tprweight_funtion(test_y, xgb2.predict(test_X))\n",
    "print(\"Performance sur le test : \", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0.3, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=1000, n_jobs=1, nthread=4, objective='binary:logistic',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=27, silent=True, subsample=0.8),\n",
       "       fit_params=None, iid=False, n_jobs=4,\n",
       "       param_grid={'colsample_bytree': [0.6, 0.7, 0.8, 0.9], 'subsample': [0.6, 0.7, 0.8, 0.9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test4 = {\n",
    "    'subsample':[i/10.0 for i in range(6,10)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=1000, \n",
    "                                                  max_depth = 3,\n",
    "                                                  min_child_weight =1,\n",
    "                                                  gamma=0.3, \n",
    "                                                  subsample=0.8, colsample_bytree=0.8,\n",
    "                                                  objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test4, \n",
    "                        scoring='roc_auc',\n",
    "#                         eval_metric=tpr_weight_funtion,\n",
    "                        n_jobs=4,\n",
    "                        iid=False, \n",
    "                        cv=5)\n",
    "\n",
    "gsearch4.fit(train_X,train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Programfiles2\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'mean_fit_time': array([142.98824615, 145.80504198, 146.85878806, 140.47146568,\n",
       "         163.01301975, 161.50363131, 154.81935678, 158.24423161,\n",
       "         172.30924616, 168.5552134 , 168.35838881, 173.46303177,\n",
       "         180.49587703, 177.32156148, 185.23489175, 182.88565192]),\n",
       "  'mean_score_time': array([0.15370898, 0.15050688, 0.14090052, 0.15390868, 0.17332425,\n",
       "         0.14461832, 0.16769414, 0.15911684, 0.14341197, 0.15707555,\n",
       "         0.15424638, 0.14089999, 0.14871607, 0.15772662, 0.16928067,\n",
       "         0.13371119]),\n",
       "  'mean_test_score': array([0.97573961, 0.97578319, 0.97645079, 0.97638787, 0.97519943,\n",
       "         0.97579305, 0.9762957 , 0.97641911, 0.97546086, 0.97581722,\n",
       "         0.97629653, 0.97617393, 0.97528851, 0.97561095, 0.97598504,\n",
       "         0.9763362 ]),\n",
       "  'mean_train_score': array([0.99588479, 0.99592735, 0.99599865, 0.99577995, 0.99604384,\n",
       "         0.99612969, 0.99614684, 0.99589864, 0.99617534, 0.99628537,\n",
       "         0.99630158, 0.99601947, 0.99632246, 0.99636558, 0.99636636,\n",
       "         0.99618329]),\n",
       "  'param_colsample_bytree': masked_array(data=[0.6, 0.6, 0.6, 0.6, 0.7, 0.7, 0.7, 0.7, 0.8, 0.8, 0.8,\n",
       "                     0.8, 0.9, 0.9, 0.9, 0.9],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_subsample': masked_array(data=[0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8,\n",
       "                     0.9, 0.6, 0.7, 0.8, 0.9],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'colsample_bytree': 0.6, 'subsample': 0.6},\n",
       "   {'colsample_bytree': 0.6, 'subsample': 0.7},\n",
       "   {'colsample_bytree': 0.6, 'subsample': 0.8},\n",
       "   {'colsample_bytree': 0.6, 'subsample': 0.9},\n",
       "   {'colsample_bytree': 0.7, 'subsample': 0.6},\n",
       "   {'colsample_bytree': 0.7, 'subsample': 0.7},\n",
       "   {'colsample_bytree': 0.7, 'subsample': 0.8},\n",
       "   {'colsample_bytree': 0.7, 'subsample': 0.9},\n",
       "   {'colsample_bytree': 0.8, 'subsample': 0.6},\n",
       "   {'colsample_bytree': 0.8, 'subsample': 0.7},\n",
       "   {'colsample_bytree': 0.8, 'subsample': 0.8},\n",
       "   {'colsample_bytree': 0.8, 'subsample': 0.9},\n",
       "   {'colsample_bytree': 0.9, 'subsample': 0.6},\n",
       "   {'colsample_bytree': 0.9, 'subsample': 0.7},\n",
       "   {'colsample_bytree': 0.9, 'subsample': 0.8},\n",
       "   {'colsample_bytree': 0.9, 'subsample': 0.9}],\n",
       "  'rank_test_score': array([12, 11,  1,  3, 16, 10,  6,  2, 14,  9,  5,  7, 15, 13,  8,  4]),\n",
       "  'split0_test_score': array([0.97658148, 0.97652118, 0.97811835, 0.97733174, 0.97525682,\n",
       "         0.97707392, 0.97746278, 0.97768581, 0.97645315, 0.97697458,\n",
       "         0.97734257, 0.9768335 , 0.97612034, 0.97625679, 0.97700048,\n",
       "         0.97733174]),\n",
       "  'split0_train_score': array([0.99562004, 0.9958708 , 0.99587586, 0.99564102, 0.99579178,\n",
       "         0.99606466, 0.99601726, 0.99583216, 0.99598676, 0.99613085,\n",
       "         0.99613453, 0.99587822, 0.99606458, 0.99630052, 0.99631598,\n",
       "         0.99618675]),\n",
       "  'split1_test_score': array([0.9746457 , 0.97428622, 0.9756132 , 0.97507708, 0.97433454,\n",
       "         0.97336201, 0.97502296, 0.97511843, 0.97261252, 0.97437822,\n",
       "         0.97505852, 0.97509176, 0.97419423, 0.97348686, 0.9752487 ,\n",
       "         0.97533799]),\n",
       "  'split1_train_score': array([0.99599132, 0.99597345, 0.99607983, 0.99590706, 0.99621214,\n",
       "         0.9962772 , 0.99626608, 0.99593434, 0.99632184, 0.99639823,\n",
       "         0.99646482, 0.99606168, 0.99655576, 0.99633868, 0.99642667,\n",
       "         0.99623792]),\n",
       "  'split2_test_score': array([0.97268499, 0.97331466, 0.97363278, 0.97406648, 0.97272906,\n",
       "         0.97380556, 0.97396172, 0.97459139, 0.97302089, 0.97320295,\n",
       "         0.97373792, 0.97431695, 0.97183809, 0.97352764, 0.97353731,\n",
       "         0.97444915]),\n",
       "  'split2_train_score': array([0.99608313, 0.99606642, 0.99603431, 0.9957307 , 0.99613517,\n",
       "         0.99620633, 0.9961716 , 0.99591513, 0.9963643 , 0.99634949,\n",
       "         0.9962036 , 0.99599747, 0.99642953, 0.99649468, 0.99643034,\n",
       "         0.99618313]),\n",
       "  'split3_test_score': array([0.98077849, 0.98133974, 0.98128756, 0.98156625, 0.9802586 ,\n",
       "         0.98113179, 0.98144295, 0.98127287, 0.98102278, 0.98051874,\n",
       "         0.98156857, 0.98106569, 0.98043525, 0.98106337, 0.98068031,\n",
       "         0.98100191]),\n",
       "  'split3_train_score': array([0.99549369, 0.99547044, 0.99563471, 0.99541577, 0.99571907,\n",
       "         0.99578547, 0.99581534, 0.99555268, 0.9956992 , 0.99594112,\n",
       "         0.99601852, 0.99570496, 0.99597994, 0.99603068, 0.99599696,\n",
       "         0.99565037]),\n",
       "  'split4_test_score': array([0.97400737, 0.97345415, 0.97360203, 0.97389781, 0.97341814,\n",
       "         0.97359197, 0.9735881 , 0.97342705, 0.97419494, 0.97401163,\n",
       "         0.97377509, 0.97356177, 0.97385464, 0.97372011, 0.97345841,\n",
       "         0.97356022]),\n",
       "  'split4_train_score': array([0.99623578, 0.99625564, 0.99636856, 0.9962052 , 0.99636105,\n",
       "         0.99631481, 0.99646391, 0.9962589 , 0.99650457, 0.99660717,\n",
       "         0.99668646, 0.996455  , 0.99658251, 0.99666335, 0.99666184,\n",
       "         0.99665829]),\n",
       "  'std_fit_time': array([2.03527231, 2.64967504, 1.40744591, 2.62927273, 1.20614701,\n",
       "         1.90051953, 3.1758905 , 4.05588647, 1.51312688, 4.3004324 ,\n",
       "         4.70296977, 2.42980759, 3.05497994, 4.25375562, 2.18184422,\n",
       "         1.74847587]),\n",
       "  'std_score_time': array([0.02059208, 0.02025856, 0.01901222, 0.01530022, 0.0402839 ,\n",
       "         0.01357021, 0.0183657 , 0.03203983, 0.03591948, 0.01112224,\n",
       "         0.01438298, 0.00863424, 0.02230248, 0.02730659, 0.01236032,\n",
       "         0.04321068]),\n",
       "  'std_test_score': array([0.00281521, 0.00300596, 0.0029303 , 0.00286405, 0.00266917,\n",
       "         0.00299491, 0.00290705, 0.00279815, 0.00308508, 0.00266867,\n",
       "         0.00294365, 0.00267609, 0.00290997, 0.00291798, 0.00268306,\n",
       "         0.00264664]),\n",
       "  'std_train_score': array([0.00028176, 0.00026123, 0.00024176, 0.00026508, 0.00024751,\n",
       "         0.00019216, 0.00022002, 0.00022603, 0.00029252, 0.00022926,\n",
       "         0.00024183, 0.00024933, 0.00025193, 0.00021093, 0.00021635,\n",
       "         0.0003202 ])},\n",
       " {'colsample_bytree': 0.6, 'subsample': 0.8},\n",
       " 0.9764507851962989)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch4.cv_results_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " {'colsample_bytree': 0.6, 'subsample': 0.8},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prob  y\n",
      "0     1  1\n",
      "1     1  1\n",
      "2     0  0\n",
      "3     0  0\n",
      "4     0  0\n",
      "Performance sur le train :  0.5950686641697878\n",
      "   prob  y\n",
      "0     0  0\n",
      "1     0  0\n",
      "2     0  0\n",
      "3     0  1\n",
      "4     0  0\n",
      "Performance sur le test :  0.3726179463459759\n"
     ]
    }
   ],
   "source": [
    "xgb5 = XGBClassifier(\n",
    "    learning_rate =0.1, \n",
    "    n_estimators=1000,\n",
    "    gamma=0.3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.6,\n",
    "    max_depth = 3,\n",
    "    min_child_weight = 1,\n",
    "    objective= 'binary:logistic', nthread=4,fpreproc=fpreproc,seed=27\n",
    ")\n",
    "# modelfit(xgb1, train, train_X)\n",
    "xgb5.fit(train_X, train_y)\n",
    "\n",
    "# Performance sur le train\n",
    "auc = tprweight_funtion(train_y, xgb5.predict(train_X))\n",
    "print(\"Performance sur le train : \", auc)\n",
    "\n",
    "# Performance sur le test\n",
    "auc = tprweight_funtion(test_y, xgb5.predict(test_X))\n",
    "print(\"Performance sur le test : \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 非平衡样本评分函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aupr(y_true,y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true,y_pred)\n",
    "    roc_aupr = auc(recall,precision) \n",
    "    return roc_aupr\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "# score = make_scorer(my_custom_loss_func, greater_is_better=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed: 25.7min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 121.8min\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed: 280.7min\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed: 471.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL : 'xgb'\n",
      "Best cv_test_roc_auc: 0.979800 using {'learning_rate': 0.15, 'n_estimators': 400}\n",
      "                                              mean_train_accuracy  \\\n",
      "params                                                              \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}             0.955771   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}             0.959385   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}             0.960582   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}             0.961555   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}             0.962335   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}             0.963073   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}             0.962282   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}             0.963554   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}             0.964698   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}             0.965628   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}             0.966537   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}             0.967093   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}             0.964730   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}             0.966002   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}             0.967232   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}             0.968215   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}             0.969081   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}             0.970161   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}             0.966633   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}             0.968044   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}             0.969434   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}             0.970535   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}             0.971583   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}             0.972599   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}             0.967991   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}             0.969776   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}             0.971155   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}             0.972524   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}             0.973892   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}             0.974908   \n",
      "...                                                           ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}             0.979815   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}             0.982648   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}             0.985011   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}             0.987267   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}             0.989052   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}             0.990410   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}             0.980745   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}             0.984049   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}             0.987481   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}             0.990431   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}             0.992580   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}             0.994612   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}             0.980981   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}             0.984765   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}             0.987577   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}             0.989683   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}             0.991875   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}             0.993425   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}             0.982595   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}             0.986433   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}             0.989501   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}             0.992185   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}             0.994419   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}             0.996002   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}              0.983354   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}              0.986807   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}              0.989704   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}              0.991918   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}              0.993778   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}              0.994964   \n",
      "\n",
      "                                              mean_train_f1  \\\n",
      "params                                                        \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}       0.812406   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}       0.830189   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}       0.835861   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}       0.840699   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}       0.844586   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}       0.848066   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}       0.844386   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}       0.850395   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}       0.855891   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}       0.860187   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}       0.864404   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}       0.866911   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}       0.855985   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}       0.861959   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}       0.867480   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}       0.871865   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}       0.875705   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}       0.880346   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}       0.864776   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}       0.871074   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}       0.877210   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}       0.881984   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}       0.886529   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}       0.890858   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}       0.870852   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}       0.878709   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}       0.884724   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}       0.890617   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}       0.896429   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}       0.900814   \n",
      "...                                                     ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}       0.921504   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}       0.933095   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}       0.942631   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}       0.951583   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}       0.958592   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}       0.963844   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}       0.925386   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}       0.938767   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}       0.952496   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}       0.964057   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}       0.972320   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}       0.980037   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}       0.926280   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}       0.941658   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}       0.952843   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}       0.961067   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}       0.969502   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}       0.975390   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}       0.932880   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}       0.948336   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}       0.960452   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}       0.970818   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}       0.979314   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}       0.985251   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}        0.936029   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}        0.949801   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}        0.961169   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}        0.969710   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}        0.976777   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}        0.981255   \n",
      "\n",
      "                                              mean_train_precision  \\\n",
      "params                                                               \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}              0.972072   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}              0.974202   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}              0.975580   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}              0.974829   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}              0.973950   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}              0.974327   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}              0.973552   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}              0.974066   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}              0.973289   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}              0.973806   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}              0.973937   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}              0.974437   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}              0.973774   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}              0.973622   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}              0.975026   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}              0.976072   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}              0.976990   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}              0.979119   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}              0.974514   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}              0.976123   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}              0.977796   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}              0.979553   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}              0.980915   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}              0.982714   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}              0.975935   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}              0.978227   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}              0.979933   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}              0.981893   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}              0.984080   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}              0.984665   \n",
      "...                                                            ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}              0.988231   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}              0.991065   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}              0.992574   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}              0.993776   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}              0.994005   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}              0.994644   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}              0.988494   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}              0.992193   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}              0.993903   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}              0.995023   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}              0.996321   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}              0.997017   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}              0.989637   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}              0.992047   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}              0.993293   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}              0.994303   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}              0.995530   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}              0.996214   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}              0.990975   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}              0.993517   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}              0.994412   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}              0.995823   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}              0.996854   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}              0.997844   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}               0.990519   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}               0.993775   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}               0.994981   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}               0.995459   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}               0.996238   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}               0.996745   \n",
      "\n",
      "                                              mean_train_recall  \\\n",
      "params                                                            \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}           0.697897   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}           0.723287   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}           0.731153   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}           0.739019   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}           0.745561   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}           0.750779   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}           0.745483   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}           0.754595   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}           0.763785   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}           0.770327   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}           0.777025   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}           0.780763   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}           0.763629   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}           0.773287   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}           0.781308   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}           0.787773   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}           0.793458   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}           0.799688   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}           0.777259   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}           0.786449   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}           0.795405   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}           0.802103   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}           0.808723   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}           0.814720   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}           0.786215   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}           0.797586   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}           0.806386   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}           0.814875   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}           0.823131   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}           0.830140   \n",
      "...                                                         ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}           0.863240   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}           0.881542   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}           0.897508   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}           0.912928   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}           0.925779   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}           0.935125   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}           0.869860   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}           0.890810   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}           0.914408   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}           0.934969   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}           0.949455   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}           0.963629   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}           0.870561   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}           0.896184   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}           0.915654   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}           0.930140   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}           0.945016   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}           0.955685   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}           0.881231   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}           0.907087   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}           0.928738   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}           0.947040   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}           0.962383   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}           0.972975   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}            0.887227   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}            0.909579   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}            0.929673   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}            0.945405   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}            0.958255   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}            0.966433   \n",
      "\n",
      "                                              mean_train_roc_auc  \\\n",
      "params                                                             \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}            0.967297   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}            0.972394   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}            0.975066   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}            0.976969   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}            0.978483   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}            0.979714   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}            0.978515   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}            0.980833   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}            0.982385   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}            0.983676   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}            0.984854   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}            0.985787   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}            0.982410   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}            0.984248   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}            0.985811   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}            0.987099   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}            0.988268   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}            0.989210   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}            0.984947   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}            0.986746   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}            0.988314   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}            0.989622   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}            0.990738   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}            0.991707   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}            0.986748   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}            0.988729   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}            0.990189   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}            0.991470   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}            0.992686   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}            0.993731   \n",
      "...                                                          ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}            0.996937   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}            0.998048   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}            0.998651   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}            0.999021   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}            0.999268   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}            0.999424   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}            0.997172   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}            0.998437   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}            0.999131   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}            0.999539   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}            0.999738   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}            0.999868   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}            0.997498   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}            0.998469   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}            0.999035   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}            0.999356   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}            0.999552   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}            0.999665   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}            0.997873   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}            0.998905   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}            0.999432   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}            0.999721   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}            0.999865   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}            0.999933   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}             0.998189   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}             0.998949   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}             0.999378   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}             0.999596   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}             0.999711   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}             0.999773   \n",
      "\n",
      "                                              mean_test_accuracy  \\\n",
      "params                                                             \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}            0.954285   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}            0.957834   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}            0.958904   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}            0.959417   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}            0.960144   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}            0.960785   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}            0.959844   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}            0.961384   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}            0.962282   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}            0.962966   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}            0.963608   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}            0.964078   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}            0.962025   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}            0.963052   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}            0.963821   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}            0.964463   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}            0.964848   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}            0.965275   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}            0.963351   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}            0.964035   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}            0.964933   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}            0.964933   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}            0.965618   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}            0.965917   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}            0.964377   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}            0.965019   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}            0.965446   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}            0.965874   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}            0.965831   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}            0.966387   \n",
      "...                                                          ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}            0.967970   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}            0.968012   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}            0.967627   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}            0.967627   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}            0.967371   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}            0.967328   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}            0.967627   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}            0.967585   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}            0.967243   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}            0.967627   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}            0.967328   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}            0.967456   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}            0.967072   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}            0.967585   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}            0.967200   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}            0.967243   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}            0.967414   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}            0.967627   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}            0.967499   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}            0.967756   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}            0.968055   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}            0.967970   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}            0.968226   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}            0.968012   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}             0.967328   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}             0.967243   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}             0.967670   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}             0.967157   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}             0.967371   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}             0.967371   \n",
      "\n",
      "                                              mean_test_f1  \\\n",
      "params                                                       \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}      0.805183   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}      0.823185   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}      0.828314   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}      0.831082   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}      0.834751   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}      0.837864   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}      0.833435   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}      0.840863   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}      0.845222   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}      0.848660   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}      0.851835   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}      0.854108   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}      0.844105   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}      0.848944   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}      0.852784   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}      0.856002   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}      0.857833   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}      0.859727   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}      0.850721   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}      0.854170   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}      0.858391   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}      0.858457   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}      0.861627   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}      0.862990   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}      0.855467   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}      0.858788   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}      0.860793   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}      0.862940   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}      0.863117   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}      0.865840   \n",
      "...                                                    ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}      0.873026   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}      0.873692   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}      0.872451   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}      0.872737   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}      0.872017   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}      0.871951   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}      0.871550   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}      0.871802   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}      0.871055   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}      0.872803   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}      0.871786   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}      0.872421   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}      0.869585   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}      0.872141   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}      0.870997   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}      0.871356   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}      0.872265   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}      0.873281   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}      0.871575   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}      0.873273   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}      0.874719   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}      0.874624   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}      0.875767   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}      0.875014   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}       0.870751   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}       0.870861   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}       0.872929   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}       0.871365   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}       0.872358   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}       0.872636   \n",
      "\n",
      "                                              mean_test_precision  \\\n",
      "params                                                              \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}             0.968959   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}             0.968715   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}             0.969483   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}             0.968062   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}             0.967070   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}             0.967283   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}             0.966195   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}             0.966659   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}             0.966611   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}             0.965380   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}             0.964475   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}             0.964276   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}             0.965797   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}             0.966115   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}             0.964920   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}             0.963694   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}             0.963435   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}             0.964299   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}             0.963697   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}             0.962477   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}             0.962412   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}             0.961642   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}             0.961906   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}             0.962001   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}             0.964744   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}             0.962438   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}             0.962169   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}             0.960917   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}             0.958104   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}             0.956938   \n",
      "...                                                           ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}             0.956492   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}             0.952830   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}             0.949018   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}             0.947148   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}             0.943729   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}             0.943088   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}             0.956393   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}             0.953004   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}             0.946910   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}             0.946745   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}             0.944349   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}             0.943804   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}             0.951700   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}             0.950309   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}             0.945528   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}             0.944299   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}             0.943465   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}             0.942893   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}             0.951586   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}             0.947389   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}             0.946563   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}             0.944282   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}             0.944433   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}             0.942683   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}              0.952196   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}              0.948526   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}              0.947827   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}              0.942041   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}              0.941497   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}              0.939286   \n",
      "\n",
      "                                              mean_test_recall  \\\n",
      "params                                                           \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}          0.689096   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}          0.715887   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}          0.723364   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}          0.728348   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}          0.734579   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}          0.739252   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}          0.733021   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}          0.744236   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}          0.751090   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}          0.757321   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}          0.762928   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}          0.766667   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}          0.749844   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}          0.757321   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}          0.764174   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}          0.770093   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}          0.773209   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}          0.775701   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}          0.761682   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}          0.767913   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}          0.774766   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}          0.775389   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}          0.780374   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}          0.782555   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}          0.768536   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}          0.775389   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}          0.778816   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}          0.783178   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}          0.785358   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}          0.790654   \n",
      "...                                                        ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}          0.803116   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}          0.806854   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}          0.807477   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}          0.809346   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}          0.810592   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}          0.810904   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}          0.800623   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}          0.803427   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}          0.806542   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}          0.809657   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}          0.809657   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}          0.811215   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}          0.800623   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}          0.805919   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}          0.807477   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}          0.809034   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}          0.811215   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}          0.813395   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}          0.804050   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}          0.809969   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}          0.813084   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}          0.814642   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}          0.816511   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}          0.816511   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}           0.802181   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}           0.804985   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}           0.809034   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}           0.810592   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}           0.812773   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}           0.814953   \n",
      "\n",
      "                                              mean_test_roc_auc  \n",
      "params                                                           \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}           0.964293  \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}           0.968929  \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}           0.971103  \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}           0.972605  \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}           0.973674  \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}           0.974464  \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}           0.973716  \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}           0.975037  \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}           0.975855  \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}           0.976433  \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}           0.976936  \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}           0.977290  \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}           0.975848  \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}           0.976707  \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}           0.977277  \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}           0.977725  \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}           0.978090  \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}           0.978316  \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}           0.976886  \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}           0.977507  \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}           0.978020  \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}           0.978347  \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}           0.978622  \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}           0.978827  \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}           0.977637  \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}           0.978195  \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}           0.978534  \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}           0.978888  \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}           0.979072  \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}           0.979268  \n",
      "...                                                         ...  \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}           0.979356  \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}           0.979457  \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}           0.979233  \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}           0.979064  \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}           0.978901  \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}           0.978865  \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}           0.979633  \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}           0.979348  \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}           0.979281  \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}           0.979210  \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}           0.979187  \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}           0.979028  \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}           0.979019  \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}           0.979028  \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}           0.979104  \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}           0.978914  \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}           0.978637  \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}           0.978495  \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}           0.979733  \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}           0.979514  \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}           0.979411  \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}           0.979164  \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}           0.979080  \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}           0.978807  \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}            0.979253  \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}            0.979073  \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}            0.978919  \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}            0.978689  \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}            0.978541  \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}            0.978421  \n",
      "\n",
      "[120 rows x 10 columns]\n",
      "                                              std_train_accuracy  \\\n",
      "params                                                             \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}            0.001496   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}            0.000721   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}            0.000782   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}            0.000834   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}            0.000910   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}            0.000819   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}            0.000926   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}            0.000754   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}            0.001280   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}            0.001104   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}            0.000986   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}            0.000967   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}            0.001235   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}            0.001123   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}            0.000940   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}            0.000890   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}            0.000796   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}            0.000809   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}            0.000999   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}            0.000936   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}            0.000788   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}            0.000865   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}            0.000887   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}            0.000860   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}            0.000914   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}            0.000797   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}            0.000850   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}            0.000626   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}            0.000872   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}            0.000864   \n",
      "...                                                          ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}            0.000934   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}            0.000909   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}            0.001624   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}            0.002479   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}            0.003184   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}            0.003497   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}            0.000718   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}            0.000626   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}            0.000662   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}            0.000815   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}            0.000647   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}            0.000582   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}            0.000909   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}            0.001748   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}            0.002390   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}            0.002850   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}            0.003249   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}            0.003648   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}            0.000874   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}            0.000265   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}            0.000422   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}            0.000430   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}            0.000311   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}            0.000352   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}             0.000703   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}             0.001412   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}             0.002142   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}             0.002724   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}             0.003132   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}             0.003194   \n",
      "\n",
      "                                              std_train_f1  \\\n",
      "params                                                       \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}      0.007576   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}      0.003405   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}      0.003381   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}      0.003566   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}      0.003889   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}      0.003495   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}      0.003950   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}      0.003192   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}      0.005656   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}      0.004839   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}      0.004130   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}      0.004021   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}      0.005314   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}      0.004791   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}      0.003870   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}      0.003747   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}      0.003353   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}      0.003485   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}      0.004187   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}      0.003974   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}      0.003394   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}      0.003655   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}      0.003754   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}      0.003658   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}      0.003900   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}      0.003438   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}      0.003591   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}      0.002598   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}      0.003703   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}      0.003685   \n",
      "...                                                    ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}      0.003884   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}      0.003640   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}      0.006467   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}      0.009797   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}      0.012439   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}      0.013587   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}      0.002889   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}      0.002517   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}      0.002591   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}      0.003131   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}      0.002461   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}      0.002182   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}      0.003705   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}      0.006997   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}      0.009470   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}      0.011218   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}      0.012638   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}      0.014089   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}      0.003530   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}      0.001042   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}      0.001641   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}      0.001634   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}      0.001169   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}      0.001314   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}       0.002809   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}       0.005571   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}       0.008405   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}       0.010558   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}       0.012034   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}       0.012165   \n",
      "\n",
      "                                              std_train_precision  \\\n",
      "params                                                              \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}             0.005635   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}             0.002720   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}             0.003429   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}             0.003520   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}             0.003530   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}             0.003351   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}             0.003704   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}             0.003477   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}             0.003015   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}             0.003200   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}             0.004026   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}             0.003961   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}             0.004226   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}             0.004173   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}             0.004252   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}             0.003537   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}             0.003269   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}             0.002553   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}             0.003964   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}             0.003286   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}             0.003080   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}             0.003007   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}             0.002493   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}             0.002270   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}             0.003675   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}             0.002861   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}             0.002531   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}             0.002712   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}             0.002390   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}             0.001789   \n",
      "...                                                           ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}             0.001389   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}             0.001989   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}             0.002281   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}             0.002402   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}             0.003050   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}             0.002485   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}             0.002033   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}             0.001164   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}             0.001163   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}             0.001096   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}             0.000939   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}             0.001162   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}             0.001451   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}             0.002200   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}             0.002365   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}             0.001950   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}             0.001913   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}             0.002176   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}             0.001408   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}             0.000567   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}             0.000571   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}             0.000947   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}             0.000536   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}             0.000597   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}              0.001136   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}              0.001967   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}              0.001291   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}              0.001783   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}              0.002059   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}              0.002171   \n",
      "\n",
      "                                              std_train_recall  \\\n",
      "params                                                           \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}          0.011571   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}          0.004936   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}          0.003724   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}          0.003803   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}          0.004148   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}          0.003878   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}          0.004227   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}          0.003631   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}          0.007513   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}          0.006448   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}          0.004779   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}          0.004508   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}          0.006408   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}          0.005897   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}          0.004177   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}          0.004757   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}          0.004417   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}          0.005051   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}          0.004814   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}          0.005075   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}          0.005021   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}          0.004922   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}          0.005086   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}          0.005330   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}          0.005317   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}          0.005068   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}          0.004810   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}          0.003599   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}          0.005624   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}          0.005835   \n",
      "...                                                        ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}          0.006536   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}          0.005284   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}          0.009932   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}          0.016000   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}          0.020497   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}          0.023321   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}          0.004161   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}          0.004155   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}          0.004042   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}          0.005107   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}          0.004190   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}          0.003427   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}          0.005662   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}          0.010888   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}          0.015460   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}          0.019116   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}          0.022036   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}          0.024651   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}          0.005424   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}          0.001626   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}          0.002797   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}          0.002664   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}          0.001964   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}          0.002408   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}           0.004249   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}           0.008621   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}           0.014532   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}           0.018294   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}           0.021111   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}           0.021365   \n",
      "\n",
      "                                              std_train_roc_auc  \\\n",
      "params                                                            \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}           0.001266   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}           0.000901   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}           0.000731   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}           0.000722   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}           0.000681   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}           0.000681   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}           0.000723   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}           0.000697   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}           0.000637   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}           0.000579   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}           0.000562   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}           0.000561   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}           0.000587   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}           0.000534   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}           0.000535   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}           0.000493   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}           0.000475   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}           0.000458   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}           0.000505   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}           0.000469   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}           0.000444   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}           0.000443   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}           0.000394   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}           0.000396   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}           0.000535   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}           0.000514   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}           0.000491   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}           0.000463   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}           0.000359   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}           0.000309   \n",
      "...                                                         ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}           0.000220   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}           0.000302   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}           0.000488   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}           0.000533   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}           0.000537   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}           0.000491   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}           0.000279   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}           0.000187   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}           0.000144   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}           0.000091   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}           0.000059   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}           0.000024   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}           0.000283   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}           0.000504   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}           0.000568   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}           0.000561   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}           0.000528   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}           0.000491   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}           0.000125   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}           0.000076   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}           0.000054   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}           0.000037   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}           0.000029   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}           0.000017   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}            0.000237   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}            0.000391   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}            0.000439   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}            0.000431   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}            0.000378   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}            0.000328   \n",
      "\n",
      "                                              std_test_accuracy  std_test_f1  \\\n",
      "params                                                                         \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}           0.003604     0.017488   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}           0.003926     0.018245   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}           0.003871     0.018418   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}           0.003945     0.018404   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}           0.004199     0.019439   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}           0.004012     0.018462   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}           0.004229     0.019505   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}           0.004118     0.018548   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}           0.003831     0.017022   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}           0.003760     0.016692   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}           0.003866     0.016919   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}           0.003585     0.015564   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}           0.003589     0.016260   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}           0.003758     0.016814   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}           0.003483     0.015464   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}           0.003333     0.014524   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}           0.003494     0.015039   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}           0.003091     0.013290   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}           0.003855     0.017071   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}           0.003491     0.015087   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}           0.003348     0.014307   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}           0.003580     0.015451   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}           0.003316     0.014168   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}           0.003173     0.013679   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}           0.003554     0.015226   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}           0.003650     0.015430   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}           0.003764     0.015928   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}           0.003523     0.014937   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}           0.003486     0.014791   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}           0.003308     0.013938   \n",
      "...                                                         ...          ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}           0.003390     0.014587   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}           0.003455     0.014701   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}           0.003505     0.014890   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}           0.003076     0.013048   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}           0.003350     0.014149   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}           0.003019     0.012581   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}           0.002965     0.012591   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}           0.002533     0.010875   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}           0.002477     0.010675   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}           0.002686     0.011433   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}           0.002535     0.010803   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}           0.002289     0.010111   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}           0.004078     0.017113   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}           0.003497     0.014491   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}           0.003572     0.015130   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}           0.003146     0.013559   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}           0.002896     0.012406   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}           0.003007     0.012909   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}           0.003357     0.014133   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}           0.003797     0.015709   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}           0.003567     0.014847   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}           0.003439     0.014455   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}           0.003505     0.014546   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}           0.003919     0.016256   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}            0.003855     0.015773   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}            0.003398     0.013877   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}            0.003310     0.013083   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}            0.002921     0.011819   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}            0.002777     0.011676   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}            0.002828     0.012089   \n",
      "\n",
      "                                              std_test_precision  \\\n",
      "params                                                             \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}            0.012051   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}            0.011922   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}            0.009343   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}            0.010896   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}            0.009823   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}            0.009560   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}            0.010453   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}            0.010090   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}            0.010429   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}            0.010493   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}            0.011615   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}            0.011211   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}            0.008268   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}            0.008915   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}            0.009052   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}            0.010167   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}            0.010624   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}            0.008863   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}            0.011609   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}            0.011472   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}            0.010997   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}            0.009561   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}            0.009907   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}            0.008245   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}            0.011852   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}            0.012886   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}            0.011956   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}            0.010645   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}            0.010310   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}            0.010049   \n",
      "...                                                          ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}            0.006431   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}            0.007933   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}            0.007938   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}            0.009305   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}            0.008669   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}            0.009031   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}            0.007625   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}            0.004502   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}            0.003969   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}            0.005229   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}            0.005105   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}            0.002676   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}            0.010742   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}            0.010318   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}            0.008505   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}            0.006220   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}            0.007509   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}            0.006425   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}            0.008390   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}            0.010773   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}            0.009386   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}            0.008249   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}            0.010155   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}            0.010617   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}             0.012965   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}             0.011158   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}             0.014001   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}             0.010371   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}             0.007395   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}             0.006865   \n",
      "\n",
      "                                              std_test_recall  \\\n",
      "params                                                          \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}         0.023738   \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}         0.023032   \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}         0.025435   \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}         0.024641   \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}         0.026169   \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}         0.024798   \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}         0.025739   \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}         0.024038   \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}         0.022015   \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}         0.022278   \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}         0.021883   \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}         0.019762   \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}         0.022042   \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}         0.022710   \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}         0.020912   \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}         0.018863   \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}         0.019042   \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}         0.016921   \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}         0.022893   \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}         0.019280   \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}         0.017858   \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}         0.019982   \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}         0.017842   \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}         0.017858   \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}         0.018708   \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}         0.018467   \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}         0.018899   \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}         0.018415   \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}         0.018546   \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}         0.017306   \n",
      "...                                                       ...   \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}         0.020819   \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}         0.020875   \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}         0.021074   \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}         0.019118   \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}         0.020084   \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}         0.017080   \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}         0.016834   \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}         0.015275   \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}         0.015589   \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}         0.016049   \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}         0.015243   \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}         0.016408   \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}         0.021785   \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}         0.017858   \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}         0.020300   \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}         0.019914   \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}         0.018309   \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}         0.019088   \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}         0.018149   \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}         0.019128   \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}         0.018821   \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}         0.019330   \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}         0.018857   \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}         0.020838   \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}          0.017923   \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}          0.015958   \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}          0.013312   \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}          0.013672   \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}          0.016110   \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}          0.017689   \n",
      "\n",
      "                                              std_test_roc_auc  \n",
      "params                                                          \n",
      "{'learning_rate': 0.01, 'n_estimators': 400}          0.004775  \n",
      "{'learning_rate': 0.01, 'n_estimators': 500}          0.004753  \n",
      "{'learning_rate': 0.01, 'n_estimators': 600}          0.004621  \n",
      "{'learning_rate': 0.01, 'n_estimators': 700}          0.004398  \n",
      "{'learning_rate': 0.01, 'n_estimators': 800}          0.004347  \n",
      "{'learning_rate': 0.01, 'n_estimators': 900}          0.004175  \n",
      "{'learning_rate': 0.02, 'n_estimators': 400}          0.004285  \n",
      "{'learning_rate': 0.02, 'n_estimators': 500}          0.004000  \n",
      "{'learning_rate': 0.02, 'n_estimators': 600}          0.003717  \n",
      "{'learning_rate': 0.02, 'n_estimators': 700}          0.003526  \n",
      "{'learning_rate': 0.02, 'n_estimators': 800}          0.003385  \n",
      "{'learning_rate': 0.02, 'n_estimators': 900}          0.003221  \n",
      "{'learning_rate': 0.03, 'n_estimators': 400}          0.003784  \n",
      "{'learning_rate': 0.03, 'n_estimators': 500}          0.003543  \n",
      "{'learning_rate': 0.03, 'n_estimators': 600}          0.003330  \n",
      "{'learning_rate': 0.03, 'n_estimators': 700}          0.003043  \n",
      "{'learning_rate': 0.03, 'n_estimators': 800}          0.002793  \n",
      "{'learning_rate': 0.03, 'n_estimators': 900}          0.002709  \n",
      "{'learning_rate': 0.04, 'n_estimators': 400}          0.003378  \n",
      "{'learning_rate': 0.04, 'n_estimators': 500}          0.003150  \n",
      "{'learning_rate': 0.04, 'n_estimators': 600}          0.002842  \n",
      "{'learning_rate': 0.04, 'n_estimators': 700}          0.002701  \n",
      "{'learning_rate': 0.04, 'n_estimators': 800}          0.002577  \n",
      "{'learning_rate': 0.04, 'n_estimators': 900}          0.002462  \n",
      "{'learning_rate': 0.05, 'n_estimators': 400}          0.003189  \n",
      "{'learning_rate': 0.05, 'n_estimators': 500}          0.002867  \n",
      "{'learning_rate': 0.05, 'n_estimators': 600}          0.002653  \n",
      "{'learning_rate': 0.05, 'n_estimators': 700}          0.002423  \n",
      "{'learning_rate': 0.05, 'n_estimators': 800}          0.002387  \n",
      "{'learning_rate': 0.05, 'n_estimators': 900}          0.002263  \n",
      "...                                                        ...  \n",
      "{'learning_rate': 0.16, 'n_estimators': 400}          0.002306  \n",
      "{'learning_rate': 0.16, 'n_estimators': 500}          0.002163  \n",
      "{'learning_rate': 0.16, 'n_estimators': 600}          0.002256  \n",
      "{'learning_rate': 0.16, 'n_estimators': 700}          0.002215  \n",
      "{'learning_rate': 0.16, 'n_estimators': 800}          0.002354  \n",
      "{'learning_rate': 0.16, 'n_estimators': 900}          0.002378  \n",
      "{'learning_rate': 0.17, 'n_estimators': 400}          0.002037  \n",
      "{'learning_rate': 0.17, 'n_estimators': 500}          0.001927  \n",
      "{'learning_rate': 0.17, 'n_estimators': 600}          0.001943  \n",
      "{'learning_rate': 0.17, 'n_estimators': 700}          0.001984  \n",
      "{'learning_rate': 0.17, 'n_estimators': 800}          0.001884  \n",
      "{'learning_rate': 0.17, 'n_estimators': 900}          0.001872  \n",
      "{'learning_rate': 0.18, 'n_estimators': 400}          0.002177  \n",
      "{'learning_rate': 0.18, 'n_estimators': 500}          0.002215  \n",
      "{'learning_rate': 0.18, 'n_estimators': 600}          0.002096  \n",
      "{'learning_rate': 0.18, 'n_estimators': 700}          0.001884  \n",
      "{'learning_rate': 0.18, 'n_estimators': 800}          0.001928  \n",
      "{'learning_rate': 0.18, 'n_estimators': 900}          0.002103  \n",
      "{'learning_rate': 0.19, 'n_estimators': 400}          0.002229  \n",
      "{'learning_rate': 0.19, 'n_estimators': 500}          0.002168  \n",
      "{'learning_rate': 0.19, 'n_estimators': 600}          0.002379  \n",
      "{'learning_rate': 0.19, 'n_estimators': 700}          0.002460  \n",
      "{'learning_rate': 0.19, 'n_estimators': 800}          0.002513  \n",
      "{'learning_rate': 0.19, 'n_estimators': 900}          0.002437  \n",
      "{'learning_rate': 0.2, 'n_estimators': 400}           0.002580  \n",
      "{'learning_rate': 0.2, 'n_estimators': 500}           0.002426  \n",
      "{'learning_rate': 0.2, 'n_estimators': 600}           0.002366  \n",
      "{'learning_rate': 0.2, 'n_estimators': 700}           0.002670  \n",
      "{'learning_rate': 0.2, 'n_estimators': 800}           0.002685  \n",
      "{'learning_rate': 0.2, 'n_estimators': 900}           0.002726  \n",
      "\n",
      "[120 rows x 10 columns]\n",
      "   prob  y\n",
      "0     0  0\n",
      "1     0  0\n",
      "2     0  0\n",
      "3     0  0\n",
      "4     0  0\n",
      "   prob  y\n",
      "0     0  0\n",
      "1     0  0\n",
      "2     0  0\n",
      "3     0  0\n",
      "4     0  0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "8 columns passed, passed data had 7 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-7ac59a8e2efe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[1;31m#                             verbose=self.verbose, pre_dispatch=self.pre_dispatch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcv_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mgird_search_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#执行主模型函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtrain_score_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtest_score_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-7ac59a8e2efe>\u001b[0m in \u001b[0;36mgird_search_model\u001b[0;34m(clf, param, name, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mscore_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_score_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#合并训练集和测试集的结果（便于展示）\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mscore_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_score_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#合并训练集和测试集的结果（便于展示）\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mscore_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'aupr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'tprweight_funtion'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'precision'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'recall'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'f1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'roc_auc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'aupr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#将结果显示为df格式，加上行列index\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'EVALUATE_METRICS:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Programfiles2\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Programfiles2\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m   7473\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   7474\u001b[0m         return _list_to_arrays(data, columns, coerce_float=coerce_float,\n\u001b[0;32m-> 7475\u001b[0;31m                                dtype=dtype)\n\u001b[0m\u001b[1;32m   7476\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   7477\u001b[0m         return _list_of_dict_to_arrays(data, columns,\n",
      "\u001b[0;32mD:\\Programfiles2\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_list_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m   7552\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_object_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   7553\u001b[0m     return _convert_object_array(content, columns, dtype=dtype,\n\u001b[0;32m-> 7554\u001b[0;31m                                  coerce_float=coerce_float)\n\u001b[0m\u001b[1;32m   7555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   7556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Programfiles2\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_convert_object_array\u001b[0;34m(content, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m   7610\u001b[0m             raise AssertionError('{col:d} columns passed, passed data had '\n\u001b[1;32m   7611\u001b[0m                                  '{con} columns'.format(col=len(columns),\n\u001b[0;32m-> 7612\u001b[0;31m                                                         con=len(content)))\n\u001b[0m\u001b[1;32m   7613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   7614\u001b[0m     \u001b[1;31m# provide soft conversion of object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 8 columns passed, passed data had 7 columns"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import auc,roc_auc_score,roc_curve,precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix,make_scorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "seed = 1231\n",
    "np.random.seed(seed)\n",
    "x_train,y_train,x_test,y_test = x_train,y_train,x_test,y_test\n",
    " \n",
    "names = [ 'xgb']\n",
    "classifiers = [XGBClassifier()]\n",
    " \n",
    "\n",
    "parameter = {'n_estimators':range(400,1000,100),'learning_rate': [i/100 for i in range(1,21)]}\n",
    " \n",
    "parameters = [parameter]\n",
    "# ,\"tprweight_funtion\":make_scorer(tprweight_funtion, greater_is_better=True)\n",
    "scoring = {'roc_auc':'roc_auc','accuracy':'accuracy', 'precision':'precision','recall':'recall','f1':'f1'}\n",
    "\n",
    "def gird_search_model(clf,param,name,x_train,y_train,x_test,y_test): #clf-classifier;param-parameter;name-classifier_name\n",
    "    model = GridSearchCV(clf,param,cv=5,verbose=2,scoring=scoring,refit='roc_auc',n_jobs=4,return_train_score=True) #GridSearchCV模型\n",
    "    fit = model.fit(x_train,y_train) #GridSearchCV模型拟合训练集数据，并返回训练器集合为fit\n",
    "    y_train_pred = fit.best_estimator_.predict(x_train) #用训练器集合中最好的estimator预测y_train_pred\n",
    "    y_test_pred = fit.best_estimator_.predict(x_test) #用训练器集合中最好的estimator预测y_test_pred\n",
    "    \n",
    "    cv_results = pd.DataFrame(fit.cv_results_).set_index(['params']) #将训练器集合fit的cv_results保存为df格式\n",
    "    cv_results_mean = cv_results[['mean_train_accuracy','mean_train_f1','mean_train_precision', 'mean_train_recall', 'mean_train_roc_auc',\n",
    "                                  'mean_test_accuracy', 'mean_test_f1', 'mean_test_precision', 'mean_test_recall','mean_test_roc_auc']] #cv_results中的各个score的mean\n",
    "    cv_results_std = cv_results[['std_train_accuracy', 'std_train_f1', 'std_train_precision','std_train_recall', 'std_train_roc_auc',\n",
    "                                 'std_test_accuracy', 'std_test_f1','std_test_precision', 'std_test_recall', 'std_test_roc_auc']] #cv_results中的各个score的std\n",
    " \n",
    "#  -------------------模型结果展示------------------------------------------------------  \n",
    "  \n",
    "    print('MODEL : %r' % name)\n",
    "    print('Best cv_test_roc_auc: %f using %s' % (fit.best_score_,fit.best_params_)) #训练器集合fit中最好的模型得到的：best_score和best_params\n",
    "    print(cv_results_mean)\n",
    "    print(cv_results_std)\n",
    "    \n",
    "    train_score_list = []\n",
    "    test_score_list = []\n",
    "    score_list = []\n",
    "    model_metrics_name = [accuracy_score,tprweight_funtion,precision_score,recall_score,f1_score,roc_auc_score,aupr] #模型评价指标，与scoreing相对应\n",
    "    for matrix in model_metrics_name: #计算各个模型评价指标\n",
    "        train_score = matrix(y_train,y_train_pred) #计算训练集的\n",
    "        test_score = matrix(y_test,y_test_pred) #计算测试集的\n",
    "        train_score_list.append(train_score) #把训练集的各个模型指标放在同一行\n",
    "        test_score_list.append(test_score) #把测试集的各个模型指标放在同一行\n",
    "    score_list.append(train_score_list) #合并训练集和测试集的结果（便于展示）\n",
    "    score_list.append(test_score_list) #合并训练集和测试集的结果（便于展示）\n",
    "    score_df = pd.DataFrame(score_list,index = ['train','test'],columns = ['accuracy','tprweight_funtion','precision','recall','f1','roc_auc','aupr']) #将结果显示为df格式，加上行列index\n",
    "    print('EVALUATE_METRICS:')\n",
    "    print(score_df)   \n",
    "    return cv_results,score_list,y_train_pred,y_test_pred\n",
    "\n",
    "train_score_list = []\n",
    "test_score_list = []\n",
    "y_train_pred_list = []\n",
    "y_test_pred_list = []\n",
    "# parallel = Parallel(n_jobs=self.n_jobs, backend=\"multiprocessing\",\n",
    "#                             verbose=self.verbose, pre_dispatch=self.pre_dispatch)\n",
    "for clf,param,name in zip(classifiers,parameters,names):\n",
    "    cv_result,score_list,y_train_pred,y_test_pred =  gird_search_model(clf,param,name,x_train,y_train,x_test,y_test) #执行主模型函数\n",
    "    train_score_list.append(score_list[0])\n",
    "    test_score_list.append(score_list[1])\n",
    "    y_train_pred_list.append(y_train_pred)\n",
    "    y_test_pred_list.append(y_test_pred)\n",
    "    print('-------------------------------------------------------------------------------------------------------------------------------')\n",
    "train_score_df = pd.DataFrame(train_score_list,index=names,columns=['acc','f1','pre','rec','roc_auc','aupr'])\n",
    "test_score_df = pd.DataFrame(test_score_list,index=names,columns=['acc','pre','rec','f1','roc_auc','aupr'])\n",
    "print('TRAIN_SCORE:')\n",
    "print(train_score_df)\n",
    "print()\n",
    "print('TEST_SCORE:')\n",
    "print(test_score_df)\n",
    "\n",
    "# 'n_estimators': [400, 500, 600, 700, 800]\n",
    "#     'max_depth':range(3,10,2),  \n",
    "#     'min_child_weight':range(1,6,2)\n",
    "# 'min_child_weight': [6, 8, 10, 12]  \n",
    "# 'gamma': [i / 10.0 for i in range(0, 5)]  \n",
    "#     'subsample': [i / 10.0 for i in range(6, 10)],  \n",
    "#     'colsample_bytree': [i / 10.0 for i in range(6, 10)]  \n",
    "\n",
    "# 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]  \n",
    "# fir\n",
    "# 'reg_alpha': [0, 0.001, 0.005, 0.01,0.03, 0.05] \n",
    "# sec\n",
    "# 'reg_alpha': [0, 0.001,0.003,0.01,0.03, 0.05, 0.1,0.3 ]\n",
    "# 'reg_alpha': [0, 0.001, 0.005, 0.01,0.03, 0.05]  \n",
    "# 0.03\n",
    "# 'reg_lambda': [1e-5, 1e-2, 0.1, 1, 100] \n",
    "# 'reg_lambda': [0, 0.1, 0.3, 1,3,10,30] \n",
    "# 这个先不调\n",
    "# Step 6: Reducing Learning Rate in order to reduce overfit\n",
    "# param_test6 = {\n",
    "# 'reg_alpha': [0, 0.001,0.003,0.01,0.03, 0.05, 0.1,0.3 ]\n",
    "# }\n",
    "# gsearch6 =GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=700, \n",
    "#                                                   max_depth = 3,\n",
    "#                                                   min_child_weight =1,\n",
    "#                                                   gamma=0.3, \n",
    "#                                                  reg_alpha = 0.03,\n",
    "#                                                   subsample=0.8, colsample_bytree=0.6,\n",
    "#                                                   objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "#                         param_grid = param_test6, \n",
    "#                         scoring='roc_auc',\n",
    "# #                         eval_metric=tpr_weight_funtion,\n",
    "#                         n_jobs=4,\n",
    "#                         iid=False, \n",
    "#                         cv=5)\n",
    "'learning_rate': 0.15, 'n_estimators': 400\n",
    "mean_train_accuracy mean_train_f1 mean_train_precision mean_train_recall mean_train_roc_auc\n",
    "mean_test_accuracy mean_test_f1 mean_test_precision mean_test_recall mean_test_roc_auc std_train_accuracy\n",
    "std_train_f1 std_train_precision std_train_recall std_train_roc_auc std_test_accuracy std_test_f1 std_test_precision\n",
    "std_test_recall std_test_roc_auc\n",
    "# gsearch6.fit(train_X,train_y)\n",
    "# gsearch6.cv_results_, gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from hyperopt import fmin, tpe, hp, rand\n",
    "# # import numpy as np\n",
    "# # from sklearn.metrics import accuracy_score\n",
    "# # from sklearn import svm\n",
    "# # from sklearn import datasets\n",
    "\n",
    "# # # SVM的三个超参数：C为惩罚因子，kernel为核函数类型，gamma为核函数的额外参数（对于不同类型的核函数有不同的含义）\n",
    "# # # 有别于传统的网格搜索（GridSearch），这里只需要给出最优参数的概率分布即可，而不需要按照步长把具体的值给一个个枚举出来\n",
    "\n",
    "# # x_train\n",
    "# # y_train\n",
    "# # x_test\n",
    "# # y_test\n",
    "   \n",
    "\n",
    "# parameter_space_svc ={\n",
    "#                         \"n_estimators\":400,\n",
    "#                          'max_depth':hp.choice('max_depth', [3,5,7,9]),  \n",
    "#                         \"objective\":\"binary:logistic\",\n",
    "#                         \"learning_rate\":0.15, \n",
    "#                         \"subsample\":0.8,\n",
    "#                         \"max_delta_step\" :1,\n",
    "#                         \"min_child_weight\":hp.choice('min_child_weight', [1,3,5,7]),\n",
    "#                         \"colsample_bytree\":0.6,\n",
    "# #                         scale_pos_weight=scale_pos_weight,\n",
    "#                         \"fpreproc\":fpreproc,\n",
    "#                         \"gamma\":0.3,\n",
    "#       \"reg_alpha\" : 0.03,\n",
    "#     \"reg_lambda\":1\n",
    "# }\n",
    "\n",
    "# # # 鸢尾花卉数据集，是一类多重变量分析的数据集\n",
    "# # # 通过花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性预测鸢尾花卉属于（Setosa，Versicolour，Virginica）三个种类中的哪一类\n",
    "# # iris = datasets.load_digits()\n",
    "\n",
    "# # #--------------------划分训练集和测试集--------------------\n",
    "# # train_data = iris.data[0:1300]\n",
    "# # train_target = iris.target[0:1300]\n",
    "# # test_data = iris.data[1300:-1]\n",
    "# # test_target = iris.target[1300:-1]\n",
    "# # #-----------------------------------------------------------\n",
    "\n",
    "# # # 计数器，每一次参数组合的枚举都会使它加1\n",
    "# count = 0\n",
    "\n",
    "# def function(args):\n",
    "#     print(args)\n",
    "\n",
    "#     # **可以把dict转换为关键字参数，可以大大简化复杂的函数调用\n",
    "#     clf = XGBClassifier(**args)\n",
    "\n",
    "#     # 训练模型\n",
    "#     clf.fit(x_train,y_train)\n",
    "\n",
    "#     # 预测测试集\n",
    "#     prediction = clf.predict(x_test)\n",
    "\n",
    "#     global count\n",
    "#     count = count + 1\n",
    "#     score = tprweight_funtion(y_test,prediction)\n",
    "#     print(\"第%s次，测试集正确率为：\" % str(count),score)\n",
    "\n",
    "#     # 由于hyperopt仅提供fmin接口，因此如果要求最大值，则需要取相反数\n",
    "#     return score\n",
    "\n",
    "# # # algo指定搜索算法，目前支持以下算法：\n",
    "# # # ①随机搜索(hyperopt.rand.suggest)\n",
    "# # # ②模拟退火(hyperopt.anneal.suggest)\n",
    "# # # ③TPE算法（hyperopt.tpe.suggest，算法全称为Tree-structured Parzen Estimator Approach）\n",
    "# # # max_evals指定枚举次数上限，即使第max_evals次枚举仍未能确定全局最优解，也要结束搜索，返回目前搜索到的最优解\n",
    "# best = fmin(function, parameter_space_svc, algo=tpe.suggest, max_evals=30)\n",
    "\n",
    "# # best[\"kernel\"]返回的是数组下标，因此需要把它还原回来\n",
    "# # kernel_list = ['rbf','poly']\n",
    "# # best[\"kernel\"] = kernel_list[best[\"kernel\"]]\n",
    "\n",
    "# print(\"最佳参数为：\",best)\n",
    "\n",
    "# # clf = svm.SVC(**best)\n",
    "# # print(clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameter_space_svc ={\n",
    "                        \"n_estimators\":400,\n",
    "                         'max_depth':,  \n",
    "                        \"objective\":\"binary:logistic\",\n",
    "                        \"learning_rate\":0.15, \n",
    "                        \"subsample\":0.8,\n",
    "                        \"max_delta_step\" :1,\n",
    "                        \"min_child_weight\":,\n",
    "                        \"colsample_bytree\":0.6,\n",
    "#                         scale_pos_weight=scale_pos_weight,\n",
    "                        \"fpreproc\":fpreproc,\n",
    "                        \"gamma\":hp.choice('max_depth', [i / 10.0 for i in range(0, 7)]),  \n",
    "      \"reg_alpha\" : 0.03,\n",
    "    \"reg_lambda\":1\n",
    "}\n",
    "\n",
    "best = fmin(function, parameter_space_svc, algo=tpe.suggest, max_evals=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for clf_name,y_train_pred,y_test_pred in zip(names,y_train_pred_list,y_test_pred_list):\n",
    "    show_curve(y_train,y_train_pred,clf_name,True)\n",
    "    show_curve(y_test,y_test_pred,clf_name,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def show_roc(roc_auc,fpr,tpr):\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--',label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def show_roc_pr(roc_aupr,recall,precision):\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--',label='ROC curve (area = %0.2f)' % roc_aupr)\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('ROC_PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    print()\n",
    "def show_curve(y_true,y_pred,clf_name,train=True):\n",
    "    fpr, tpr, thresholds1 = roc_curve(y_true,y_pred)\n",
    "    precision, recall, thresholds2 = precision_recall_curve(y_true,y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_aupr = auc(recall,precision) \n",
    "    if train == True:\n",
    "        print('%s  (%s)' %(clf_name,\"train\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prob  y\n",
      "0     0  0\n",
      "1     0  0\n",
      "2     0  0\n",
      "3     0  0\n",
      "4     0  0\n",
      "Performance sur le train :  0.6857943925233645\n",
      "   prob  y\n",
      "0     0  0\n",
      "1     0  0\n",
      "2     0  0\n",
      "3     0  0\n",
      "4     0  0\n",
      "Performance sur le test :  0.5146046511627906\n"
     ]
    }
   ],
   "source": [
    "#  0.47710951526032314\n",
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label == 1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    return (dtrain, dtest, param)\n",
    "# reg_alpha = 0.01\n",
    "\n",
    "# reg_alpha = 0.01\n",
    "# 0.69 0.46615\n",
    "# 'reg_alpha': [0, 0.001, 0.005, 0.01,0.03, 0.05] \n",
    "#  0.701  0.4947\n",
    "# 0.70 0.49\n",
    "\n",
    "xgb5 = XGBClassifier(\n",
    "    learning_rate =0.15, \n",
    "    n_estimators=400,\n",
    "    gamma=0.3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.6,\n",
    "    max_depth = 3,\n",
    "    reg_alpha = 0.03,\n",
    "    reg_lambda=1,\n",
    "    min_child_weight = 1,\n",
    "    objective= 'binary:logistic', nthread=4,fpreproc=fpreproc,seed=27\n",
    ")\n",
    "# modelfit(xgb1, train, train_X)\n",
    "xgb5.fit(x_train, y_train)\n",
    "# x_train\n",
    "# y_train\n",
    "# x_test\n",
    "# y_test\n",
    "# Performance sur le train\n",
    "auc = tprweight_funtion(y_train, xgb5.predict(x_train))\n",
    "print(\"Performance sur le train : \", auc)\n",
    "\n",
    "# Performance sur le test\n",
    "auc = tprweight_funtion(y_test, xgb5.predict(x_test))\n",
    "print(\"Performance sur le test : \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调参2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 5\n",
    "kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label == 1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    return (dtrain, dtest, param)\n",
    "MAX_ROUNDS = 400\n",
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE =0.15\n",
    "EARLY_STOPPING_ROUNDS = 50  \n",
    "# scale_pos_weight = float(np.sum(train_features[\"Tag\"] == 0)) / np.sum(train_features[\"Tag\"] == 1)\n",
    "\n",
    "# parameter_space_svc ={\n",
    "#                         \"n_estimators\":400,\n",
    "#                          'max_depth':hp.choice('max_depth', [3,5,7,9]),  \n",
    "#                         \"objective\":\"binary:logistic\",\n",
    "#                         \"learning_rate\":0.15, \n",
    "#                         \"subsample\":0.8,\n",
    "#                         \"max_delta_step\" :1,\n",
    "#                         \"min_child_weight\":hp.choice('min_child_weight', [1,3,5,7]),\n",
    "#                         \"colsample_bytree\":0.6,\n",
    "# #                         scale_pos_weight=scale_pos_weight,\n",
    "#                         \"fpreproc\":fpreproc,\n",
    "#                         \"gamma\":0.3,\n",
    "#       \"reg_alpha\" : 0.03,\n",
    "#     \"reg_lambda\":1\n",
    "# }\n",
    "'max_depth', [3,5,7,9]\n",
    "'min_child_weight', [1,3,5,7]\n",
    "import itertools\n",
    "# gamma\":hp.choice('gamma', [i / 10.0 for i in range(0, 7)])\n",
    "\n",
    "max_depths = [3,5,7,9]\n",
    "min_child_weights = [1,3,5,7]\n",
    "paraCo = list(itertools.product(max_depths,min_child_weights))\n",
    "\n",
    "#  learning_rate =\n",
    "#     n_estimators=,\n",
    "#     gamma=,\n",
    "#     subsample=,\n",
    "#     colsample_bytree=0.8,\n",
    "#     max_depth = 5,\n",
    "#     min_child_weight = 5,\n",
    "#     objective= 'binary:logistic', nthread=4,scale_pos_weight=1,seed=27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "parameters = {'nthread' : [-1],\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.1], #so called `eta` value\n",
    "              \"subsample\":[1] ,\n",
    "              'max_depth':[3], #[6,14],\n",
    "              \"min_child_weight\":[1],\n",
    "              'min_child_weight': [1], # [1,7,13],\n",
    "              \"colsample_bytree\":[1],\n",
    "              'silent': [1],\n",
    "              \"gamma\":[0] ,\n",
    "              \"reg_alpha\" :[0],\n",
    "              \"reg_lambda\":[1],\n",
    "              'n_estimators': [400,500,600,700], #number of trees, change it to 1000 for better results\n",
    "              'seed': [1337]}\n",
    "product = [x for x in itertools.product(*list(parameters.values())) ]\n",
    "g = [dict((zip(parameters.keys(), p))) for p in product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'subsample': 1, 'seed': 1337, 'silent': 1, 'colsample_bytree': 1, 'min_child_weight': 1, 'nthread': -1, 'gamma': 0, 'learning_rate': 0.1, 'reg_alpha': 0, 'n_estimators': 400, 'reg_lambda': 1, 'objective': 'binary:logistic'}\n",
      "{'max_depth': 3, 'subsample': 1, 'seed': 1337, 'silent': 1, 'colsample_bytree': 1, 'min_child_weight': 1, 'nthread': -1, 'gamma': 0, 'learning_rate': 0.1, 'reg_alpha': 0, 'n_estimators': 500, 'reg_lambda': 1, 'objective': 'binary:logistic'}\n",
      "{'max_depth': 3, 'subsample': 1, 'seed': 1337, 'silent': 1, 'colsample_bytree': 1, 'min_child_weight': 1, 'nthread': -1, 'gamma': 0, 'learning_rate': 0.1, 'reg_alpha': 0, 'n_estimators': 600, 'reg_lambda': 1, 'objective': 'binary:logistic'}\n",
      "{'max_depth': 3, 'subsample': 1, 'seed': 1337, 'silent': 1, 'colsample_bytree': 1, 'min_child_weight': 1, 'nthread': -1, 'gamma': 0, 'learning_rate': 0.1, 'reg_alpha': 0, 'n_estimators': 700, 'reg_lambda': 1, 'objective': 'binary:logistic'}\n"
     ]
    }
   ],
   "source": [
    "# [dict(zip(parameters.keys(), p)) for p in product]\n",
    "for m in g:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([[1], [1337], [1], [1], [1], [-1], [0], [0.1], [0], ['binary:logistic'], [400, 500, 600, 700], [1], [3]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "       prob  y\n",
      "0  0.004462  0\n",
      "1  0.006660  0\n",
      "2  0.000014  0\n",
      "3  0.013395  0\n",
      "4  0.000098  0\n",
      "  Gini =  0.7853113983548766\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.583174  1\n",
      "1  0.003746  0\n",
      "2  0.000086  0\n",
      "3  0.000015  0\n",
      "4  0.000359  0\n",
      "  Gini =  0.7824009324009324\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000113  0\n",
      "1  0.000047  0\n",
      "2  0.000131  0\n",
      "3  0.000512  0\n",
      "4  0.035099  0\n",
      "  Gini =  0.7850402761795168\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.000498  0\n",
      "1  0.000049  0\n",
      "2  0.007041  0\n",
      "3  0.998215  1\n",
      "4  0.000012  0\n",
      "  Gini =  0.7625145518044237\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000367  0\n",
      "1  0.000028  0\n",
      "2  0.000141  0\n",
      "3  0.000056  0\n",
      "4  0.000128  0\n",
      "  Gini =  0.7982311320754718\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.583174  1\n",
      "1  0.000498  0\n",
      "2  0.004462  0\n",
      "3  0.003746  0\n",
      "4  0.000086  0\n",
      "para:max_depth:3, min_child_weight:1, score:0.7834539089848308\n",
      "\n",
      "Fold  0\n",
      "       prob  y\n",
      "0  0.004925  0\n",
      "1  0.007772  0\n",
      "2  0.000019  0\n",
      "3  0.019425  0\n",
      "4  0.000125  0\n",
      "  Gini =  0.7901292596944771\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.693775  1\n",
      "1  0.000843  0\n",
      "2  0.000144  0\n",
      "3  0.000039  0\n",
      "4  0.000147  0\n",
      "  Gini =  0.77016317016317\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000086  0\n",
      "1  0.000049  0\n",
      "2  0.000264  0\n",
      "3  0.000434  0\n",
      "4  0.010218  0\n",
      "  Gini =  0.7819332566168009\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.001705  0\n",
      "1  0.000065  0\n",
      "2  0.006490  0\n",
      "3  0.999698  1\n",
      "4  0.000066  0\n",
      "  Gini =  0.7664726426076833\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000516  0\n",
      "1  0.000061  0\n",
      "2  0.000123  0\n",
      "3  0.000170  0\n",
      "4  0.001348  0\n",
      "  Gini =  0.7998820754716982\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.693775  1\n",
      "1  0.001705  0\n",
      "2  0.004925  0\n",
      "3  0.000843  0\n",
      "4  0.000144  0\n",
      "para:max_depth:3, min_child_weight:3, score:0.7844574095682614\n",
      "\n",
      "Fold  0\n",
      "       prob  y\n",
      "0  0.005692  0\n",
      "1  0.004455  0\n",
      "2  0.000012  0\n",
      "3  0.013010  0\n",
      "4  0.000094  0\n",
      "  Gini =  0.7799059929494713\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.606693  1\n",
      "1  0.001143  0\n",
      "2  0.000069  0\n",
      "3  0.000028  0\n",
      "4  0.000341  0\n",
      "  Gini =  0.7720279720279719\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000161  0\n",
      "1  0.000043  0\n",
      "2  0.000211  0\n",
      "3  0.000635  0\n",
      "4  0.042684  0\n",
      "  Gini =  0.7902186421173762\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.000970  0\n",
      "1  0.000032  0\n",
      "2  0.009686  0\n",
      "3  0.999429  1\n",
      "4  0.000023  0\n",
      "  Gini =  0.7681024447031432\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000604  0\n",
      "1  0.000055  0\n",
      "2  0.000105  0\n",
      "3  0.000070  0\n",
      "4  0.000567  0\n",
      "  Gini =  0.7976415094339623\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.606693  1\n",
      "1  0.000970  0\n",
      "2  0.005692  0\n",
      "3  0.001143  0\n",
      "4  0.000069  0\n",
      "para:max_depth:3, min_child_weight:5, score:0.7849474912485414\n",
      "\n",
      "Fold  0\n",
      "       prob  y\n",
      "0  0.002353  0\n",
      "1  0.007448  0\n",
      "2  0.000027  0\n",
      "3  0.010219  0\n",
      "4  0.000088  0\n",
      "  Gini =  0.7866039952996474\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.623858  1\n",
      "1  0.000873  0\n",
      "2  0.000090  0\n",
      "3  0.000010  0\n",
      "4  0.000224  0\n",
      "  Gini =  0.774941724941725\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000106  0\n",
      "1  0.000046  0\n",
      "2  0.000313  0\n",
      "3  0.000725  0\n",
      "4  0.047484  0\n",
      "  Gini =  0.7823935558112773\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.001037  0\n",
      "1  0.000016  0\n",
      "2  0.006569  0\n",
      "3  0.998828  1\n",
      "4  0.000019  0\n",
      "  Gini =  0.7670547147846334\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000397  0\n",
      "1  0.000045  0\n",
      "2  0.000095  0\n",
      "3  0.000306  0\n",
      "4  0.000774  0\n",
      "  Gini =  0.8090801886792452\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.623858  1\n",
      "1  0.001037  0\n",
      "2  0.002353  0\n",
      "3  0.000873  0\n",
      "4  0.000090  0\n",
      "para:max_depth:3, min_child_weight:7, score:0.7853908984830805\n",
      "\n",
      "Fold  0\n",
      "       prob  y\n",
      "0  0.000815  0\n",
      "1  0.001700  0\n",
      "2  0.000003  0\n",
      "3  0.001265  0\n",
      "4  0.000037  0\n",
      "  Gini =  0.8038777908343127\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.593561  1\n",
      "1  0.000343  0\n",
      "2  0.000069  0\n",
      "3  0.000002  0\n",
      "4  0.000009  0\n",
      "  Gini =  0.7722610722610723\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000005  0\n",
      "1  0.000013  0\n",
      "2  0.000220  0\n",
      "3  0.000103  0\n",
      "4  0.000688  0\n",
      "  Gini =  0.795627157652474\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.000101  0\n",
      "1  0.000004  0\n",
      "2  0.000258  0\n",
      "3  0.999914  1\n",
      "4  0.000008  0\n",
      "  Gini =  0.7868451688009314\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000031  0\n",
      "1  0.000023  0\n",
      "2  0.000104  0\n",
      "3  0.000006  0\n",
      "4  0.000094  0\n",
      "  Gini =  0.8147405660377358\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.593561  1\n",
      "1  0.000101  0\n",
      "2  0.000815  0\n",
      "3  0.000343  0\n",
      "4  0.000069  0\n",
      "para:max_depth:5, min_child_weight:1, score:0.7949124854142358\n",
      "\n",
      "Fold  0\n",
      "           prob  y\n",
      "0  1.476261e-03  0\n",
      "1  1.338912e-03  0\n",
      "2  8.333944e-07  0\n",
      "3  5.974968e-04  0\n",
      "4  1.362415e-05  0\n",
      "  Gini =  0.7945945945945946\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.643132  1\n",
      "1  0.000606  0\n",
      "2  0.000018  0\n",
      "3  0.000003  0\n",
      "4  0.000016  0\n",
      "  Gini =  0.7733100233100233\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000017  0\n",
      "1  0.000008  0\n",
      "2  0.000420  0\n",
      "3  0.000169  0\n",
      "4  0.001732  0\n",
      "  Gini =  0.7889528193325661\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.000240  0\n",
      "1  0.000002  0\n",
      "2  0.000763  0\n",
      "3  0.999864  1\n",
      "4  0.000010  0\n",
      "  Gini =  0.7816065192083819\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000177  0\n",
      "1  0.000048  0\n",
      "2  0.000092  0\n",
      "3  0.000027  0\n",
      "4  0.000063  0\n",
      "  Gini =  0.820872641509434\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.643132  1\n",
      "1  0.000240  0\n",
      "2  0.001476  0\n",
      "3  0.000606  0\n",
      "4  0.000018  0\n",
      "para:max_depth:5, min_child_weight:3, score:0.7931855309218203\n",
      "\n",
      "Fold  0\n",
      "       prob  y\n",
      "0  0.005050  0\n",
      "1  0.001820  0\n",
      "2  0.000002  0\n",
      "3  0.001373  0\n",
      "4  0.000018  0\n",
      "  Gini =  0.7950646298472385\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.629230  1\n",
      "1  0.000396  0\n",
      "2  0.000032  0\n",
      "3  0.000006  0\n",
      "4  0.000112  0\n",
      "  Gini =  0.7738927738927739\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000020  0\n",
      "1  0.000011  0\n",
      "2  0.000099  0\n",
      "3  0.000163  0\n",
      "4  0.007815  0\n",
      "  Gini =  0.7922899884925202\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.000349  0\n",
      "1  0.000001  0\n",
      "2  0.000507  0\n",
      "3  0.999814  1\n",
      "4  0.000011  0\n",
      "  Gini =  0.7769499417927823\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000030  0\n",
      "1  0.000053  0\n",
      "2  0.000060  0\n",
      "3  0.000057  0\n",
      "4  0.000071  0\n",
      "  Gini =  0.8142688679245284\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.629230  1\n",
      "1  0.000349  0\n",
      "2  0.005050  0\n",
      "3  0.000396  0\n",
      "4  0.000032  0\n",
      "para:max_depth:5, min_child_weight:5, score:0.7944224037339556\n",
      "\n",
      "Fold  0\n",
      "       prob  y\n",
      "0  0.001709  0\n",
      "1  0.006792  0\n",
      "2  0.000002  0\n",
      "3  0.002965  0\n",
      "4  0.000033  0\n",
      "  Gini =  0.7934195064629846\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.484498  1\n",
      "1  0.000621  0\n",
      "2  0.000022  0\n",
      "3  0.000006  0\n",
      "4  0.000053  0\n",
      "  Gini =  0.7775058275058275\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000010  0\n",
      "1  0.000014  0\n",
      "2  0.000133  0\n",
      "3  0.000186  0\n",
      "4  0.003793  0\n",
      "  Gini =  0.7861910241657077\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.000175  0\n",
      "1  0.000005  0\n",
      "2  0.000728  0\n",
      "3  0.999765  1\n",
      "4  0.000008  0\n",
      "  Gini =  0.7727590221187427\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000061  0\n",
      "1  0.000060  0\n",
      "2  0.000089  0\n",
      "3  0.000029  0\n",
      "4  0.000156  0\n",
      "  Gini =  0.8161556603773585\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.484498  1\n",
      "1  0.000175  0\n",
      "2  0.001709  0\n",
      "3  0.000621  0\n",
      "4  0.000022  0\n",
      "para:max_depth:5, min_child_weight:7, score:0.791901983663944\n",
      "\n",
      "Fold  0\n",
      "       prob  y\n",
      "0  0.000579  0\n",
      "1  0.000775  0\n",
      "2  0.000002  0\n",
      "3  0.000135  0\n",
      "4  0.000007  0\n",
      "  Gini =  0.800235017626322\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.566623  1\n",
      "1  0.000086  0\n",
      "2  0.000015  0\n",
      "3  0.000005  0\n",
      "4  0.000019  0\n",
      "  Gini =  0.7763403263403263\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000002  0\n",
      "1  0.000004  0\n",
      "2  0.000076  0\n",
      "3  0.000076  0\n",
      "4  0.002101  0\n",
      "  Gini =  0.7944764096662831\n",
      "\n",
      "Fold  3\n",
      "           prob  y\n",
      "0  2.012794e-04  0\n",
      "1  5.685741e-07  0\n",
      "2  1.556092e-04  0\n",
      "3  9.999634e-01  1\n",
      "4  1.273949e-05  0\n",
      "  Gini =  0.7878928987194412\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000010  0\n",
      "1  0.000012  0\n",
      "2  0.000014  0\n",
      "3  0.000011  0\n",
      "4  0.000005  0\n",
      "  Gini =  0.8183962264150944\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.566623  1\n",
      "1  0.000201  0\n",
      "2  0.000579  0\n",
      "3  0.000086  0\n",
      "4  0.000015  0\n",
      "para:max_depth:7, min_child_weight:1, score:0.7968028004667445\n",
      "\n",
      "Fold  0\n",
      "           prob  y\n",
      "0  6.449294e-04  0\n",
      "1  1.056400e-03  0\n",
      "2  4.267697e-07  0\n",
      "3  3.344928e-04  0\n",
      "4  5.930831e-06  0\n",
      "  Gini =  0.7989424206815512\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.437658  1\n",
      "1  0.000325  0\n",
      "2  0.000010  0\n",
      "3  0.000001  0\n",
      "4  0.000012  0\n",
      "  Gini =  0.7763403263403263\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000003  0\n",
      "1  0.000004  0\n",
      "2  0.000055  0\n",
      "3  0.000112  0\n",
      "4  0.003088  0\n",
      "  Gini =  0.7937859608745685\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.000166  0\n",
      "1  0.000004  0\n",
      "2  0.001627  0\n",
      "3  0.999871  1\n",
      "4  0.000007  0\n",
      "  Gini =  0.7826542491268917\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000007  0\n",
      "1  0.000044  0\n",
      "2  0.000012  0\n",
      "3  0.000032  0\n",
      "4  0.000010  0\n",
      "  Gini =  0.8225235849056604\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.437658  1\n",
      "1  0.000166  0\n",
      "2  0.000645  0\n",
      "3  0.000325  0\n",
      "4  0.000010  0\n",
      "para:max_depth:7, min_child_weight:3, score:0.7966161026837806\n",
      "\n",
      "Fold  0\n",
      "       prob  y\n",
      "0  0.000948  0\n",
      "1  0.000881  0\n",
      "2  0.000003  0\n",
      "3  0.001053  0\n",
      "4  0.000029  0\n",
      "  Gini =  0.807990599294947\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.402701  1\n",
      "1  0.000266  0\n",
      "2  0.000024  0\n",
      "3  0.000001  0\n",
      "4  0.000226  0\n",
      "  Gini =  0.7771561771561771\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000011  0\n",
      "1  0.000001  0\n",
      "2  0.000133  0\n",
      "3  0.000188  0\n",
      "4  0.003192  0\n",
      "  Gini =  0.7869965477560414\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.000320  0\n",
      "1  0.000005  0\n",
      "2  0.000174  0\n",
      "3  0.999955  1\n",
      "4  0.000003  0\n",
      "  Gini =  0.779627473806752\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000014  0\n",
      "1  0.000010  0\n",
      "2  0.000054  0\n",
      "3  0.000023  0\n",
      "4  0.000098  0\n",
      "  Gini =  0.8221698113207547\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.402701  1\n",
      "1  0.000320  0\n",
      "2  0.000948  0\n",
      "3  0.000266  0\n",
      "4  0.000024  0\n",
      "para:max_depth:7, min_child_weight:5, score:0.7950291715285881\n",
      "\n",
      "Fold  0\n",
      "       prob  y\n",
      "0  0.001211  0\n",
      "1  0.000732  0\n",
      "2  0.000004  0\n",
      "3  0.001598  0\n",
      "4  0.000021  0\n",
      "  Gini =  0.8039952996474736\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.359057  1\n",
      "1  0.000256  0\n",
      "2  0.000026  0\n",
      "3  0.000004  0\n",
      "4  0.000078  0\n",
      "  Gini =  0.7752913752913753\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000013  0\n",
      "1  0.000023  0\n",
      "2  0.000052  0\n",
      "3  0.000117  0\n",
      "4  0.018430  0\n",
      "  Gini =  0.7940161104718066\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.000132  0\n",
      "1  0.000003  0\n",
      "2  0.001115  0\n",
      "3  0.999967  1\n",
      "4  0.000021  0\n",
      "  Gini =  0.7819557625145519\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000017  0\n",
      "1  0.000028  0\n",
      "2  0.000068  0\n",
      "3  0.000014  0\n",
      "4  0.000049  0\n",
      "  Gini =  0.8119103773584906\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.359057  1\n",
      "1  0.000132  0\n",
      "2  0.001211  0\n",
      "3  0.000256  0\n",
      "4  0.000026  0\n",
      "para:max_depth:7, min_child_weight:7, score:0.7946091015169195\n",
      "\n",
      "Fold  0\n",
      "           prob  y\n",
      "0  9.009367e-04  0\n",
      "1  5.288938e-04  0\n",
      "2  6.795750e-07  0\n",
      "3  5.922653e-04  0\n",
      "4  1.411470e-05  0\n",
      "  Gini =  0.8050528789659225\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.441736  1\n",
      "1  0.000071  0\n",
      "2  0.000017  0\n",
      "3  0.000001  0\n",
      "4  0.000119  0\n",
      "  Gini =  0.7728438228438228\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000004  0\n",
      "1  0.000006  0\n",
      "2  0.000152  0\n",
      "3  0.000107  0\n",
      "4  0.001686  0\n",
      "  Gini =  0.7874568469505179\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.000445  0\n",
      "1  0.000002  0\n",
      "2  0.000184  0\n",
      "3  0.999959  1\n",
      "4  0.000007  0\n",
      "  Gini =  0.7912689173457509\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000002  0\n",
      "1  0.000010  0\n",
      "2  0.000018  0\n",
      "3  0.000011  0\n",
      "4  0.000005  0\n",
      "  Gini =  0.8178066037735848\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.441736  1\n",
      "1  0.000445  0\n",
      "2  0.000901  0\n",
      "3  0.000071  0\n",
      "4  0.000017  0\n",
      "para:max_depth:9, min_child_weight:1, score:0.7964760793465577\n",
      "\n",
      "Fold  0\n",
      "           prob  y\n",
      "0  6.445635e-04  0\n",
      "1  6.855394e-04  0\n",
      "2  7.074743e-07  0\n",
      "3  3.148561e-04  0\n",
      "4  5.849678e-06  0\n",
      "  Gini =  0.8048178613396005\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.650997  1\n",
      "1  0.000668  0\n",
      "2  0.000020  0\n",
      "3  0.000002  0\n",
      "4  0.000093  0\n",
      "  Gini =  0.7800699300699301\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000005  0\n",
      "1  0.000006  0\n",
      "2  0.000024  0\n",
      "3  0.000117  0\n",
      "4  0.000691  0\n",
      "  Gini =  0.79102416570771\n",
      "\n",
      "Fold  3\n",
      "           prob  y\n",
      "0  2.161189e-04  0\n",
      "1  6.074227e-07  0\n",
      "2  2.597084e-04  0\n",
      "3  9.999272e-01  1\n",
      "4  2.745302e-05  0\n",
      "  Gini =  0.7790454016298021\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000007  0\n",
      "1  0.000007  0\n",
      "2  0.000009  0\n",
      "3  0.000007  0\n",
      "4  0.000012  0\n",
      "  Gini =  0.8194575471698113\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.650997  1\n",
      "1  0.000216  0\n",
      "2  0.000645  0\n",
      "3  0.000668  0\n",
      "4  0.000020  0\n",
      "para:max_depth:9, min_child_weight:3, score:0.7966861143523921\n",
      "\n",
      "Fold  0\n",
      "       prob  y\n",
      "0  0.000358  0\n",
      "1  0.000665  0\n",
      "2  0.000001  0\n",
      "3  0.001153  0\n",
      "4  0.000007  0\n",
      "  Gini =  0.8044653349001175\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.290322  1\n",
      "1  0.000115  0\n",
      "2  0.000011  0\n",
      "3  0.000005  0\n",
      "4  0.000025  0\n",
      "  Gini =  0.7793706293706293\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000005  0\n",
      "1  0.000015  0\n",
      "2  0.000030  0\n",
      "3  0.000068  0\n",
      "4  0.003804  0\n",
      "  Gini =  0.7953970080552358\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.000106  0\n",
      "1  0.000003  0\n",
      "2  0.000113  0\n",
      "3  0.999871  1\n",
      "4  0.000005  0\n",
      "  Gini =  0.7828870779976718\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000016  0\n",
      "1  0.000025  0\n",
      "2  0.000040  0\n",
      "3  0.000029  0\n",
      "4  0.000028  0\n",
      "  Gini =  0.814622641509434\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.290322  1\n",
      "1  0.000106  0\n",
      "2  0.000358  0\n",
      "3  0.000115  0\n",
      "4  0.000011  0\n",
      "para:max_depth:9, min_child_weight:5, score:0.7942357059509918\n",
      "\n",
      "Fold  0\n",
      "       prob  y\n",
      "0  0.000360  0\n",
      "1  0.002239  0\n",
      "2  0.000003  0\n",
      "3  0.000466  0\n",
      "4  0.000020  0\n",
      "  Gini =  0.8029377203290247\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.511215  1\n",
      "1  0.000192  0\n",
      "2  0.000011  0\n",
      "3  0.000004  0\n",
      "4  0.000122  0\n",
      "  Gini =  0.7743589743589744\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000016  0\n",
      "1  0.000011  0\n",
      "2  0.000168  0\n",
      "3  0.000395  0\n",
      "4  0.003940  0\n",
      "  Gini =  0.7857307249712313\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.000271  0\n",
      "1  0.000003  0\n",
      "2  0.001038  0\n",
      "3  0.999910  1\n",
      "4  0.000015  0\n",
      "  Gini =  0.7752037252619324\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.000068  0\n",
      "1  0.000040  0\n",
      "2  0.000035  0\n",
      "3  0.000118  0\n",
      "4  0.000027  0\n",
      "  Gini =  0.8178066037735848\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.511215  1\n",
      "1  0.000271  0\n",
      "2  0.000360  0\n",
      "3  0.000192  0\n",
      "4  0.000011  0\n",
      "para:max_depth:9, min_child_weight:7, score:0.7921120186697783\n"
     ]
    }
   ],
   "source": [
    "paraScores = []\n",
    "for max_depth,min_child_weight in paraCo:\n",
    "    model = XGBClassifier(    \n",
    "                        n_estimators=MAX_ROUNDS,\n",
    "                        max_depth=max_depth,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        learning_rate=LEARNING_RATE, \n",
    "                        subsample=0.8,\n",
    "                        max_delta_step = 1,\n",
    "                        min_child_weight=min_child_weight,\n",
    "                        colsample_bytree=0.6,\n",
    "#                         scale_pos_weight=scale_pos_weight,\n",
    "                        fpreproc=fpreproc,\n",
    "                        gamma=0.3,\n",
    "      reg_alpha = 0.03,\n",
    "    reg_lambda=1\n",
    "                     )\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "\n",
    "        # Create data for this fold\n",
    "        y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "        X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "        X_test = test_df.copy()\n",
    "        print( \"\\nFold \", i)\n",
    "    #     if i >1:\n",
    "    #         break\n",
    "\n",
    "        # Enocode data\n",
    "    #     for f in f_cats:\n",
    "    #         X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "    #                                                         trn_series=X_train[f],\n",
    "    #                                                         val_series=X_valid[f],\n",
    "    #                                                         tst_series=X_test[f],\n",
    "    #                                                         target=y_train,\n",
    "    #                                                         min_samples_leaf=200,\n",
    "    #                                                         smoothing=10,\n",
    "    #                                                         noise_level=0\n",
    "    #                                                         )\n",
    "        # Run model for this fold\n",
    "        if OPTIMIZE_ROUNDS:\n",
    "            eval_set=[(X_valid,y_valid)]\n",
    "            fit_model = model.fit( X_train, y_train, \n",
    "                                   eval_set=eval_set,\n",
    "                                   eval_metric=tpr_weight_funtion,\n",
    "                                   early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                                   verbose=False\n",
    "                                 )\n",
    "            print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "            print( \"  Best gini = \", model.best_score )\n",
    "        else:\n",
    "            fit_model = model.fit( X_train, y_train )\n",
    "\n",
    "        # Generate validation predictions for this fold\n",
    "        pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "        print( \"  Gini = \", tprweight_funtion(y_valid, pred) )\n",
    "        y_valid_pred.iloc[test_index] = pred\n",
    "\n",
    "        # Accumulate test set predictions\n",
    "        y_test_pred += fit_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "        del X_test, X_train, X_valid, y_train\n",
    "\n",
    "    y_test_pred /= K  # Average test set predictions\n",
    "\n",
    "    print( \"\\nGini for full training set:\" )\n",
    "    tpr = tprweight_funtion(y, y_valid_pred)\n",
    "    paraScores.append([max_depth,min_child_weight,tpr])\n",
    "    print(\"para:max_depth:%s, min_child_weight:%s, score:%s\"%( str(max_depth),str(min_child_weight),str(tpr)  ))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 1, 0.7834539089848308],\n",
       " [3, 3, 0.7844574095682614],\n",
       " [3, 5, 0.7849474912485414],\n",
       " [3, 7, 0.7853908984830805],\n",
       " [5, 7, 0.791901983663944],\n",
       " [9, 7, 0.7921120186697783],\n",
       " [5, 3, 0.7931855309218203],\n",
       " [9, 5, 0.7942357059509918],\n",
       " [5, 5, 0.7944224037339556],\n",
       " [7, 7, 0.7946091015169195],\n",
       " [5, 1, 0.7949124854142358],\n",
       " [7, 5, 0.7950291715285881],\n",
       " [9, 1, 0.7964760793465577],\n",
       " [7, 3, 0.7966161026837806],\n",
       " [9, 3, 0.7966861143523921],\n",
       " [7, 1, 0.7968028004667445]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted(paraScores, key = lambda x: x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 5\n",
    "kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "MAX_ROUNDS = 700\n",
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE =0.03\n",
    "EARLY_STOPPING_ROUNDS = 50  \n",
    "# scale_pos_weight = float(np.sum(train_features[\"Tag\"] == 0)) / np.sum(train_features[\"Tag\"] == 1)\n",
    "model = XGBClassifier(    \n",
    "                        n_estimators=MAX_ROUNDS,\n",
    "                        max_depth=13,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        learning_rate=LEARNING_RATE, \n",
    "                        subsample=0.7,\n",
    "                        max_delta_step = 1,\n",
    "                        min_child_weight=1,\n",
    "                        colsample_bytree=0.9,\n",
    "#                         scale_pos_weight=scale_pos_weight,\n",
    "                        fpreproc=fpreproc,\n",
    "                        gamma=7,\n",
    "      reg_alpha =0.03,\n",
    "    reg_lambda=0\n",
    "                     )\n",
    "\n",
    "\n",
    "#  learning_rate =\n",
    "#     n_estimators=,\n",
    "#     gamma=,\n",
    "#     subsample=,\n",
    "#     colsample_bytree=0.8,\n",
    "#     max_depth = 5,\n",
    "#     min_child_weight = 5,\n",
    "#     objective= 'binary:logistic', nthread=4,scale_pos_weight=1,seed=27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "       prob  y\n",
      "0  0.010323  0\n",
      "1  0.002240  0\n",
      "2  0.000342  0\n",
      "3  0.003573  0\n",
      "4  0.000515  0\n",
      "  Gini =  0.8074030552291422\n",
      "\n",
      "Fold  1\n",
      "       prob  y\n",
      "0  0.394365  1\n",
      "1  0.001219  0\n",
      "2  0.000844  0\n",
      "3  0.000371  0\n",
      "4  0.002217  0\n",
      "  Gini =  0.774009324009324\n",
      "\n",
      "Fold  2\n",
      "       prob  y\n",
      "0  0.000763  0\n",
      "1  0.000379  0\n",
      "2  0.001041  0\n",
      "3  0.002176  0\n",
      "4  0.004957  0\n",
      "  Gini =  0.7891829689298044\n",
      "\n",
      "Fold  3\n",
      "       prob  y\n",
      "0  0.001339  0\n",
      "1  0.000417  0\n",
      "2  0.006402  0\n",
      "3  0.998318  1\n",
      "4  0.000288  0\n",
      "  Gini =  0.7810244470314318\n",
      "\n",
      "Fold  4\n",
      "       prob  y\n",
      "0  0.002047  0\n",
      "1  0.000824  0\n",
      "2  0.001355  0\n",
      "3  0.000251  0\n",
      "4  0.000571  0\n",
      "  Gini =  0.8068396226415095\n",
      "\n",
      "Gini for full training set:\n",
      "       prob  y\n",
      "0  0.394365  1\n",
      "1  0.001339  0\n",
      "2  0.010323  0\n",
      "3  0.001219  0\n",
      "4  0.000844  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7958226371061843"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    \n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "    X_test = test_df.copy()\n",
    "    print( \"\\nFold \", i)\n",
    "#     if i >1:\n",
    "#         break\n",
    "    \n",
    "    # Enocode data\n",
    "#     for f in f_cats:\n",
    "#         X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "#                                                         trn_series=X_train[f],\n",
    "#                                                         val_series=X_valid[f],\n",
    "#                                                         tst_series=X_test[f],\n",
    "#                                                         target=y_train,\n",
    "#                                                         min_samples_leaf=200,\n",
    "#                                                         smoothing=10,\n",
    "#                                                         noise_level=0\n",
    "#                                                         )\n",
    "    # Run model for this fold\n",
    "    if OPTIMIZE_ROUNDS:\n",
    "        eval_set=[(X_valid,y_valid)]\n",
    "        fit_model = model.fit( X_train, y_train, \n",
    "                               eval_set=eval_set,\n",
    "                               eval_metric=tpr_weight_funtion,\n",
    "                               early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                               verbose=False\n",
    "                             )\n",
    "        print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "        print( \"  Best gini = \", model.best_score )\n",
    "    else:\n",
    "        fit_model = model.fit( X_train, y_train )\n",
    "        \n",
    "    # Generate validation predictions for this fold\n",
    "    pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "    print( \"  Gini = \", tprweight_funtion(y_valid, pred) )\n",
    "    y_valid_pred.iloc[test_index] = pred\n",
    "    \n",
    "    # Accumulate test set predictions\n",
    "    y_test_pred += fit_model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    del X_test, X_train, X_valid, y_train\n",
    "    \n",
    "y_test_pred /= K  # Average test set predictions\n",
    "\n",
    "print( \"\\nGini for full training set:\" )\n",
    "tprweight_funtion(y, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = pd.DataFrame()\n",
    "val['UID'] = id_train\n",
    "val['Tag'] = y_valid_pred.values\n",
    "val.to_csv('xgb_validBeatBaseV13.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['UID'] = id_test\n",
    "sub['Tag'] = y_test_pred\n",
    "sub.to_csv('xgb_submitBeatBaseV23.csv', float_format='%.6f', index=False)\n",
    "# xgb_submitBeatBaseV13 去掉部分无用特征后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID_VAL1trans_type1merchanthas_4.0:0.0\n",
      "UID_VAL1trans_type1merchanthas_5.0:0.0\n",
      "UID_VAL1trans_type1merchanthas_6.0:0.0\n",
      "UID_VAL1trans_type1merchanthas_7.0:0.0030677614\n",
      "UID_VAL1trans_type1merchanthas_nan:0.003879816\n",
      "UID_VAL2trans_type1merchanthas_4.0:0.00054136966\n",
      "UID_VAL2trans_type1merchanthas_5.0:0.00018045655\n",
      "UID_VAL2trans_type1merchanthas_6.0:0.001624109\n",
      "UID_VAL2trans_type1merchanthas_7.0:0.0008120545\n",
      "UID_VAL2trans_type1merchanthas_nan:0.0017143373\n",
      "UID_VAL3trans_type1merchanthas_4.0:0.00018045655\n",
      "UID_VAL3trans_type1merchanthas_5.0:0.0\n",
      "UID_VAL3trans_type1merchanthas_6.0:0.0\n",
      "UID_VAL3trans_type1merchanthas_7.0:9.022828e-05\n",
      "UID_VAL3trans_type1merchanthas_nan:0.0018947938\n",
      "UID_VAL4trans_type1merchanthas_3.0:0.0\n",
      "UID_VAL4trans_type1merchanthas_4.0:0.0063159796\n",
      "UID_VAL4trans_type1merchanthas_5.0:0.008301002\n",
      "UID_VAL4trans_type1merchanthas_6.0:0.0\n",
      "UID_VAL4trans_type1merchanthas_nan:0.00027068483\n",
      "UID_VAL5trans_type1merchanthas_3.0:0.008391229\n",
      "UID_VAL5trans_type1merchanthas_4.0:0.0\n",
      "UID_VAL5trans_type1merchanthas_nan:0.0\n",
      "UID_VAL6trans_type1merchanthas_3.0:0.0\n",
      "UID_VAL6trans_type1merchanthas_4.0:9.022828e-05\n",
      "UID_VAL6trans_type1merchanthas_5.0:0.0\n",
      "UID_VAL6trans_type1merchanthas_nan:0.00018045655\n",
      "UID_VAL7trans_type1merchanthas_3.0:0.0\n",
      "UID_VAL7trans_type1merchanthas_4.0:0.0011729676\n",
      "UID_VAL7trans_type1merchanthas_5.0:0.0\n",
      "UID_VAL7trans_type1merchanthas_nan:9.022828e-05\n",
      "UID_VAL8trans_type1merchanthas_3.0:0.0\n",
      "UID_VAL8trans_type1merchanthas_4.0:0.0\n",
      "UID_VAL8trans_type1merchanthas_nan:0.0\n",
      "UID_VAL9trans_type1merchanthas_3.0:0.0\n",
      "UID_VAL9trans_type1merchanthas_4.0:0.0\n",
      "UID_VAL9trans_type1merchanthas_nan:0.0\n",
      "UID_VAL10trans_type1merchanthas_3.0:0.0\n",
      "UID_VAL10trans_type1merchanthas_4.0:0.0\n",
      "UID_VAL10trans_type1merchanthas_nan:0.0\n",
      "UID_VAL11trans_type1merchanthas_3.0:0.0\n",
      "UID_VAL11trans_type1merchanthas_4.0:0.0\n",
      "UID_VAL11trans_type1merchanthas_nan:0.0\n",
      "UID_VAL12trans_type1merchanthas_3.0:0.0\n",
      "UID_VAL12trans_type1merchanthas_4.0:0.0\n",
      "UID_VAL12trans_type1merchanthas_nan:0.0\n",
      "UID_VAL13trans_type1merchanthas_3.0:0.0\n",
      "UID_VAL13trans_type1merchanthas_4.0:0.0\n",
      "UID_VAL13trans_type1merchanthas_5.0:0.0\n",
      "UID_VAL13trans_type1merchanthas_nan:0.0\n",
      "UID_VAL14trans_type1merchanthas_4.0:0.0\n",
      "UID_VAL14trans_type1merchanthas_5.0:0.0\n",
      "UID_VAL14trans_type1merchanthas_6.0:0.0\n",
      "UID_VAL14trans_type1merchanthas_7.0:0.0\n",
      "UID_VAL14trans_type1merchanthas_nan:0.0\n",
      "UID_VAL15trans_type1merchanthas_4.0:0.0\n",
      "UID_VAL15trans_type1merchanthas_6.0:0.0\n",
      "UID_VAL15trans_type1merchanthas_nan:0.0\n",
      "UID_VAL1trans_type2merchanthas_3.0:0.0\n",
      "UID_VAL1trans_type2merchanthas_4.0:0.0\n",
      "UID_VAL1trans_type2merchanthas_5.0:0.0\n",
      "UID_VAL1trans_type2merchanthas_6.0:0.0010827393\n",
      "UID_VAL1trans_type2merchanthas_7.0:0.001624109\n",
      "UID_VAL1trans_type2merchanthas_nan:0.0004511414\n",
      "UID_VAL2trans_type2merchanthas_4.0:0.00027068483\n",
      "UID_VAL2trans_type2merchanthas_5.0:9.022828e-05\n",
      "UID_VAL2trans_type2merchanthas_6.0:0.001985022\n",
      "UID_VAL2trans_type2merchanthas_7.0:0.001624109\n",
      "UID_VAL2trans_type2merchanthas_nan:0.0011729676\n",
      "UID_VAL3trans_type2merchanthas_4.0:0.00027068483\n",
      "UID_VAL3trans_type2merchanthas_5.0:0.00018045655\n",
      "UID_VAL3trans_type2merchanthas_6.0:0.0\n",
      "UID_VAL3trans_type2merchanthas_7.0:0.0\n",
      "UID_VAL3trans_type2merchanthas_nan:0.0\n",
      "UID_VAL4trans_type2merchanthas_3.0:0.0\n",
      "UID_VAL4trans_type2merchanthas_4.0:0.0\n",
      "UID_VAL4trans_type2merchanthas_5.0:0.0\n",
      "UID_VAL4trans_type2merchanthas_6.0:0.0\n",
      "UID_VAL4trans_type2merchanthas_7.0:0.0\n",
      "UID_VAL4trans_type2merchanthas_nan:9.022828e-05\n",
      "UID_VAL5trans_type2merchanthas_3.0:0.0\n",
      "UID_VAL5trans_type2merchanthas_4.0:0.0003609131\n",
      "UID_VAL5trans_type2merchanthas_5.0:0.0\n",
      "UID_VAL5trans_type2merchanthas_6.0:0.0\n",
      "UID_VAL5trans_type2merchanthas_7.0:0.0012631959\n",
      "UID_VAL5trans_type2merchanthas_nan:0.0007218262\n",
      "UID_VAL1amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL1amt_src1merchanthas_5.0:0.0\n",
      "UID_VAL1amt_src1merchanthas_6.0:0.0\n",
      "UID_VAL1amt_src1merchanthas_7.0:0.00018045655\n",
      "UID_VAL1amt_src1merchanthas_nan:0.0003609131\n",
      "UID_VAL2amt_src1merchanthas_4.0:0.0007218262\n",
      "UID_VAL2amt_src1merchanthas_5.0:0.0017143373\n",
      "UID_VAL2amt_src1merchanthas_6.0:0.0014436524\n",
      "UID_VAL2amt_src1merchanthas_7.0:0.0017143373\n",
      "UID_VAL2amt_src1merchanthas_nan:0.0024361636\n",
      "UID_VAL3amt_src1merchanthas_4.0:0.00054136966\n",
      "UID_VAL3amt_src1merchanthas_5.0:0.0011729676\n",
      "UID_VAL3amt_src1merchanthas_6.0:0.000992511\n",
      "UID_VAL3amt_src1merchanthas_7.0:0.0024361636\n",
      "UID_VAL3amt_src1merchanthas_nan:0.0012631959\n",
      "UID_VAL4amt_src1merchanthas_4.0:0.0023459352\n",
      "UID_VAL4amt_src1merchanthas_5.0:0.001985022\n",
      "UID_VAL4amt_src1merchanthas_6.0:0.0004511414\n",
      "UID_VAL4amt_src1merchanthas_7.0:0.001624109\n",
      "UID_VAL4amt_src1merchanthas_nan:0.0053234682\n",
      "UID_VAL5amt_src1merchanthas_4.0:0.00027068483\n",
      "UID_VAL5amt_src1merchanthas_nan:9.022828e-05\n",
      "UID_VAL6amt_src1merchanthas_4.0:0.0012631959\n",
      "UID_VAL6amt_src1merchanthas_5.0:0.00027068483\n",
      "UID_VAL6amt_src1merchanthas_6.0:0.0\n",
      "UID_VAL6amt_src1merchanthas_7.0:0.0007218262\n",
      "UID_VAL6amt_src1merchanthas_nan:0.0020752505\n",
      "UID_VAL7amt_src1merchanthas_4.0:0.0018947938\n",
      "UID_VAL7amt_src1merchanthas_5.0:0.0012631959\n",
      "UID_VAL7amt_src1merchanthas_6.0:0.0012631959\n",
      "UID_VAL7amt_src1merchanthas_7.0:0.0013534242\n",
      "UID_VAL7amt_src1merchanthas_nan:0.0031579898\n",
      "UID_VAL8amt_src1merchanthas_4.0:0.00018045655\n",
      "UID_VAL8amt_src1merchanthas_5.0:0.0\n",
      "UID_VAL8amt_src1merchanthas_6.0:0.0\n",
      "UID_VAL8amt_src1merchanthas_7.0:0.0004511414\n",
      "UID_VAL8amt_src1merchanthas_nan:0.003879816\n",
      "UID_VAL9amt_src1merchanthas_4.0:0.00054136966\n",
      "UID_VAL9amt_src1merchanthas_5.0:9.022828e-05\n",
      "UID_VAL9amt_src1merchanthas_6.0:0.0\n",
      "UID_VAL9amt_src1merchanthas_7.0:9.022828e-05\n",
      "UID_VAL9amt_src1merchanthas_nan:0.00054136966\n",
      "UID_VAL10amt_src1merchanthas_4.0:0.0008120545\n",
      "UID_VAL10amt_src1merchanthas_5.0:0.004962555\n",
      "UID_VAL10amt_src1merchanthas_6.0:0.002255707\n",
      "UID_VAL10amt_src1merchanthas_7.0:0.00063159794\n",
      "UID_VAL10amt_src1merchanthas_nan:0.0004511414\n",
      "UID_VAL11amt_src1merchanthas_3.0:0.0\n",
      "UID_VAL11amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL11amt_src1merchanthas_5.0:0.0\n",
      "UID_VAL11amt_src1merchanthas_nan:0.0\n",
      "UID_VAL12amt_src1merchanthas_3.0:0.0\n",
      "UID_VAL12amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL12amt_src1merchanthas_nan:0.0\n",
      "UID_VAL13amt_src1merchanthas_3.0:0.0\n",
      "UID_VAL13amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL13amt_src1merchanthas_nan:0.0\n",
      "UID_VAL14amt_src1merchanthas_1.0:0.0\n",
      "UID_VAL14amt_src1merchanthas_nan:0.0\n",
      "UID_VAL15amt_src1merchanthas_3.0:0.0\n",
      "UID_VAL15amt_src1merchanthas_4.0:0.0066768927\n",
      "UID_VAL15amt_src1merchanthas_nan:0.0\n",
      "UID_VAL16amt_src1merchanthas_3.0:0.0\n",
      "UID_VAL16amt_src1merchanthas_5.0:0.0\n",
      "UID_VAL16amt_src1merchanthas_nan:0.0\n",
      "UID_VAL17amt_src1merchanthas_3.0:0.0\n",
      "UID_VAL17amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL17amt_src1merchanthas_nan:0.0\n",
      "UID_VAL18amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL18amt_src1merchanthas_5.0:0.0\n",
      "UID_VAL18amt_src1merchanthas_6.0:0.0\n",
      "UID_VAL18amt_src1merchanthas_7.0:0.0\n",
      "UID_VAL18amt_src1merchanthas_nan:0.0\n",
      "UID_VAL19amt_src1merchanthas_3.0:0.0\n",
      "UID_VAL19amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL19amt_src1merchanthas_5.0:0.0\n",
      "UID_VAL19amt_src1merchanthas_nan:0.0\n",
      "UID_VAL20amt_src1merchanthas_3.0:0.0\n",
      "UID_VAL20amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL20amt_src1merchanthas_5.0:0.0\n",
      "UID_VAL20amt_src1merchanthas_nan:0.0\n",
      "UID_VAL21amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL21amt_src1merchanthas_5.0:0.0\n",
      "UID_VAL21amt_src1merchanthas_nan:0.0\n",
      "UID_VAL22amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL22amt_src1merchanthas_6.0:0.0\n",
      "UID_VAL22amt_src1merchanthas_nan:0.0\n",
      "UID_VAL23amt_src1merchanthas_3.0:0.0\n",
      "UID_VAL23amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL23amt_src1merchanthas_5.0:0.0\n",
      "UID_VAL23amt_src1merchanthas_nan:0.0\n",
      "UID_VAL24amt_src1merchanthas_3.0:0.0\n",
      "UID_VAL24amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL24amt_src1merchanthas_5.0:0.0\n",
      "UID_VAL24amt_src1merchanthas_nan:0.0\n",
      "UID_VAL25amt_src1merchanthas_1.0:0.0\n",
      "UID_VAL25amt_src1merchanthas_nan:0.0\n",
      "UID_VAL26amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL26amt_src1merchanthas_5.0:0.0\n",
      "UID_VAL26amt_src1merchanthas_nan:0.0\n",
      "UID_VAL27amt_src1merchanthas_1.0:0.0\n",
      "UID_VAL27amt_src1merchanthas_nan:0.0\n",
      "UID_VAL28amt_src1merchanthas_3.0:0.0\n",
      "UID_VAL28amt_src1merchanthas_4.0:0.0\n",
      "UID_VAL28amt_src1merchanthas_5.0:0.0\n",
      "UID_VAL28amt_src1merchanthas_6.0:0.010737165\n",
      "UID_VAL28amt_src1merchanthas_nan:0.00054136966\n",
      "device1_null:0.0\n",
      "device2_null:0.0\n",
      "UIDInWifiUserCntBoxIdx_4.0:0.0062257512\n",
      "UIDInWifiUserCntBoxIdx_5.0:0.0011729676\n",
      "UIDInWifiUserCntBoxIdx_6.0:0.0007218262\n",
      "UIDInWifiUserCntBoxIdx_7.0:0.008301002\n",
      "UIDInWifiUserCntBoxIdx_nan:0.0024361636\n",
      "modeTypes_1.0:0.0018947938\n",
      "modeTypes_2.0:0.0026166202\n",
      "modeTypes_3.0:0.0034286745\n",
      "modeTypes_4.0:0.0018947938\n",
      "modeTypes_5.0:0.00054136966\n",
      "modeTypes_6.0:0.0050527835\n",
      "modeTypes_7.0:0.0\n",
      "modeTypes_8.0:0.0010827393\n",
      "modeTypes_9.0:0.0060452945\n",
      "modeTypes_10.0:9.022828e-05\n",
      "modeTypes_11.0:0.00018045655\n",
      "modeTypes_12.0:0.0009022828\n",
      "modeTypes_nan:9.022828e-05\n",
      "ProV_AnHui:0.0004511414\n",
      "ProV_BeiJing:0.0\n",
      "ProV_CHINAArea:0.0008120545\n",
      "ProV_ChongQing:0.0\n",
      "ProV_FuJian:9.022828e-05\n",
      "ProV_GanSu:0.0004511414\n",
      "ProV_GuangDong:0.00063159794\n",
      "ProV_GuangXi:0.00018045655\n",
      "ProV_GuiZhou:0.00018045655\n",
      "ProV_HaiNan:0.0\n",
      "ProV_HeBei:0.0004511414\n",
      "ProV_HeNan:9.022828e-05\n",
      "ProV_HeiLongJiang:0.00027068483\n",
      "ProV_HuBei:9.022828e-05\n",
      "ProV_HuNan:0.0007218262\n",
      "ProV_JiLin:0.0\n",
      "ProV_JiangSu:0.0021654787\n",
      "ProV_JiangXi:0.0013534242\n",
      "ProV_LiaoNing:0.0012631959\n",
      "ProV_NeiMengGu:0.0026166202\n",
      "ProV_NingXia:0.0\n",
      "ProV_QingHai:0.0\n",
      "ProV_ShanDong:0.0004511414\n",
      "ProV_ShanXi:0.00054136966\n",
      "ProV_ShangHai:0.0\n",
      "ProV_SiChuan:0.0015338807\n",
      "ProV_TaiWan:0.0\n",
      "ProV_TianJin:0.0\n",
      "ProV_XiZang:0.0\n",
      "ProV_XinJiang:0.0\n",
      "ProV_YunNan:0.00018045655\n",
      "ProV_ZheJiang:0.0007218262\n",
      "ProV_unknow:0.0030677614\n",
      "ProV_nan:0.003609131\n",
      "members_min:0.011729676\n",
      "members_max:0.01732383\n",
      "members_mean:0.00992511\n",
      "uni_mem_min:0.0133537855\n",
      "uni_mem_max:0.014346296\n",
      "uni_mem_mean:0.015519264\n",
      "uni_device_min:0.021203645\n",
      "uni_device_max:0.00992511\n",
      "uni_device_mean:0.013895155\n",
      "uni_channel_min:0.0\n",
      "uni_channel_max:0.0\n",
      "uni_channel_mean:9.022828e-05\n",
      "uni_trans_min:0.0\n",
      "uni_trans_max:0.0\n",
      "uni_trans_mean:0.0008120545\n",
      "trans_amt_min:0.018135883\n",
      "trans_amt_max:0.020842733\n",
      "trans_amt_mean:0.008571686\n",
      "memp_min:0.015970405\n",
      "memp_max:0.014075612\n",
      "memp_mean:0.012090589\n",
      "merchant_x:0.006135523\n",
      "merchant_y:9.022828e-05\n",
      "device2:0.0027970767\n",
      "amt_src1:0.0030677614\n",
      "time0:0.0007218262\n",
      "time1:0.0030677614\n",
      "time2:0.008571686\n",
      "time3:0.0036993595\n",
      "time4:0.0024361636\n",
      "time5:9.022828e-05\n",
      "time6:0.00027068483\n",
      "mean_time:0.0031579898\n",
      "min_time:0.0056843814\n",
      "max_time:0.004782099\n",
      "most_time:0.003879816\n",
      "user_dayCnt1:0.0060452945\n",
      "user_dayCnt2:0.004601642\n",
      "user_dayCnt3:0.0040602726\n",
      "user_dayCnt4:0.0008120545\n",
      "user_dayCnt5:0.0037895876\n",
      "user_dayCnt6:0.0\n",
      "user_dayCnt7:9.022828e-05\n",
      "user_dayCnt8:0.004782099\n",
      "user_dayCnt9:0.0055941534\n",
      "user_dayCnt10:0.0003609131\n",
      "user_dayCnt11:0.0003609131\n",
      "user_dayCnt12:0.0\n",
      "user_dayCnt13:0.00063159794\n",
      "user_dayCnt14:0.0014436524\n",
      "user_dayCnt15:0.0056843814\n",
      "user_dayCnt16:0.0031579898\n",
      "user_dayCnt17:0.001985022\n",
      "user_dayCnt18:0.0043309573\n",
      "user_dayCnt19:0.0037895876\n",
      "user_dayCnt20:0.0024361636\n",
      "user_dayCnt21:0.0003609131\n",
      "user_dayCnt22:0.0036993595\n",
      "user_dayCnt23:0.0028873049\n",
      "user_dayCnt24:0.0008120545\n",
      "user_dayCnt25:0.0036993595\n",
      "user_dayCnt26:0.0003609131\n",
      "user_dayCnt27:0.0018947938\n",
      "user_dayCnt28:0.0003609131\n",
      "user_dayCnt29:0.005503925\n",
      "user_dayCnt30:0.004150501\n",
      "User_meanOfAll_geo_code:0.01100785\n",
      "User_totalOfAll_geo_code:0.0115492195\n",
      "User_maxOfAll_geo_code:0.0025263918\n",
      "User_minOfAll_geo_code:0.0089326\n",
      "User_meanOfAll_mac1:0.0066768927\n",
      "User_totalOfAll_mac1:0.003609131\n",
      "User_maxOfAll_mac1:9.022828e-05\n",
      "User_minOfAll_mac1:0.003518903\n",
      "User_meanOfAll_ip1:0.024271406\n",
      "User_totalOfAll_ip1:0.015519264\n",
      "User_maxOfAll_ip1:0.009473969\n",
      "User_minOfAll_ip1:0.007759632\n",
      "balUserbal_minOverDaysmean:0.00027068483\n",
      "balUserbal_minOverDaystotal:0.002255707\n",
      "balUserbal_minOverDaysmax:0.0\n",
      "balUserbal_minOverDaysmin:9.022828e-05\n",
      "balUserbal_maxOverDaysmean:9.022828e-05\n",
      "balUserbal_maxOverDaystotal:0.0007218262\n",
      "balUserbal_maxOverDaysmax:9.022828e-05\n",
      "balUserbal_maxOverDaysmin:9.022828e-05\n",
      "balUserbal_totalOverDaysmean:0.004782099\n",
      "balUserbal_totalOverDaystotal:0.0025263918\n",
      "balUserbal_totalOverDaysmax:0.002255707\n",
      "balUserbal_totalOverDaysmin:0.0013534242\n",
      "balUserbal_meanOverDaysmean:0.0\n",
      "balUserbal_meanOverDaystotal:0.0\n",
      "balUserbal_meanOverDaysmax:0.0\n",
      "balUserbal_meanOverDaysmin:0.0\n",
      "operRounds:0.011098078\n",
      "maxOperCntinRoutes:0.024271406\n",
      "minOperCntinRoutes:0.023188667\n",
      "meanOperCntinRoutes:0.032662638\n",
      "userOperPenc:0.03392583\n",
      "marketFreq:0.00054136966\n",
      "MAXrouteCnt:0.011458991\n",
      "MINrouteCNT:0.0059550665\n",
      "FirOpperGroupCNT:0.014887665\n",
      "FirOpperRank:0.0014436524\n",
      "MINrouteGroupCNT:0.015970405\n",
      "MINrouteRank:0.0014436524\n",
      "MAXrouteGroupCNT:0.014977894\n",
      "MAXrouteRank:0.0027970767\n",
      "mostOperCntinRoutesGroupCNT:0.004962555\n",
      "mostOperCntinRoutesRank:0.0004511414\n",
      "first_cnt_min:0.00054136966\n",
      "first_cnt_max:0.0014436524\n",
      "first_cnt_mean:0.008391229\n",
      "sec_cnt_min:0.008752143\n",
      "sec_cnt_max:0.008120545\n",
      "sec_cnt_mean:0.008571686\n",
      "sec_fir_per_min:0.01362447\n",
      "sec_fir_per_max:0.010556708\n",
      "sec_fir_per_mean:0.0050527835\n",
      "feature0_min:0.0054136966\n",
      "feature0_max:0.0018947938\n",
      "feature0_mean:0.0018045655\n",
      "feature1_min:0.0044211857\n",
      "feature1_max:0.0026166202\n",
      "feature1_mean:0.0025263918\n",
      "feature2_min:0.0\n",
      "feature2_max:0.0\n",
      "feature2_mean:0.0\n",
      "feature3_min:0.00027068483\n",
      "feature3_max:0.00027068483\n",
      "feature3_mean:0.00018045655\n",
      "degree:0.012812415\n",
      "comunitiName:0.017955428\n",
      "TagPredict:0.033564918\n"
     ]
    }
   ],
   "source": [
    "for fn,f in zip(train_features,model.feature_importances_):\n",
    "    print(fn +  \":\" +str(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.6, 0.6), (0.6, 0.7), (0.6, 0.8), (0.6, 0.9), (0.7, 0.6), (0.7, 0.7), (0.7, 0.8), (0.7, 0.9), (0.8, 0.6), (0.8, 0.7), (0.8, 0.8), (0.8, 0.9), (0.9, 0.6), (0.9, 0.7), (0.9, 0.8), (0.9, 0.9)]\n",
      "\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# max_depths = [3,5,7,9]\n",
    "# min_child_weights = [1,3,5,7]    \n",
    "from itertools import product\n",
    "# l = [1, 2, 3]\n",
    "# subsample = [i / 10.0 for i in range(6, 10)]\n",
    "# colsample_bytree = [i / 10.0 for i in range(6, 10)]  \n",
    "\n",
    "\n",
    "  \n",
    "subsample = [i/10.0 for i in range(6,10)]\n",
    "colsample_bytree = [i/10.0 for i in range(6,10)]\n",
    "# max_depth = range(3,10,2)\n",
    "# min_child_weight = range(1,6,2)\n",
    "\n",
    "print(list(product(subsample,colsample_bytree    )))\n",
    "print()\n",
    "print(len(list(product(subsample,colsample_bytree    ))))\n",
    "# parameter = {'n_estimators':range(400,1000,100),'learning_rate': ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paraLists = list(([0.6,0.6,0.7845507584597432],\n",
    "[0.6,0.7,0.783313885647608],\n",
    "[0.6,0.8,0.7849008168028003],\n",
    "[0.6,0.9,0.7859743290548425],\n",
    "                 \n",
    "                 \n",
    "                 )\n",
    ")\n",
    "subsample = []\n",
    "colsample_bytree = []\n",
    "scores = []\n",
    "for d,c,s in paraLists:\n",
    "    colsample_bytree.append(d)\n",
    "    subsample.append(c)\n",
    "    scores.append(s)\n",
    "data = pd.DataFrame()\n",
    "data[\"subsample\"] = subsample\n",
    "data[\"colsample_bytree\"] = colsample_bytree\n",
    "data[\"scores\"] = scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot out seaching result of max_depth| min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqEAAAHjCAYAAAAT2sakAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x72c20cb438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import re\n",
    "\n",
    "# paraRes = [ 'dict_values([3, 5])0.7855309218203034', 'dict_values([3, 3])0.7842473745624271', 'dict_values([1, 3])0.7871411901983665', 'dict_values([1, 5])0.7904317386231039']\n",
    "# for da in paraRes:\n",
    "#     print(re.findall( r'\\[(.+)\\]', da))\n",
    "#     print(re.findall( r'(\\d{5,20})', da)[0][:4])\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np; np.random.seed(0)\n",
    "import seaborn as sns; \n",
    "sns.set()\n",
    "\n",
    "# flights = sns.load_dataset(\"flights\")\n",
    "# flights = flights.pivot(\"max_depth\", \"min_child_weight\", \"scores\")\n",
    "flights = data.pivot(\"max_depth\", \"min_child_weight\", \"scores\")\n",
    "# ax = sns.heatmap(flights, cmap=\"YlGnBu\")\n",
    "\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8,5))\n",
    "ax = sns.heatmap(flights,cmap = 'RdBu',ax=ax,vmin=0.77, vmax=0.81,annot=True,fmt ='0.3g')\n",
    "\n",
    "#设置坐标字体方向\n",
    "label_y = ax.get_yticklabels()\n",
    "plt.setp(label_y, rotation=45, horizontalalignment='right')\n",
    "label_x = ax.get_xticklabels()\n",
    "plt.setp(label_x, rotation=45, horizontalalignment='right')\n",
    "plt.xlabel('min_child_weight')#设置坐标名称\n",
    "plt.ylabel('max_depth')\n",
    "plt.title('scores')#标题\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot out seaching result of subSample| subTreeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 树的深度不高，暂时不用调  Plot just gamma no serach  =》 range 0，+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0), (2, 0.01), (3, 0.03), (4, 0.1), (5, 0.3), (6, 1), (7, 3), (8, 7)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/gAAAK9CAYAAACQOcf8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhSsuKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x72c1d7c128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt # to plot graph\n",
    "import seaborn as sns # for intractve graphs\n",
    "import numpy as np # for linear algebra\n",
    "import datetime # to dela with date and time\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# [0, 0.001,0.003,0.01,0.03, 0.05,\\ 0.1,0.3,0.5 \n",
    "prsc = [\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    [0, 0.8016102683780629], \n",
    "    [0.01, 0.8035939323220537],\n",
    "    [0.03, 0.8011201866977831],\n",
    "    [0.1, 0.802100350058343],\n",
    "    [0.3, 0.7867911318553092],\n",
    "    [1, 0.7861843640606768],\n",
    "    [3, 0.7863243873978996],\n",
    "    [7, 0.800630105017503]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# prinxAndScore = []\n",
    "idx = []\n",
    "score = []\n",
    "paraIDX = []\n",
    "for i,v in enumerate(prsc):\n",
    "    idx.append(i+1)\n",
    "    score.append(v[1])\n",
    "    paraIDX.append((i+1, v[0] ))\n",
    "print(paraIDX)\n",
    "data = pd.DataFrame()\n",
    "data[\"paraIdx\"] = idx\n",
    "data[\"score\"] = score\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "sns.pointplot(x=\"paraIdx\", y=\"score\", data=data)\n",
    "plt.title(\"para scores\")\n",
    "plt.grid(True)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
