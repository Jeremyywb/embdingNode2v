{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**导入所需库**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**读取数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_df = pd.read_csv('../cache/operation_pre.csv', encoding='gbk', nrows=nrows)\n",
    "transaction_df = pd.read_csv('../cache/transaction_pre.csv', encoding='gbk', nrows=nrows)\n",
    "tag_trn = pd.read_csv('../data/tag_train_new.csv')\n",
    "submission1 = pd.read_csv('../data/提交样例.csv')\n",
    "submission2 = pd.read_csv('../data/submit_example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([tag_trn, submission2], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**工具函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算两个序列之间重合度\n",
    "def calculate_intersection_ratio(x, y):\n",
    "    try:\n",
    "        inter = x.intersection(y)\n",
    "        union = x.union(y)\n",
    "        return 1.0*len(inter)/len(union)\n",
    "    except:\n",
    "        return np.nan\n",
    "# 计算行为A到行为B时间间隔小于阈值的个数\n",
    "def getActionTimeSpan(df_action_of_userid, actiontypeA, actiontypeB, timethred):\n",
    "    timespan_list = []\n",
    "    i = 0\n",
    "    while i < (len(df_action_of_userid) - 1):\n",
    "        if df_action_of_userid['action_type'].iat[i] == actiontypeA:\n",
    "            timeA = df_action_of_userid['actionTimestamp'].iat[i]\n",
    "            for j in range(i + 1, len(df_action_of_userid)):\n",
    "                if df_action_of_userid['action_type'].iat[j] == actiontypeA:\n",
    "                    timeA = df_action_of_userid['actionTimestamp'].iat[j]\n",
    "                if df_action_of_userid['action_type'].iat[j] == actiontypeB:\n",
    "                    timeB = df_action_of_userid['actionTimestamp'].iat[j]\n",
    "                    timespan_list.append(timeB-timeA)\n",
    "                    i = j\n",
    "                    break\n",
    "        i += 1\n",
    "    return np.sum(np.array(timespan_list) <= timethred) / (np.sum(np.array(timespan_list)) + 1.0)\n",
    "# 计算用户当前行为与前一行为的所使用的设备及所处环境的吻合程度\n",
    "def check_has_changed(df, df_feats, key, value, n, name):\n",
    "    df_feats = df_feats.sort_values(by=['UID', 'day', 'time'])\n",
    "    data_temp = df_feats[key+['day', 'time', value]].copy()\n",
    "    shift_value = data_temp.groupby(key)[['day', 'time', value]].shift(n)\n",
    "    shift_value = shift_value.rename(columns={'day': 'bef_day',\n",
    "                                     'time': 'bef_time',\n",
    "                                     value: 'bef_'+value})\n",
    "    data_temp = data_temp.rename(columns={'day': 'cur_day',\n",
    "                                  'time': 'cur_time',\n",
    "                                  value: 'cur_'+value})\n",
    "    data_temp = pd.concat([data_temp, shift_value], axis=1)\n",
    "    data_temp[name] = 0\n",
    "    data_temp.loc[data_temp['cur_'+value]!=data_temp['bef_'+value], name] = 1\n",
    "    data_temp.loc[(data_temp['cur_'+value].isnull()==True)&(data_temp['bef_'+value].isnull()==True), name] = 0\n",
    "    data_temp['day_diff'] = data_temp['cur_day']-data_temp['bef_day']\n",
    "    data_temp['time_diff'] = data_temp.apply(lambda x: time_diff(x['cur_time'], x['bef_time']), axis=1)\n",
    "    data_temp['time_diff'] = 1440.0*data_temp['day_diff']+data_temp['time_diff']\n",
    "    # 全局\n",
    "    gp = data_temp.groupby(key)[name].agg({name+'_count': 'sum',\n",
    "                                           name+'_rt': 'mean'}).reset_index()\n",
    "    df = df.merge(gp, on=['UID'], how='left')\n",
    "    # 在1小时内\n",
    "    tmp = data_temp[data_temp['time_diff']<=60].copy()\n",
    "    gp = tmp.groupby(key)[name].agg({name+'_count_in_one_hour': 'sum',\n",
    "                                     name+'_rt_in_one_hour': 'mean'}).reset_index()\n",
    "    df = df.merge(gp, on=['UID'], how='left')\n",
    "    # 在1分钟内\n",
    "    tmp = data_temp[data_temp['time_diff']<=1].copy()\n",
    "    gp = tmp.groupby(key)[name].agg({name+'_count_in_one_minute': 'sum',\n",
    "                                     name+'_rt_in_one_minute': 'mean'}).reset_index()\n",
    "    df = df.merge(gp, on=['UID'], how='left')\n",
    "    return df\n",
    "# word2vec特征\n",
    "def w2v_features(df, df_feats, key, value, word_ndim,colname):\n",
    "    df_feats = df_feats.sort_values(by=['UID', 'actionTime'])\n",
    "    val_list = df_feats.groupby(key).apply(lambda x: x[value].tolist()).reset_index()\n",
    "    val_list.columns = ['UID', value+'_list']\n",
    "    val_list[value+'_list'] = val_list[value+'_list'].apply(lambda x: [str(f) for f in x])\n",
    "\n",
    "    model = gensim.models.Word2Vec(val_list[value+'_list'], size=word_ndim, window=10, min_count=2, workers=4)\n",
    "    model.save('../cache/'+value+'_word2vec.model')\n",
    "    \n",
    "    vocab = list(model.wv.vocab.keys())\n",
    "    w2c_arr = []\n",
    "    for v in vocab :\n",
    "        w2c_arr.append(list(model.wv[v]))\n",
    "        \n",
    "    df_w2c_start = pd.DataFrame()\n",
    "    df_w2c_start[value] = vocab\n",
    "    df_w2c_start = pd.concat([df_w2c_start, pd.DataFrame(w2c_arr)], axis=1)\n",
    "    df_w2c_start.columns = [value] + [colname+str(i) for i in range(word_ndim)]\n",
    "\n",
    "    df_feats[value] = df_feats[value].astype(str)\n",
    "    \n",
    "    tmp = df_feats.merge(df_w2c_start, on=[value], how='left')\n",
    "    \n",
    "    gp = tmp.groupby(['UID'])[[colname+str(i) for i in range(word_ndim)]].agg({'mean', 'skew'})\n",
    "    gp.columns = pd.Index([e[0] + \"_\" + e[1] for e in gp.columns.tolist()])\n",
    "    gp.reset_index(inplace=True)\n",
    "    df = df.merge(gp, on=['UID'], how='left')\n",
    "    return df\n",
    "# 计算分组时间差\n",
    "def group_diff_time(data, key, value, n, name):\n",
    "    data_temp = data[key + [value]].copy()\n",
    "    shift_value = data_temp.groupby(key)[value].shift(n)\n",
    "    data_temp[name] = data_temp[value] - shift_value\n",
    "    return data_temp[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**特征工程**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**operation+transaction**  \n",
    "1. 用户操作（成功，失败，缺失）频次/交易频次/行为频次，频率，转化率 \n",
    "2. 用户操作/交易/整体使用设备种类个数/差值/重合度\n",
    "3. 用户操作/交易/整体使用环境种类个数/差值/重合度\n",
    "4. 行为设备/环境缺失个数平均值、最大值\n",
    "5. 用户行为地点发生在中国境内的频次及频率\n",
    "6. 用户当前行为是否为常用设备(device1,device2,device_code,geo_code,province,city,district,ip,ip_sub,mac1),统计频次及频率\n",
    "7. 用户第一次操作/交易/行为时间，最后一次操作/交易/行为时间(actionTimestamp)\n",
    "8. 用户每天操作/交易/行为频次、转化率统计(mean,max,min,std,max-min,skew)\n",
    "9. 用户操作到交易时间间隔小于100秒的次数\n",
    "10. 用户每天交易第一个时间到最后一个时间时间差(mean, max, min, std)\n",
    "11. 设备、环境的热度(mean,max,min,skew,std,sum)，设备被用户操作的时间间隔(mean,max,min,std,skew,sum)\n",
    "12. 用户行为/操作/交易天数\n",
    "13. 用户当前行为发生设备、环境与前1行为的差异频次\n",
    "14. 设备、ip第一次出现时间、最后一次出现时间的时间差\n",
    "15. 用户同一天使用的操作类型、操作系统、版本、wifi的种类个数\n",
    "16. 用户交易时间和操作时间的重合度\n",
    "17. 用户行为时间的统计特征\n",
    "18. 设备的w2v特征\n",
    "19. 以用户最后一次行为时间做滑窗统计操作/交易/转化率特征\n",
    "20. 用户使用不同设备时间长度\n",
    "21. 用户行为时间差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_df['action_type'] = 1\n",
    "transaction_df['action_type'] = 2\n",
    "transaction_df['success'] = 1\n",
    "inner_cols = list(set(operation_df).intersection(set(transaction_df))) # 至合并operation_df和transaction_df有共同的特征列\n",
    "user_action = pd.concat([operation_df[inner_cols], transaction_df[inner_cols]], axis=0, ignore_index=True)\n",
    "user_action = user_action.sort_values(by=['UID', 'actionTime'])\n",
    "operation_df = operation_df.sort_values(by=['UID', 'actionTime'])\n",
    "transaction_df = transaction_df.sort_values(by=['UID', 'actionTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = user_action.groupby(['UID', 'action_type']).size().unstack().reset_index().fillna(0)\n",
    "gp.columns = ['UID', 'uid_operation_cnt', 'uid_trade_cnt']\n",
    "gp['uid_action_cnt'] = gp['uid_operation_cnt'] + gp['uid_trade_cnt']\n",
    "gp['uid_trade_ratio'] = gp['uid_trade_cnt'] / gp['uid_action_cnt']\n",
    "gp['uid_trade_operation_ratio'] = gp['uid_trade_ratio'] / (0.01+gp['uid_operation_cnt'])\n",
    "data = data.merge(gp, on=['UID'], how='left').fillna(0)\n",
    "\n",
    "stats = user_action[user_action['action_type']==1].copy()\n",
    "gp = stats.groupby(['UID', 'success']).size().unstack().reset_index().fillna(0)\n",
    "gp.columns = ['UID','uid_unknown_operation_cnt', 'uid_failure_operation_cnt', 'uid_success_operation_cnt']\n",
    "gp['uid_operation_cnt_diff'] = gp['uid_success_operation_cnt']-gp['uid_failure_operation_cnt']\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "data['uid_success_operation_ratio'] = data['uid_success_operation_cnt'] / (0.01+data['uid_operation_cnt'])\n",
    "data['uid_failure_operation_ratio'] = data['uid_failure_operation_cnt'] / (0.01+data['uid_operation_cnt'])\n",
    "data['uid_unknown_operation_ratio'] = data['uid_unknown_operation_cnt'] / (0.01+data['uid_operation_cnt'])\n",
    "data['uid_trade_success_operation_ratio'] = data['uid_trade_ratio'] / (0.01+data['uid_success_operation_cnt'])\n",
    "data['uid_trade_failure_operation_ratio'] = data['uid_trade_ratio'] / (0.01+data['uid_failure_operation_cnt'])\n",
    "data['uid_trade_unknown_operation_ratio'] = data['uid_trade_ratio'] / (0.01+data['uid_unknown_operation_cnt'])\n",
    "del data['uid_unknown_operation_cnt'], data['uid_trade_unknown_operation_ratio'], data['uid_unknown_operation_ratio']# 新的测试集里success没有缺失的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_cols = ['device1', 'device2', 'device_code', 'mac1', 'device_brand']\n",
    "tmp1 = user_action[user_action['action_type']==1].copy()\n",
    "tmp2 = user_action[user_action['action_type']==2].copy()\n",
    "for col in device_cols:\n",
    "    gp = user_action.groupby(['UID'])[col].nunique().reset_index().rename(columns={col: 'uid_action_'+col+'_nunique'})\n",
    "    data = data.merge(gp, on=['UID'], how='left')\n",
    "    gp1 = tmp1.groupby(['UID'])[col].nunique().reset_index().rename(columns={col: 'uid_operation_'+col+'_nunique'})\n",
    "    data = data.merge(gp1, on=['UID'], how='left')\n",
    "    gp2 = tmp2.groupby(['UID'])[col].nunique().reset_index().rename(columns={col: 'uid_trade_'+col+'_nunique'})\n",
    "    data = data.merge(gp2, on=['UID'], how='left')\n",
    "    data['uid_trade_operation_'+col+'_diff'] = data['uid_trade_'+col+'_nunique'] - data['uid_operation_'+col+'_nunique']\n",
    "for col in device_cols:\n",
    "    gp1 = tmp1.groupby(['UID']).apply(lambda x: set(x[col].tolist())).reset_index()\n",
    "    gp2 = tmp2.groupby(['UID']).apply(lambda x: set(x[col].tolist())).reset_index()\n",
    "    gp1.columns = ['UID', 'uid_operation_'+col+'_list']\n",
    "    gp2.columns = ['UID', 'uid_trade_'+col+'_list']\n",
    "    data = data.merge(gp1, on=['UID'], how='left')\n",
    "    data = data.merge(gp2, on=['UID'], how='left')\n",
    "    data['uid_trade_operation_'+col+'_intersection_ratio'] = data.apply(lambda x: calculate_intersection_ratio(x['uid_operation_'+col+'_list'], x['uid_trade_'+col+'_list']), axis=1)    \n",
    "    del data['uid_operation_'+col+'_list'], data['uid_trade_'+col+'_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cols = ['ip', 'ip_sub', 'geo_code', 'nation', 'city', 'district']\n",
    "tmp1 = user_action[user_action['action_type']==1].copy()\n",
    "tmp2 = user_action[user_action['action_type']==2].copy()\n",
    "for col in env_cols:\n",
    "    gp = user_action.groupby(['UID'])[col].nunique().reset_index().rename(columns={col: 'uid_action_'+col+'_nunique'})\n",
    "    data = data.merge(gp, on=['UID'], how='left')\n",
    "    gp1 = tmp1.groupby(['UID'])[col].nunique().reset_index().rename(columns={col: 'uid_operation_'+col+'_nunique'})\n",
    "    data = data.merge(gp1, on=['UID'], how='left')\n",
    "    gp2 = tmp2.groupby(['UID'])[col].nunique().reset_index().rename(columns={col: 'uid_trade_'+col+'_nunique'})\n",
    "    data = data.merge(gp2, on=['UID'], how='left')\n",
    "    data['uid_trade_operation_'+col+'_diff'] = data['uid_trade_'+col+'_nunique'] - data['uid_operation_'+col+'_nunique']\n",
    "for col in env_cols:\n",
    "    gp1 = tmp1.groupby(['UID']).apply(lambda x: set(x[col].tolist())).reset_index()\n",
    "    gp2 = tmp2.groupby(['UID']).apply(lambda x: set(x[col].tolist())).reset_index()\n",
    "    gp1.columns = ['UID', 'uid_operation_'+col+'_list']\n",
    "    gp2.columns = ['UID', 'uid_trade_'+col+'_list']\n",
    "    data = data.merge(gp1, on=['UID'], how='left')\n",
    "    data = data.merge(gp2, on=['UID'], how='left')\n",
    "    data['uid_trade_operation_'+col+'_intersection_ratio'] = data.apply(lambda x: calculate_intersection_ratio(x['uid_operation_'+col+'_list'], x['uid_trade_'+col+'_list']), axis=1)    \n",
    "    del data['uid_operation_'+col+'_list'], data['uid_trade_'+col+'_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = user_action.groupby(['UID'])[['device_miss_cnt', 'env_miss_cnt']].agg({'mean', 'max'})\n",
    "gp.columns = pd.Index([e[0]+\"_\"+e[1] for e in gp.columns.tolist()])\n",
    "gp.reset_index(inplace=True)\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "\n",
    "used_cols = ['ip_sub', 'device1', 'mac1','ip',\n",
    "             'device_code', 'device2', 'geo_code']\n",
    "for col in used_cols:\n",
    "    gp = user_action.groupby(['UID'])[col].count().reset_index().rename(columns={col: 'uid_action_nonan_'+col+'_count'})\n",
    "    data = data.merge(gp, on=['UID'], how='left')\n",
    "    data['uid_action_nonan_'+col+'_ratio'] = data['uid_action_nonan_'+col+'_count']/(0.01+data['uid_action_cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = user_action[user_action['is_china']>=0].copy()\n",
    "gp = tmp.groupby(['UID'])['is_china'].agg({'uid_action_in_china_count':'sum',\n",
    "                                           'uid_action_in_china_ratio':'mean'}).reset_index()\n",
    "data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['device1', 'ip', 'ip_sub', 'mac1', 'device2','device_code','geo_code','province','city','district']\n",
    "stats = user_action.copy()\n",
    "for col in used_cols:\n",
    "    gp = user_action.groupby(['UID', col]).size().reset_index().rename(columns={0: 'uid_action_favor_'+col+'_count'})\n",
    "    gp = gp.sort_values(by=['UID', 'uid_action_favor_'+col+'_count'])\n",
    "    gp = gp.groupby(['UID']).last().reset_index().rename(columns={col: 'uid_action_favor_'+col})\n",
    "    stats = stats.merge(gp, on=['UID'], how='left')\n",
    "    stats['is_action_favor_'+col] = 0\n",
    "    stats.loc[stats['uid_action_favor_'+col]==stats[col], 'is_action_favor_'+col] = 1\n",
    "    gp = stats.groupby(['UID'])['is_action_favor_'+col].agg({'is_action_favor_'+col+'_count': 'sum',\n",
    "                                                             'is_action_favor_'+col+'_mean': 'mean'}).reset_index()\n",
    "    data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = user_action.groupby(['UID'])['actionTimestamp'].agg({'uid_action_last_actionTimestamp': 'max',\n",
    "                                                          'uid_action_first_actionTimestamp': 'min'}).reset_index()\n",
    "gp['uid_actionTimestamp_timedelta'] = gp['uid_action_last_actionTimestamp'] - gp['uid_action_first_actionTimestamp']\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "\n",
    "gp = user_action[user_action['action_type']==2].groupby(['UID'])['actionTimestamp'].agg({'uid_trade_last_actionTimestamp': 'max',\n",
    "                                                                                         'uid_trade_first_actionTimestamp': 'min'}).reset_index()\n",
    "gp['uid_trade_actionTimestamp_timedelta'] = gp['uid_trade_last_actionTimestamp'] - gp['uid_trade_first_actionTimestamp']\n",
    "data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = user_action.groupby(['UID','day','action_type',]).size().unstack().fillna(0).reset_index()\n",
    "stats.columns = ['UID', 'day', 'uid_operation_day_count', 'uid_trade_day_count']\n",
    "stats['uid_action_day_count'] = stats['uid_operation_day_count']+stats['uid_trade_day_count']\n",
    "stats['uid_trade_day_ratio'] = stats['uid_trade_day_count']/stats['uid_action_day_count']\n",
    "gp = stats.groupby(['UID'])[['uid_action_day_count', 'uid_operation_day_count', 'uid_trade_day_count', 'uid_trade_day_ratio']].agg({'max',\n",
    "                                                                                                                                   'min',\n",
    "                                                                                                                                   'std',\n",
    "                                                                                                                                   'mean',\n",
    "                                                                                                                                   'skew'})\n",
    "gp.columns = pd.Index([e[0]+\"_\"+e[1] for e in gp.columns.tolist()])\n",
    "gp.reset_index(inplace=True)\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "data['uid_action_day_count_diff'] = data['uid_action_day_count_max'] - data['uid_action_day_count_min']\n",
    "data['uid_trade_day_count_diff'] = data['uid_trade_day_count_max'] - data['uid_trade_day_count_min']\n",
    "data['uid_operation_day_count_diff'] = data['uid_operation_day_count_max'] - data['uid_operation_day_count_min']\n",
    "data['uid_trade_day_ratio_diff'] = data['uid_trade_day_ratio_max'] - data['uid_trade_day_ratio_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_action = user_action.sort_values(by=['actionTime'])\n",
    "userid = user_action['UID'].unique()\n",
    "timespancount_dict = {'UID': [],\n",
    "                      'uid_operation_to_trade_timdelta_count': []}\n",
    "for uid in userid:\n",
    "    action_df = user_action[user_action['UID']==uid].copy()\n",
    "    actiontimespancount = getActionTimeSpan(action_df, 1, 2, timethred = 100)\n",
    "    timespancount_dict['UID'].append(uid)\n",
    "    timespancount_dict['uid_operation_to_trade_timdelta_count'].append(actiontimespancount)\n",
    "timespancount_dict = pd.DataFrame(timespancount_dict)\n",
    "data = data.merge(timespancount_dict, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = user_action[user_action['action_type']==2].copy()\n",
    "gp = stats.groupby(['UID', 'day'])['actionTimestamp'].agg({'uid_trade_day_last_actionTimestamp': 'max',\n",
    "                                                           'uid_trade_day_first_actionTimestamp':'min'}).reset_index()\n",
    "gp['uid_trade_day_timedelta'] = gp['uid_trade_day_last_actionTimestamp']-gp['uid_trade_day_first_actionTimestamp']\n",
    "gp = gp.groupby(['UID'])['uid_trade_day_timedelta'].agg({'uid_trade_day_timedelta_mean': 'mean',\n",
    "                                                         'uid_trade_day_timedelta_max': 'max',\n",
    "                                                         'uid_trade_day_timedelta_min': 'min',\n",
    "                                                         'uid_trade_day_timedelta_std': 'std',\n",
    "                                                         'uid_trade_day_timedelta_skew': 'skew'}).reset_index()\n",
    "data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols =['city', 'ip', 'nation','device2','geo_code', \n",
    "            'district','ip_sub','device1', 'mac1','device_brand',\n",
    "            'province','device_code']\n",
    "for col in used_cols:\n",
    "    stats = user_action.groupby([col])['UID'].nunique().reset_index().rename(columns={'UID': col+'_used_uid_nunique'})\n",
    "    tmp = user_action.merge(stats, on=[col], how='left')\n",
    "    gp = tmp.groupby(['UID'])[col+'_used_uid_nunique'].agg({col+'_used_uid_nunique_max': 'max',\n",
    "                                                            col+'_used_uid_nunique_mean': 'mean',\n",
    "                                                            col+'_used_uid_nunique_min': 'min',\n",
    "                                                            col+'_used_uid_nunique_skew': 'skew',\n",
    "                                                            col+'_used_uid_nunique_mean': 'std'}).reset_index()\n",
    "    data = data.merge(gp, on=['UID'], how='left')\n",
    "used_cols = ['device_code', 'ip', 'ip_sub', 'mac1', 'device1',\n",
    "             'device2', 'geo_code']\n",
    "for col in used_cols:    \n",
    "    stats = user_action[user_action[col].isnull()==False][['UID', 'actionTimestamp']+[col]].copy()\n",
    "    stats = stats.sort_values(by=[col, 'actionTimestamp'])\n",
    "    gp = stats.groupby(col)['actionTimestamp'].agg({col+'_used_user_actionTimestamp_min':'min',\n",
    "                                                    col+'_used_user_actionTimestamp_max':'max'}).reset_index()\n",
    "    gp[col+'_used_user_actionTimestamp_timedelta'] = gp[col+'_used_user_actionTimestamp_max'] - gp[col+'_used_user_actionTimestamp_min']\n",
    "    tmp = user_action.merge(gp, on=[col], how='left')\n",
    "    gp = tmp.groupby(['UID'])[col+'_used_user_actionTimestamp_timedelta'].agg({col+'_used_user_actionTimestamp_timedelta_max': 'max',\n",
    "                                                                               col+'_used_user_actionTimestamp_timedelta_min': 'min',\n",
    "                                                                               col+'_used_user_actionTimestamp_timedelta_mean': 'mean',\n",
    "                                                                               col+'_used_user_actionTimestamp_timedelta_skew': 'skew',\n",
    "                                                                               col+'_used_user_actionTimestamp_timedelta_std': 'std'})\n",
    "    gp.reset_index(inplace=True)\n",
    "    data = data.merge(gp, on=['UID'], how='left')\n",
    "\n",
    "used_cols = ['device_code', 'ip', 'ip_sub', 'mac1', 'device1',\n",
    "             'device2', 'geo_code']\n",
    "for col in used_cols:    \n",
    "    stats = user_action[user_action[col].isnull()==False][['UID', 'actionTimestamp']+[col]].copy()\n",
    "    stats = stats.sort_values(by=[col, 'actionTimestamp'])\n",
    "    stats['timedelta'] = group_diff_time(stats, [col], 'actionTimestamp', 1, 'timedelta')\n",
    "    gp = stats.groupby([col])['timedelta'].agg({col+'_used_timedelta_mean': 'mean',\n",
    "                                                col+'_used_timedelta_skew': 'skew',\n",
    "                                                col+'_used_timedelta_std': 'std',\n",
    "                                                col+'_used_timedelta_max': 'max',\n",
    "                                                col+'_used_timedelta_min': 'min',}).reset_index()\n",
    "    tmp = user_action.merge(gp, on=[col], how='left')\n",
    "    columns = [f for f in gp.columns if f not in ['UID']]\n",
    "    gp = tmp.groupby(['UID'])[columns].agg({'max', 'min', 'mean', 'skew', 'std'})\n",
    "    gp.columns = pd.Index([e[0]+\"_\"+e[1] for e in gp.columns.tolist()])\n",
    "    gp.reset_index(inplace=True)\n",
    "    data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = user_action.groupby(['UID'])['day'].nunique().reset_index().rename(columns={'day': 'uid_action_day_nunique'})\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "data['uid_action_day_nunique'].fillna(0, inplace=True)\n",
    "gp = user_action[user_action['action_type']==1].groupby(['UID'])['day'].nunique().reset_index().rename(columns={'day': 'uid_operation_day_nunique'})\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "data['uid_operation_day_nunique'].fillna(0, inplace=True)\n",
    "gp = user_action[user_action['action_type']==2].groupby(['UID'])['day'].nunique().reset_index().rename(columns={'day': 'uid_trade_day_nunique'})\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "data['uid_trade_day_nunique'].fillna(0, inplace=True)\n",
    "data['uid_trade_day_nunique_ratio'] = data['uid_trade_day_nunique']/(0.01+data['uid_action_day_nunique'])\n",
    "data['uid_operation_day_nunique_ratio'] = data['uid_operation_day_nunique']/(0.01+data['uid_action_day_nunique'])\n",
    "data['uid_trade_operation_day_nunique_ratio'] = data['uid_trade_day_nunique']/(0.01+data['uid_operation_day_nunique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['device1', 'device2', 'device_code', 'mac1', 'device_brand', 'ip', 'ip_sub', 'geo_code']\n",
    "for col in used_cols:\n",
    "    gp = user_action.groupby(['UID', 'day'])[col].nunique().reset_index().rename(columns={col:'uid_action_use_'+col+'_day_nunique'})\n",
    "    gp = gp.groupby(['UID'])['uid_action_use_'+col+'_day_nunique'].agg({'uid_action_use_'+col+'_day_nunique_mean': 'mean',\n",
    "                                                                        'uid_action_use_'+col+'_day_nunique_max': 'max',\n",
    "                                                                        'uid_action_use_'+col+'_day_nunique_min': 'min',\n",
    "                                                                        'uid_action_use_'+col+'_day_nunique_skew': 'skew',\n",
    "                                                                        'uid_action_use_'+col+'_day_nunique_std': 'std'}).reset_index()\n",
    "    data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = user_action[user_action['action_type']==1].copy()\n",
    "tmp2 = user_action[user_action['action_type']==2].copy()\n",
    "gp1 = tmp1.groupby(['UID']).apply(lambda x: set(x['day'].tolist())).reset_index()\n",
    "gp2 = tmp2.groupby(['UID']).apply(lambda x: set(x['day'].tolist())).reset_index()\n",
    "gp1.columns = ['UID', 'uid_operation_day_list']\n",
    "gp2.columns = ['UID', 'uid_trade_day_list']\n",
    "data = data.merge(gp1, on=['UID'], how='left')\n",
    "data = data.merge(gp2, on=['UID'], how='left')\n",
    "data['uid_trade_operation_day_intersection_ratio'] = data.apply(lambda x: calculate_intersection_ratio(x['uid_operation_day_list'], x['uid_trade_day_list']), axis=1)    \n",
    "del data['uid_operation_day_list'], data['uid_trade_day_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = user_action.groupby(['UID'])['day'].agg({'uid_action_day_mean': 'mean',\n",
    "                                              'uid_action_day_max': 'max',\n",
    "                                              'uid_action_day_min': 'min',\n",
    "                                              'uid_action_day_sum': 'sum',\n",
    "                                              'uid_action_day_skew': 'skew',\n",
    "                                              'uid_action_day_std': 'std'}).reset_index()\n",
    "data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = w2v_features(data, user_action, ['UID'], 'ip', 10, 'w2v_ip_')\n",
    "data = w2v_features(data, user_action, ['UID'], 'ip_sub', 10, 'w2v_ip_sub_')\n",
    "data = w2v_features(data, user_action, ['UID'], 'mac1', 10, 'w2v_mac1_')\n",
    "# (df, df_feats, key, value, word_ndim,colname):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = user_action.groupby(['UID'])['actionTimestamp'].max().reset_index().rename(columns={'actionTimestamp': 'last_actionTimestamp'})\n",
    "tmp = user_action.merge(stats, on=['UID'], how='left')\n",
    "tmp['timedelta'] = tmp['last_actionTimestamp']-tmp['actionTimestamp']\n",
    "for i in [600,1800,3600,36000,86400,259200,864000]:\n",
    "    gp = tmp[tmp['timedelta']<=i].copy()\n",
    "    gp = gp.groupby(['UID', 'action_type']).size().unstack().fillna(0).reset_index()\n",
    "    gp.columns = ['UID', 'uid_operation_cnt_in_{}s'.format(i), 'uid_trade_cnt_in_{}s'.format(i)]\n",
    "    gp['uid_action_cnt_in_{}s'.format(i)] = gp['uid_operation_cnt_in_{}s'.format(i)]+gp['uid_trade_cnt_in_{}s'.format(i)]\n",
    "    gp['uid_trade_ratio_in_{}s'.format(i)] = gp['uid_trade_cnt_in_{}s'.format(i)]/gp['uid_action_cnt_in_{}s'.format(i)]\n",
    "    gp['uid_trade_operation_ratio_in_{}s'.format(i)] = gp['uid_trade_cnt_in_{}s'.format(i)]/(0.01+gp['uid_operation_cnt_in_{}s'.format(i)])\n",
    "    data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transaction**  \n",
    "1. 用户各平台交易频次/频率\n",
    "2. 用户整体/各平台交易金额\n",
    "3. 用户使用的资金来源、交易方式、交易商家、交易金额、账户、转出账户、转入账户、营销活动种类个数\n",
    "4. 用户当前交易行为是否使用常用的资金来源、交易方式、交易商家、账户\n",
    "5. 用户在黑名单商户交易频次\n",
    "6. 用户进行交易的商家\\账户的热度,该商家第一次交易时间、最后一次交易时间、交易时间间隔\n",
    "7. 用户参与活动交易的频次、金额总和\n",
    "8. 用户进行交易的账户、转出账户、转入账户的热度\n",
    "9. 商家\\交易金额\\bal的w2v特征\n",
    "10. 用户同一天使用的交易类型、账户、活动、资金源的种类个数\n",
    "11. 商户画像（金额、子商户个数， 转入/转出账户个数）\n",
    "12. bal的统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = transaction_df[transaction_df['channel'].isin([140, 102, 119])==True].groupby(['UID', 'channel']).size().unstack().reset_index().fillna(0)\n",
    "gp.columns = ['UID', 'uid_trade_count_on_channel_102', 'uid_trade_count_on_channel_119', 'uid_trade_count_on_channel_140']\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "data['uid_trade_ratio_on_channel_102'] = data['uid_trade_count_on_channel_102']/(0.01+data['uid_trade_cnt'])\n",
    "data['uid_trade_ratio_on_channel_119'] = data['uid_trade_count_on_channel_119']/(0.01+data['uid_trade_cnt'])\n",
    "data['uid_trade_ratio_on_channel_140'] = data['uid_trade_count_on_channel_140']/(0.01+data['uid_trade_cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = transaction_df.groupby(['UID'])['trans_amt'].agg({'uid_trans_amt_mean': 'mean',\n",
    "                                                       'uid_trans_amt_sum': 'sum',\n",
    "                                                       'uid_trans_amt_max': 'max', \n",
    "                                                       'uid_trans_amt_min': 'min',\n",
    "                                                       'uid_trans_amt_std': 'std',\n",
    "                                                       'uid_trans_amt_skew': 'skew'}).reset_index()\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "data['uid_trans_amt_diff'] = data['uid_trans_amt_max']-data['uid_trans_amt_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['amt_src1', 'amt_src2', 'trans_type1', 'trans_type2', 'merchant', 'trans_amt', 'acc_id1', 'acc_id2', 'acc_id3', 'market_code', 'market_type']\n",
    "for col in used_cols:\n",
    "    gp = transaction_df.groupby(['UID'])[col].nunique().reset_index().rename(columns={col: 'uid_trade_'+col+'_nunique'})\n",
    "    data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['amt_src1', 'trans_type1', 'merchant', 'acc_id1']\n",
    "stats = transaction_df.copy()\n",
    "for col in used_cols:\n",
    "    gp = transaction_df.groupby(['UID', col]).size().reset_index().rename(columns={0: 'uid_trade_favor_'+col+'_count'})\n",
    "    gp = gp.sort_values(by=['UID', 'uid_trade_favor_'+col+'_count'])\n",
    "    gp = gp.groupby(['UID']).last().reset_index().rename(columns={col: 'uid_trade_favor_'+col})\n",
    "    stats = stats.merge(gp, on=['UID'], how='left')\n",
    "    stats['is_trade_favor_'+col] = 0\n",
    "    stats.loc[stats['uid_trade_favor_'+col]==stats[col], 'is_trade_favor_'+col] = 1\n",
    "    gp = stats.groupby(['UID'])['is_trade_favor_'+col].agg({'is_trade_favor_'+col+'_count': 'sum',\n",
    "                                                            'is_trade_favor_'+col+'_mean': 'mean'}).reset_index()\n",
    "    data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = transaction_df.groupby(['merchant'])['UID'].agg({'merchant_traded_uid_count': 'count',\n",
    "                                                         'merchant_traded_uid_nunique': 'nunique'}).reset_index()\n",
    "tmp = transaction_df.merge(stats, on=['merchant'], how='left')\n",
    "gp = tmp.groupby(['UID'])[['merchant_traded_uid_count', 'merchant_traded_uid_nunique']].agg({'max', 'mean', 'min', 'skew', 'sum', 'std'})\n",
    "gp.columns = pd.Index([e[0]+\"_\"+e[1] for e in gp.columns.tolist()])\n",
    "gp.reset_index(inplace=True)\n",
    "data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df['is_market'] = transaction_df['market_code'].apply(lambda x: 'market' if pd.isna(x)==False else 'nomarket')\n",
    "gp = transaction_df.groupby(['UID', 'is_market'])['trans_amt'].agg({'count', 'mean'}).unstack().fillna(0)\n",
    "gp.columns = pd.Index(['uid_trade_on_'+e[1]+\"_\"+e[0] for e in gp.columns.tolist()])\n",
    "gp.reset_index(inplace=True)\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "data['uid_trade_market_count_diff'] = data['uid_trade_on_market_count'] - data['uid_trade_on_nomarket_count']\n",
    "data['uid_trade_market_mean_diff'] = data['uid_trade_on_market_mean'] - data['uid_trade_on_nomarket_mean']\n",
    "data['uid_trade_on_market_ratio'] = data['uid_trade_on_market_count'] / (0.01+data['uid_action_cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = transaction_df[transaction_df['acc_id1'].isnull()==False].copy()\n",
    "gp = gp.groupby(['acc_id1'])['UID'].nunique().reset_index().rename(columns={'UID': 'acc_id1_traded_uid_nunique'})\n",
    "stats = transaction_df.merge(gp, on=['acc_id1'], how='left')\n",
    "gp = stats.groupby(['UID'])['acc_id1_traded_uid_nunique'].agg({'acc_id1_traded_uid_nunique_mean': 'mean',\n",
    "                                                               'acc_id1_traded_uid_nunique_max': 'max',\n",
    "                                                               'acc_id1_traded_uid_nunique_min': 'min',\n",
    "                                                               'acc_id1_traded_uid_nunique_skew': 'skew',\n",
    "                                                               'acc_id1_traded_uid_nunique_std': 'std'}).reset_index()\n",
    "gp.fillna(0, inplace=True)\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "\n",
    "gp = transaction_df[transaction_df['acc_id2'].isnull()==False].copy()\n",
    "gp = gp.groupby(['acc_id2'])['UID'].nunique().reset_index().rename(columns={'UID': 'acc_id2_traded_uid_nunique'})\n",
    "stats = transaction_df.merge(gp, on=['acc_id2'], how='left')\n",
    "gp = stats.groupby(['UID'])['acc_id2_traded_uid_nunique'].agg({'acc_id2_traded_uid_nunique_mean': 'mean',\n",
    "                                                               'acc_id2_traded_uid_nunique_max': 'max',\n",
    "                                                               'acc_id2_traded_uid_nunique_max': 'min',\n",
    "                                                               'acc_id2_traded_uid_nunique_skew': 'skew',\n",
    "                                                               'acc_id2_traded_uid_nunique_std': 'std'}).reset_index()\n",
    "gp.fillna(0, inplace=True)\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "\n",
    "gp = transaction_df[transaction_df['acc_id3'].isnull()==False].copy()\n",
    "gp = gp.groupby(['acc_id3'])['UID'].nunique().reset_index().rename(columns={'UID': 'acc_id3_traded_uid_nunique'})\n",
    "stats = transaction_df.merge(gp, on=['acc_id3'], how='left')\n",
    "gp = stats.groupby(['UID'])['acc_id3_traded_uid_nunique'].agg({'acc_id3_traded_uid_nunique_mean': 'mean',\n",
    "                                                               'acc_id3_traded_uid_nunique_max': 'max',\n",
    "                                                               'acc_id3_traded_uid_nunique_min': 'min',\n",
    "                                                               'acc_id3_traded_uid_nunique_skew': 'skew',\n",
    "                                                               'acc_id3_traded_uid_nunique_std': 'std'}).reset_index()\n",
    "gp.fillna(0, inplace=True)\n",
    "data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = w2v_features(data, transaction_df, ['UID'], 'merchant', 10, 'w2v_merchant_')\n",
    "data = w2v_features(data, transaction_df, ['UID'], 'trans_amt', 10, 'w2v_trans_amt_')\n",
    "data = w2v_features(data, transaction_df, ['UID'], 'bal', 10, 'w2v_bal_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['merchant', 'acc_id1', 'acc_id2', 'acc_id3']\n",
    "test_uid = list(data[data['Tag']==0.5]['UID'].unique())\n",
    "for col in used_cols:\n",
    "    stats = transaction_df[transaction_df[col].isnull()==False][['UID', 'actionTimestamp']+[col]].copy()\n",
    "    stats = stats.sort_values(by=[col, 'actionTimestamp'])\n",
    "    gp = stats.groupby(col)['actionTimestamp'].agg({col+'_traded_user_actionTimestamp_min': 'min',\n",
    "                                                    col+'_traded_user_actionTimestamp_max': 'max'})\n",
    "    gp[col+'_traded_user_actionTimestamp_timedelta'] = gp[col+'_traded_user_actionTimestamp_max']-gp[col+'_traded_user_actionTimestamp_min']\n",
    "    gp.reset_index(inplace=True)\n",
    "    tmp = transaction_df.merge(gp, on=[col], how='left')\n",
    "    gp = tmp.groupby(['UID'])[col+'_traded_user_actionTimestamp_timedelta'].agg({col+'_traded_user_actionTimestamp_timedelta_max':'max',\n",
    "                                                                                 col+'_traded_user_actionTimestamp_timedelta_min': 'min',\n",
    "                                                                                 col+'_traded_user_actionTimestamp_timedelta_mean': 'mean',\n",
    "                                                                                 col+'_traded_user_actionTimestamp_timedelta_skew': 'skew',\n",
    "                                                                                 col+'_traded_user_actionTimestamp_timedelta_std': 'std'})\n",
    "    gp.reset_index(inplace=True)\n",
    "    data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['amt_src1', 'amt_src2', 'trans_type1', 'trans_type2', 'merchant', 'acc_id1', 'acc_id2', 'acc_id3', 'market_code', 'market_type']\n",
    "for col in used_cols:\n",
    "    gp = transaction_df.groupby(['UID', 'day'])[col].nunique().reset_index().rename(columns={col:'uid_trade_use_'+col+'_day_nunique'})\n",
    "    gp = gp.groupby(['UID'])['uid_trade_use_'+col+'_day_nunique'].agg({'uid_trade_use_'+col+'_day_nunique_mean': 'mean',\n",
    "                                                                       'uid_trade_use_'+col+'_day_nunique_max': 'max',\n",
    "                                                                       'uid_trade_use_'+col+'_day_nunique_min': 'min',\n",
    "                                                                       'uid_trade_use_'+col+'_day_nunique_skew': 'skew',\n",
    "                                                                       'uid_trade_use_'+col+'_day_nunique_std': 'std'}).reset_index()\n",
    "    data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = {'code1': ['nunique'],\n",
    "       'code2': ['nunique'],\n",
    "       'acc_id1': ['nunique'],\n",
    "       'acc_id2': ['nunique'],\n",
    "       'acc_id3': ['nunique'],\n",
    "       'ip': ['nunique'],\n",
    "       'geo_code': ['nunique'],\n",
    "       'trans_amt': ['mean', 'max', 'min', 'std', 'skew'],\n",
    "       'day': ['mean', 'max', 'min', 'std', 'skew']}\n",
    "gp = transaction_df.groupby(['merchant']).agg(agg)\n",
    "gp.columns = pd.Index(['merchant_'+e[0]+\"_\"+e[1] for e in gp.columns.tolist()])\n",
    "gp.reset_index(inplace=True)\n",
    "columns = [f for f in gp.columns if f not in ['UID']]\n",
    "tmp = transaction_df.merge(gp, on=['merchant'], how='left')\n",
    "gp = tmp.groupby(['UID'])[columns].agg({'max', 'min', 'sum', 'mean', 'std', 'skew'})\n",
    "gp.columns = pd.Index(['uid_'+e[0]+\"_\"+e[1] for e in gp.columns.tolist()])\n",
    "gp.reset_index(inplace=True)\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "data['uid_trans_amt_mean_vs_merchant_trans_amt_mean'] = data['uid_trans_amt_mean'] / (0.01+data['uid_merchant_trans_amt_mean_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = transaction_df.groupby(['UID'])['bal'].agg({'uid_bal_mean': 'mean',\n",
    "                                                 'uid_bal_sum': 'sum',\n",
    "                                                 'uid_bal_max': 'max', \n",
    "                                                 'uid_bal_min': 'min',\n",
    "                                                 'uid_bal_std': 'std',\n",
    "                                                 'uid_bal_skew': 'skew',\n",
    "                                                }).reset_index()\n",
    "data = data.merge(gp, on=['UID'], how='left')\n",
    "data['uid_bal_diff'] = data['uid_bal_max']-data['uid_bal_min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**operation**\n",
    "1. 用户使用的操作类型、操作系统、版本、wifi的种类个数\n",
    "2. 用户当前操作是否是最常用用的操作系统、版本、wifi环境、mac2地址\n",
    "3. 用户第一次使用该设备、该wifi设备的时间、及最后一次使用时间，时间差(mean,max,skew,min,std)\n",
    "4. 用户同一天使用的操作类型、操作系统、版本、wifi的种类个数\n",
    "5. 设备、环境使用的最早时间和最晚时间，时间间隔\n",
    "6. 操作类型、操作版本、wifi、mac2的w2v特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['mode', 'os', 'version', 'os_version', 'wifi', 'mac2']\n",
    "for col in used_cols:\n",
    "    gp = operation_df.groupby(['UID'])[col].nunique().reset_index().rename(columns={col: 'uid_operation_'+col+'_nunique'})\n",
    "    data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['os', 'version', 'os_version', 'wifi', 'mac2']\n",
    "stats = operation_df.copy()\n",
    "for col in used_cols:\n",
    "    gp = operation_df.groupby(['UID', col]).size().reset_index().rename(columns={0: 'uid_operation_favor_'+col+'_count'})\n",
    "    gp = gp.sort_values(by=['UID', 'uid_operation_favor_'+col+'_count'])\n",
    "    gp = gp.groupby(['UID']).last().reset_index().rename(columns={col: 'uid_operation_favor_'+col})\n",
    "    stats = stats.merge(gp, on=['UID'], how='left')\n",
    "    stats['is_operation_favor_'+col] = 0\n",
    "    stats.loc[stats['uid_operation_favor_'+col]==stats[col], 'is_operation_favor_'+col] = 1\n",
    "    gp = stats.groupby(['UID'])['is_operation_favor_'+col].agg({'is_operation_favor_'+col+'_count': 'sum',\n",
    "                                                                'is_operation_favor_'+col+'_mean': 'mean'}).reset_index()\n",
    "    data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['os', 'version', 'os_version', 'wifi', 'mac2']\n",
    "for col in used_cols:\n",
    "    tmp = operation_df[['UID', 'actionTimestamp']+[col]].copy()\n",
    "    gp = tmp.groupby(['UID', col])['actionTimestamp'].agg({col+'_opered_user_actionTimestamp_min': 'min',\n",
    "                                                           col+'_opered_user_actionTimestamp_max': 'max'})\n",
    "    gp[col+'_opered_user_actionTimestamp_timedelta'] = gp[col+'_opered_user_actionTimestamp_max']-gp[col+'_opered_user_actionTimestamp_min']\n",
    "    gp.reset_index(inplace=True)\n",
    "    gp = gp.groupby(['UID'])[col+'_opered_user_actionTimestamp_timedelta'].agg({col+'_opered_user_actionTimestamp_timedelta_mean': 'mean',\n",
    "                                                                                col+'_opered_user_actionTimestamp_timedelta_max': 'max', \n",
    "                                                                                col+'_opered_user_actionTimestamp_timedelta_min': 'min',\n",
    "                                                                                col+'_opered_user_actionTimestamp_timedelta_skew': 'skew',\n",
    "                                                                                col+'_opered_user_actionTimestamp_timedelta_std': 'std'\n",
    "                                                                                }).reset_index()\n",
    "    data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['os', 'version', 'os_version', 'wifi', 'mac2']\n",
    "for col in used_cols:\n",
    "    gp = operation_df.groupby(['UID', 'day'])[col].nunique().reset_index().rename(columns={col:'uid_use_'+col+'_day_nunique'})\n",
    "    gp = gp.groupby(['UID'])['uid_use_'+col+'_day_nunique'].agg({'uid_use_'+col+'_day_nunique_mean': 'mean',\n",
    "                                                                 'uid_use_'+col+'_day_nunique_max': 'max',\n",
    "                                                                 'uid_use_'+col+'_day_nunique_min': 'min',\n",
    "                                                                 'uid_use_'+col+'_day_nunique_skew': 'skew',\n",
    "                                                                 'uid_use_'+col+'_day_nunique_std': 'std'}).reset_index()\n",
    "    data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used_cols = ['os','version', 'os_version', 'wifi', 'mac2']\n",
    "used_cols = ['os', 'os_version', 'wifi']\n",
    "for col in used_cols:    \n",
    "    stats = operation_df[operation_df[col].isnull()==False][['UID', 'actionTimestamp']+[col]].copy()\n",
    "    stats = stats.sort_values(by=[col, 'actionTimestamp'])\n",
    "    stats['timedelta'] = group_diff_time(stats, [col], 'actionTimestamp', 1, 'timedelta')\n",
    "    gp = stats.groupby([col])['timedelta'].agg({col+'_used_timedelta_mean': 'mean',\n",
    "                                                col+'_used_timedelta_skew': 'skew',\n",
    "                                                col+'_used_timedelta_std': 'std',\n",
    "                                                col+'_used_timedelta_max': 'max',\n",
    "                                                col+'_used_timedelta_min': 'min'}).reset_index()\n",
    "    tmp = operation_df.merge(gp, on=[col], how='left')\n",
    "    columns = [f for f in gp.columns if f not in ['UID', col]]\n",
    "#     gp = tmp.groupby(['UID'])[columns].agg({'max', 'min', 'mean', 'skew', 'std'})\n",
    "    gp = tmp.groupby(['UID']).agg({'max', 'min', 'mean', 'skew', 'std'})\n",
    "    gp.columns = pd.Index([e[0]+\"_\"+e[1] for e in gp.columns.tolist()])\n",
    "    gp.reset_index(inplace=True)\n",
    "    data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = w2v_features(data, operation_df, ['UID'], 'mode', 10, 'w2v_mode_')\n",
    "data = w2v_features(data, operation_df, ['UID'], 'mac2', 10, 'w2v_mac2_')\n",
    "data = w2v_features(data, operation_df, ['UID'], 'wifi', 10, 'w2v_wifi_')\n",
    "data = w2v_features(data, operation_df, ['UID'], 'os', 10, 'w2v_os_')\n",
    "data = w2v_features(data, operation_df, ['UID'], 'version', 10, 'w2v_version_')\n",
    "data = w2v_features(data, operation_df, ['UID'], 'os_version', 10, 'w2v_os_version_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = {'channel': ['nunique'],\n",
    "       'bal': ['mean', 'max', 'min', 'std', 'skew']}\n",
    "gp = transaction_df.groupby(['merchant']).agg(agg)\n",
    "gp.columns = pd.Index(['merchant_'+e[0]+\"_\"+e[1] for e in gp.columns.tolist()])\n",
    "gp.reset_index(inplace=True)\n",
    "columns = [f for f in gp.columns if f not in ['UID']]\n",
    "tmp = transaction_df.merge(gp, on=['merchant'], how='left')\n",
    "gp = tmp.groupby(['UID'])[columns].agg({'max', 'min', 'sum', 'mean', 'std', 'skew'})\n",
    "gp.columns = pd.Index(['uid_'+e[0]+\"_\"+e[1] for e in gp.columns.tolist()])\n",
    "gp.reset_index(inplace=True)\n",
    "data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = transaction_df[['UID', 'actionTimestamp']].copy()\n",
    "tmp = tmp.sort_values(by=['UID', 'actionTimestamp'])\n",
    "tmp['timedelta'] = group_diff_time(tmp, ['UID'], 'actionTimestamp', 1, 'timedelta')\n",
    "gp = tmp.groupby(['UID'])['timedelta'].agg({'user_trade_timedelta_mean': 'mean',\n",
    "                                           'user_trade_timedelta_max': 'max',\n",
    "                                           'user_trade_timedelta_min': 'min',\n",
    "                                           'user_trade_timedelta_sum': 'sum',\n",
    "                                           'user_trade_timedelta_std': 'std',\n",
    "                                           'user_trade_timedelta_skew': 'skew'}).reset_index()\n",
    "data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModeTimeSpan(df_action_of_userid, actiontypeA, actiontypeB, timethred):\n",
    "    timespan_list = []\n",
    "    i = 0\n",
    "    while i < (len(df_action_of_userid) - 1):\n",
    "        if df_action_of_userid['mode'].iat[i] == actiontypeA:\n",
    "            timeA = df_action_of_userid['actionTimestamp'].iat[i]\n",
    "            for j in range(i + 1, len(df_action_of_userid)):\n",
    "                if df_action_of_userid['mode'].iat[j] == actiontypeA:\n",
    "                    timeA = df_action_of_userid['actionTimestamp'].iat[j]\n",
    "                if df_action_of_userid['mode'].iat[j] == actiontypeB:\n",
    "                    timeB = df_action_of_userid['actionTimestamp'].iat[j]\n",
    "                    timespan_list.append(timeB-timeA)\n",
    "                    i = j\n",
    "                    break\n",
    "        i += 1\n",
    "    return np.sum(np.array(timespan_list) <= timethred) / (np.sum(np.array(timespan_list)) + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = operation_df[['UID', 'mode', 'actionTimestamp']].copy()\n",
    "tmp2 = transaction_df[['UID', 'actionTimestamp']].copy()\n",
    "tmp2['mode'] = 'trade'\n",
    "user_action = pd.concat([tmp1, tmp2], axis=0, ignore_index=True)\n",
    "user_action = user_action.sort_values(by=['UID', 'actionTimestamp'])\n",
    "\n",
    "userid = user_action['UID'].unique()\n",
    "timespancount_dict = {'UID': [],\n",
    "                      'uid_operation_mode1_to_trade_timdelta_count': []}\n",
    "for uid in userid:\n",
    "    action_df = user_action[user_action['UID']==uid].copy()\n",
    "    actiontimespancount = getModeTimeSpan(action_df, 'c8741ce15ceac2a4', 'trade', timethred = 100)\n",
    "    timespancount_dict['UID'].append(uid)\n",
    "    timespancount_dict['uid_operation_mode1_to_trade_timdelta_count'].append(actiontimespancount)\n",
    "timespancount_dict = pd.DataFrame(timespancount_dict)\n",
    "data = data.merge(timespancount_dict, on=['UID'], how='left')\n",
    "\n",
    "timespancount_dict = {'UID': [],\n",
    "                      'uid_operation_mode2_to_trade_timdelta_count': []}\n",
    "for uid in userid:\n",
    "    action_df = user_action[user_action['UID']==uid].copy()\n",
    "    actiontimespancount = getModeTimeSpan(action_df, 'acfaded7e04e7ba0', 'trade', timethred = 100)\n",
    "    timespancount_dict['UID'].append(uid)\n",
    "    timespancount_dict['uid_operation_mode2_to_trade_timdelta_count'].append(actiontimespancount)\n",
    "timespancount_dict = pd.DataFrame(timespancount_dict)\n",
    "data = data.merge(timespancount_dict, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df = transaction_df.sort_values(by=['UID', 'actionTimestamp'])\n",
    "agg = {'channel': ['nunique'],\n",
    "       'acc_id2': ['nunique'],\n",
    "       'acc_id3': ['nunique'],\n",
    "       'amt_src1': ['nunique'],\n",
    "       'amt_src2': ['nunique'],\n",
    "       'ip': ['nunique'],\n",
    "       'day': ['mean', 'max', 'min', 'std', 'skew'],\n",
    "       'trans_amt': ['mean', 'max', 'min', 'std', 'skew'],\n",
    "       'bal': ['mean', 'max', 'min', 'std', 'skew', 'first', 'last']}\n",
    "gp = transaction_df.groupby(['acc_id1']).agg(agg)\n",
    "gp.columns = pd.Index(['acc_id1_'+e[0]+\"_\"+e[1] for e in gp.columns.tolist()])\n",
    "gp.reset_index(inplace=True)\n",
    "columns = [f for f in gp.columns if f not in ['UID']]\n",
    "tmp = transaction_df.merge(gp, on=['acc_id1'], how='left')\n",
    "gp = tmp.groupby(['UID'])[columns].agg({'max', 'min', 'sum', 'mean', 'std', 'skew'})\n",
    "gp.columns = pd.Index(['uid_'+e[0]+\"_\"+e[1] for e in gp.columns.tolist()])\n",
    "gp.reset_index(inplace=True)\n",
    "data = data.merge(gp, on=['UID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>Tag</th>\n",
       "      <th>uid_operation_cnt</th>\n",
       "      <th>uid_trade_cnt</th>\n",
       "      <th>uid_action_cnt</th>\n",
       "      <th>uid_trade_ratio</th>\n",
       "      <th>uid_trade_operation_ratio</th>\n",
       "      <th>uid_failure_operation_cnt</th>\n",
       "      <th>uid_success_operation_cnt</th>\n",
       "      <th>uid_operation_cnt_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>uid_acc_id1_bal_first_max</th>\n",
       "      <th>uid_acc_id1_bal_first_std</th>\n",
       "      <th>uid_acc_id1_bal_first_mean</th>\n",
       "      <th>uid_acc_id1_bal_first_min</th>\n",
       "      <th>uid_acc_id1_bal_last_skew</th>\n",
       "      <th>uid_acc_id1_bal_last_sum</th>\n",
       "      <th>uid_acc_id1_bal_last_max</th>\n",
       "      <th>uid_acc_id1_bal_last_std</th>\n",
       "      <th>uid_acc_id1_bal_last_mean</th>\n",
       "      <th>uid_acc_id1_bal_last_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1508 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     UID  Tag  uid_operation_cnt  uid_trade_cnt  uid_action_cnt  \\\n",
       "0  10000  1.0                0.0            0.0             0.0   \n",
       "1  10001  0.0                0.0            0.0             0.0   \n",
       "2  10002  0.0                0.0            0.0             0.0   \n",
       "3  10003  0.0                0.0            0.0             0.0   \n",
       "4  10004  0.0                1.0            0.0             1.0   \n",
       "\n",
       "   uid_trade_ratio  uid_trade_operation_ratio  uid_failure_operation_cnt  \\\n",
       "0              0.0                        0.0                        NaN   \n",
       "1              0.0                        0.0                        NaN   \n",
       "2              0.0                        0.0                        NaN   \n",
       "3              0.0                        0.0                        NaN   \n",
       "4              0.0                        0.0                        0.0   \n",
       "\n",
       "   uid_success_operation_cnt  uid_operation_cnt_diff  \\\n",
       "0                        NaN                     NaN   \n",
       "1                        NaN                     NaN   \n",
       "2                        NaN                     NaN   \n",
       "3                        NaN                     NaN   \n",
       "4                        1.0                     1.0   \n",
       "\n",
       "             ...             uid_acc_id1_bal_first_max  \\\n",
       "0            ...                                   NaN   \n",
       "1            ...                                   NaN   \n",
       "2            ...                                   NaN   \n",
       "3            ...                                   NaN   \n",
       "4            ...                                   NaN   \n",
       "\n",
       "   uid_acc_id1_bal_first_std  uid_acc_id1_bal_first_mean  \\\n",
       "0                        NaN                         NaN   \n",
       "1                        NaN                         NaN   \n",
       "2                        NaN                         NaN   \n",
       "3                        NaN                         NaN   \n",
       "4                        NaN                         NaN   \n",
       "\n",
       "   uid_acc_id1_bal_first_min  uid_acc_id1_bal_last_skew  \\\n",
       "0                        NaN                        NaN   \n",
       "1                        NaN                        NaN   \n",
       "2                        NaN                        NaN   \n",
       "3                        NaN                        NaN   \n",
       "4                        NaN                        NaN   \n",
       "\n",
       "   uid_acc_id1_bal_last_sum  uid_acc_id1_bal_last_max  \\\n",
       "0                       NaN                       NaN   \n",
       "1                       NaN                       NaN   \n",
       "2                       NaN                       NaN   \n",
       "3                       NaN                       NaN   \n",
       "4                       NaN                       NaN   \n",
       "\n",
       "   uid_acc_id1_bal_last_std  uid_acc_id1_bal_last_mean  \\\n",
       "0                       NaN                        NaN   \n",
       "1                       NaN                        NaN   \n",
       "2                       NaN                        NaN   \n",
       "3                       NaN                        NaN   \n",
       "4                       NaN                        NaN   \n",
       "\n",
       "   uid_acc_id1_bal_last_min  \n",
       "0                       NaN  \n",
       "1                       NaN  \n",
       "2                       NaN  \n",
       "3                       NaN  \n",
       "4                       NaN  \n",
       "\n",
       "[5 rows x 1508 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存特征文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../cache/features_add.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
